ocean protocol: decentralized substrate for data services technical whitepaper ocean protocol foundation with bigchaindb gmbh and newton circus (dex pte. ltd.) version -apr- abstract this paper presents ocean protocol. ocean decentralized protocol and network artificial intelligence (ai) data/services. aims spread the benefits ai, unlocking data while preserving privacy. helps power marketplaces buy/sell data services, software publish and access commons data, and ai/data science tools consume data. ocean does decentralized orchestration: its core are decentralized service agreements and decentralized access control, which execute decentralized virtual machines. this allows connection to, monetization of, and curation arbitrary data services. that, ocean adds network rewards incentivize data sharing, including privacy-preserving data commons. this technical whitepaper intended accompany the information memorandum for the token distribution details published ocean protocol foundation ltd. information memorandum "). accordingly, this technical whitepaper intended read conjunction with, and subject to, the legal disclaimers and notices set out the information memorandum. oceanprotocol.com bigchaindb.com dex.sg contents overview introduction ... service agreements access control ... proof-of-service, and incentives ... on-chain bounties ... permissionless ocean tokens paper organization context use cases ... proprietary data: autonomous vehicles ... regulated data: medical research ... global data commons stakeholders pricing ocean decentralized orchestration ocean inter-service network decentralized federated learning ocean core architecture introduction architecture overview tier application layer ... pleuston frontend marketplace template ... data science tools tier protocol layer ... squid libraries main api ocean ... aquarius metadata management ... brizo capabilities for publishers service providers tier decentralized layer ocean core behavior asset registration asset consumption identity metadata dids ddos assets metadata ontology service execution agreements (seas) introduction unhappy paths service execution traditional service level agreements introducing seas web web service agreements sea templates seas example providing data anatomy modular sea ... service identifier ... conditions and fulfillments ... reward logic the life service execution agreement ... service publication ... access control ... verified services ... combining publishing, consumption, and validation initial network deployment physical network network legals network governance proofs service introduction service integrity: data integrity computational integrity incentives core token design: curated proofs market ... introduction ... network rewards incentivize relevant data/services make available ... separation roles vs. one "unified" keeper ... network rewards: towards implementation ... network rewards: seas-based implementation curation markets details ... introduction ... tokens for services: drops ... bonding curves ... un-staking ... convergence relevant data/services network rewards schedule ... bitcoin rewards schedule ... challenges for ocean ... addressing the challenges ... ocean network reward curves ... ocean network reward equations on-chain bounties introduction design clans introduction clans design going permissionless outstanding concerns conclusion references appendix: secret store introduction architecture encryption decryption ... authorization ... deployment appendix: computing services details (brizo) motivation architecture enabling publisher services (brizo) responsibilities flow appendix: addressing key goals incentive design appendix: faqs and concerns data storage arbitration rights violation paid data rights violation free data (elsa anna attack) data escapes curation clones sybil downloads rich get richer pump-and-dump drops network rewards for on-premise data appendix: one-token vs. two-token systems appendix: data pricing strategies overview introduction modern society runs data economist modern artificial intelligence (ai) extracts value from data. more data means more accurate models [banko] [halevy] which turn means more benefits society and business. the greatest beneficiaries are companies that have both vast data and internal expertise, like google and facebook. contrast, startups have amazing algorithms but are starving for data; and typical enterprises are drowning data but have less expertise. the power both data and ai--and therefore society--is the hands few. our aim spread the benefits equalizing the opportunity access data. reduce this practical goal, our aim develop protocol and network--a tokenized ecosystem. this network can used foundational substrate power new ecosystem data marketplaces, and more broadly, data sharing for the public good. this not easy, there are several challenges: how make data available for use ai, without losing control the data data escapes) more generally, how resolve the tension between wanting more data for better models, with the individual right privacy? how incentivize putting data the commons? this actually re-statement the classic paradox "information wants free, and expensive the physics bits make nearly free copy and spread. yet the right information the right place changes your life. [information] can address this? can reward relevant data over spam? how can the system not only self-sustain but actually improve over the decades, without guidance centralized actor? how spread the benefits and control this ecosystem? have devised design called ocean protocol that, believe, meets these objectives. answer the first two questions above, ocean has functionality reconcile spread data with privacy and the commons. has infrastructure for service agreements and access control, which allows compute brought the data. has cryptographic proof-of-service and network rewards incentivize supply relevant data services. for the last two questions, the ocean design uses on-chain bounties ensure long-term sustainability and improvement public network. finally, ocean will ensure spread benefits and control going permissionless. the next four subsections provide brief introduction these four solutions. later sections will provide greater detail for each. ... service agreements access control this addresses: "how make data available for use ai, without losing control the data data escapes )?" and related questions. the ocean network composed data assets and services. assets are the form data and algorithms. services are processing and persistence which leverage assets. the assets and services are made available for consumption via the network. the heart this are decentralized service execution agreements (seas) and decentralized access control, which together power data service supply chains. this allows connection to, monetization of, and curation arbitrary data services. this allows compute brought the data thereby solving the concern data escapes since sensitive data never needs leave the premises (just the results computation). this baseline functionality will coincide with the deployment the initial physical network. ocean's main target users--ai researchers and data scientists--expect reasonable network performance and usage costs. for this reason, ocean will initially ship proof-of-authority (poa) network called "pacific", where each node parity ethereum client. refer nodes and node operators "keepers" [zurrer] highlight that they are different than traditional blockchain "miners". the keepers community will collectively govern adding/removing keepers and smart contract upgrades. ... proof-of-service, and incentives this addresses: "how incentivize putting data the commons?" and related questions. top the seas infrastructure, ocean adds network rewards incentivize data sharing, which will lead commons. specifically, ocean incentivizes participants submit, refer, and make available (provably) quality data services, via new construction that call curated proofs market (cpm). cpm has two parts: predicted popularity dataset/service, and its actual popularity: cryptographic proof. the actual popularity the count the number times the dataset/service delivered made available. avoid being gamed, must made available provable fashion using cryptographic proof. for example, this may proof data availability. curation market. this for predicted popularity, proxy for relevance. the crowd knows much better than designers ocean whether given dataset/service relevant; harness the power the crowd via curation market. this market can thought giving reputation data/services where the actor must "put their money where their mouth is." they stake buy "shares" (aka drops) that dataset/service. the earlier that actor stakes bets given dataset/service, the more drops they get for the amount staked, and turn the higher the reward. avoid people gaming the reward system, only stakeholders provably making high-quality data/services available will able reap rewards. network rewards for given dataset/service are distributed based amount stake that dataset/service, and its actual popularity. other words, cpms instantiate the goals verification virality the best our knowledge, ocean the first system that explicitly incentivizes people share their data/services, independent whether free priced. whoever bets the most popular data/service (and makes available) wins the most rewards. ... on-chain bounties this addresses the question "how can the system not only self-sustain but actually improve over the decades, without guidance centralized actor?" ocean will utility network serve the public large. therefore must self-sustaining; and the technology and ecosystem need steady improvements, the community and for the community. this end, the ocean network will have on-chain bounties system where portion the network rewards are for technology improvement and ecosystem development. elaborate this later section. ... permissionless this addresses the question "how spread the benefits and control this ecosystem way that balances needs among various stakeholders and towards all society?". here, ocean will from poa permissionless, while maintaining performance. technology options include eth., parity substrate, and cosmos sdk. elaborate this later. ocean tokens ocean tokens are the main tokens the network. denote ocean tokens with ticker symbol ocean. they are used several ways. first, ocean tokens are used unit exchange for buying and selling data/services. marketplace would price data/services ocean tokens (ocean), any other currency the marketplace's vendor's choice, such usd, eur, eth, dai. the latter, the marketplace would use crypto exchange convert just-in-time ocean. therefore the ocean network would only see ocean. explicitly chose one-token design over two-token design for simplicity, and help equalize the access upside opportunities owning assets. the appendix has details. second, ocean tokens are used for staking this includes staking each iven dataset/service, and introduce long tail additional tokens called drops. drops are derivative tokens ocean tokens denoted d". each dataset would have its own derivative token. for example, drops stake dataset x". drops relate ocean tokens via curation markets' bonding curves, which determine the exchange rates between them for different dataset/services. finally, ocean tokens are used dispensing network rewards according ocean's inflation schedule. paper organization the rest this paper organized follows. section provides context with discussion use cases, stakeholders, and data ecosystem. sections and describe ocean core architecture and behavior, respectively. sections describe core components ocean for ocean these core components enable the construction data marketplaces and data commons, and integration data science tools. section identity metadata section service execution agreements access control section initial poa network deployment sections describe components beyond that realize the full vision ocean. the roadmap [oceanblog_roadmap] has schedule details. section proofs-of-service section incentives section on-chain bounties section clans section permissionless network deployment section describes outstanding concerns. section concludes. section has references. the appendices give information: the components secret store and computing services; how the token design addresses key goals; faqs and concerns, including arbitration, rights, and data escapes; and data pricing strategies. ocean work progress. therefore this document should taken current conception what are targeting with ocean, with some description the how. continue develop the technology, anticipate that there could changes the "what" the "how" the technology, from the building blocks specific parameters. please treat these the current suggestions and options rather than final choices. when ocean's public network live, should considered as-is, and the reader should not infer any guarantee that particular functionality described this whitepaper will delivered. this also goes for future updates the network. context use cases these use cases and others guide our design. ... proprietary data: autonomous vehicles leading use case for proprietary data autonomous vehicles (avs). the rand corporation calculated that billion trillion miles driven are needed get models accurate enough for production deployment avs [kalra] our collaborators toyota research institute (tri) saw that would prohibitively expensive for each automaker generate that much data its own. why not pool the data, via data marketplace? with this goal our minds, built such prototype together with tri [bigchaindb_tri_] however, single data marketplace may itself centralized, which means arrive another data silo. need substrate that enables many ata marketplaces emerge. this key goal ocean protocol. critical new benefits emerge: higher liquidity for each marketplace, and organizations are directly incentivized pool data rather than silo it. training data illustrates how not all data fungible: mile driven blizzard worth more than mile driven empty, sunny desert highway. but one mile the blizzard fungible with other miles blizzards. the system must account for both fungible and non-fungible data. the tri collaboration helped spark the creation mobi [mobi] blockchain consortium automakers representing world auto production. collaborate with mobi towards data marketplace, and more. ... regulated data: medical research this leading use case for data that must follow data protection regulations support privacy; and therefore will need privacy-preserving compute services. munich-based connectedlife [connectedlife] along with medical researchers the national neuroscience institute singapore, specialist professionals and hospital groups singapore, germany, and elsewhere are working towards objective measurement the symptoms parkinson's disease. the goal build subject-specific and generalized models based patient bio-medical and free-living sensor data. however, ethical concerns and personal data protection laws prevent patient data from being copied and shared without considerable transformation the data taking place and thereby removing much the value and potential impact in-terms patient-data-driven applications. data marketplace makes easier connect the data suppliers; and must decentralized avoid the siloing issue. this provides with excellent use case for privacy-preserving compute. ... global data commons our vision grow massive set data assets, all free for the planet use. we've seen glimpses the power this. for example, imagenet open dataset with over million tagged images much larger than previous open image datasets. has allowed researchers train image classifiers with radically less error than before, for dozens computer vision applications [imagenet] there are several other open data efforts; unfortunately each siloed with little incentive create more current, valuable data/information and share among them. directly incentivizing data sharing can address this. second problem that there are costs hosting data even free consume; commons datasets can offline funding not secured. ocean's network rewards are new way pay for this cost, and therefore solve the issue. stakeholders understanding network stakeholders precursor system design. table outlines the stakeholder roles participating the network. there are roles beyond, from developers auditors, but they are outside the scope this paper. table key stakeholders ocean ecosystem stakeholder role what value they can provide what they might get return data/service provider, data custodian, data owner data/service. suppliers the market. ocean tokens for making available providing service data/service curators signal the relative value data/service ocean tokens for curating data/service verifier. includes resolution linked proofs other chains data/service (via provider etc), verification ocean tokens for verification data/service consumer, e.g. data scientist ocean tokens, signals curators data/service (market's demand) keepers correctly run nodes network ocean tokens for chainkeeping marketplaces, data commons (optional) connect other actors. run metadata store, secret store transaction fees pricing marketplaces will have their own approaches pricing, but for discoverability, liquidity, and clearing, ocean itself will store the pricing information. envision the following. free data. want encourage growing data commons for the world, where anyone can download commons data without needing pay. priced fungible data/services. some data exchangeable with decent liquidity, for example electrocardiogram (ecg) data for people singapore the roughly same people germany. exchanges are low-friction way handle fungible data and services, they let the market determine the price automated way. priced non-fungible data/services. some data services are not easily exchangeable; for example single dataset with exclusive license. when that's the case, pricing options include fixed price, auction, and royalties. each has pros and cons. the appendix elaborates possible pricing strategies. for any pricing that more complex than "fixed price", ocean network will most likely need have smart contracts holding the service contract. ocean will provide schemas for the more common pricing approaches (fixed, royalties, auction, etc.). ocean decentralized orchestration the world big data, there are frameworks for large compute jobs (or streams) like mapreduce, hadoop, spark, and flink. the user configures directed acyclic graph (dag) compute and storage. often those dags are simple compute pipeline. the framework orchestrates the work. figure illustrates. since loves data, people use these frameworks the course building models (or ai-tuned variants like tensorflow). figure example compute dag. orchestration frameworks execute compute dags. ocean does decentralized orchestration orchestrates the execution compute dags decentralized setting. ocean, the compute dag specified service execution agreement (sea). sea the decentralized equivalent service level agreements (slas) found big data environments. figure shows the mechanics ocean orchestration. sea contract handles each step the dag compute; and compose one higher level sea for the whole dag. all the seas have guaranteed execution due running the blockchain-based ocean network (bottom). once you deploy them, they simply go; single entity can't intervene and stop them (unless it's within the definition sea). the decentralized setting means that seas unlock new capabilities, like privacy-preserving compute. arbitrary forms compute can brought the data. therefore private data--the most valuable data--never needs leave the premises, while the same time value can extracted from it. envision that ocean will have adapters leverage existing front and back-end components existing orchestration frameworks. for example, people might specify sea within apache spark context; but then the sea will executed within ocean. template seas make commonly-repeated tasks easier deploy. figure the mechanics ocean orchestration ocean inter-service network executing compute dags, each step might have one many providers. there will providers for data, algorithms, compute, storage, etc. each these may centralized behind firewall, centralized the cloud (e.g. aws ec), fully decentralized their own networks (e.g. filecoin golem, enigma). figure ocean acts inter-service network ocean doesn't provide these services itself, simply connects them. this makes inter-service network specifically for compute dags big data ai. figure illustrates. decentralized federated learning ocean and seas enable decentralized ederated learning. traditional federated learning [konecny] the model learned across many private data silos great starting point for data privacy. the challenge that centralized entity orchestrates the learning and stores the final model. this opens privacy attack vectors. for example, being the man the middle orchestration means can get signals about information flow. furthermore, fully controlling the final model dangerous that model can reverse-engineered get pii. also means that the orchestration task can halted midway single controlling entity, for example government overstepping. decentralized federated learning learns also across many private data silos, with the benefits for doing so. addition, single entity controls the execution the federated learning task; and desired, control the final model also decentralized. this addresses privacy attack vectors, spreads control, and guarantees execution sea. openmined [openmined] related effort that focuses solely decentralized federated learning. complementary ocean; look forward when plugs into ocean service. ocean core architecture introduction ocean's decentralized service execution agreements (seas) and decentralized access control combine power data service supply chains. this allows compute brought the data thereby solving the concern data escapes since sensitive data never needs leave the premises. this allows connection to, monetization of, and curation arbitrary data services. this image, logos shown are example services that could integrated. does not imply that the services are currently integrated, nor does imply business relationship. this section describes the architecture and components these foundational elements ocean. they're described more precise software form than later sections because they've already been built and are running testnet. this section provides high-level overview. further details will found the ocean software documentation [oceandocs] and ocean ocean enhancement proposals (oeps) [oeps_] architecture overview the ocean protocol network architecture implemented based oep [oep_] figure shows the following components (from top bottom): frontend (tier application implemented using html javascript css, running the client side (user's browser). data science tools (tier applications executed data scientists, typically getting access the ocean data and executing algorithms top that data. aquarius (tier backend application providing metadata storage and management services. typically executed marketplaces. brizo (tier backend application providing access control services, i.e. consumer access publisher data compute services. typically executed publishers. keeper contracts (tier solidity smart contracts running decentralized ethereum virtual machine (evm). critically, neither metadata nor data stored the blockchain network (ocean keeper network). marketplaces store the searchable metadata. publishers are incentivized register their work several marketplaces. figure ocean protocol network architecture ocean architecture has similarity that the domain name system (dns) [dns] the dns, global singleton "thin" lower-level store holds mapping from domain name (e.g. amazon.com) address (e.g. ...). one level diverse set marketplaces like godaddy and gandi.net, where "thick" metadata including personally identifiable information (pii) held, such customer credit cards and contact information. ocean has global singleton "thin" lower-level blockchain store holding registered decentralized identifiers (dids) [did] and associated permissions and service agreements. one level diverse set marketplaces for data and compute services, where "thick" metadata including pii held. just godaddy, gandi.net and the like are each incentivized hold large number domain addresses and act accordingly, marketplaces ocean will have the same incentives, which means that there will probably many ocean marketplaces having large supplies data and services. now elaborate each ocean component, for tiers and respectively. tier application layer ... pleuston frontend marketplace template pleuston [oceanpleuston] implements template for marketplace, which includes example usage lower-tier components. helps demonstrate some the capabilities ocean. not final product, but can used reference implement further marketplaces using the existing code. the pleuston application implements the following high-level functionality: publishing allows the user publish new assets the network. consuming allows the user list and consume published assets. marketplace enables complex interactions and advanced capabilities like: publishing assets existing cloud providers search and discovery assets filtering user registration using specific kyc processes (to added) marketplaces communicate with the following external components: smart contracts enable interaction with the ocean keeper contracts that provide the market business logic. this integration implemented using the ethereum javascript api (web.js) [webjs] aquarius and brizo for metadata management and access assets (details are below). this communication implemented with http apis. the frontend application will subscribe the evm transaction log and listen the events emitted from the smart contracts, enabling the receipt asynchronous messages. this will facilitate the triggering automatic actions when some events are raised (e.g. the request asset triggered automatically when the purchase has been confirmed). figure frontend high-level architecture the squid library shown figure encapsulates the logic deal with the ocean components (such keeper nodes and aquarius nodes). squid libraries (written various programming languages) are part tier [oceandocs] has further documentation setting marketplaces. ... data science tools data science tools are the interface ocean used researchers and data scientists. typically written python, those tools and libraries expose high-level api allowing one integrate ocean capabilities various computation pipelines. ocean manta ray notebooks provide guided tour ocean protocol interactive jupyter notebook environment. more information [oceandatascience] tier protocol layer includes all the high-level libraries interact with ocean protocol, and the enabler interacting with the keeper contracts. ... squid libraries main api ocean squid high level specification api abstracting the interaction with the most relevant ocean protocol components. allows one use ocean capabilities without worrying about the details the underlying keeper contracts metadata storage systems. the complete specification the squid api github [squidapi] this can considered the formal specification interface with the ocean ecosystem. initially the squid api implemented javascript [squidjs] (used pleuston), python [squidpy] (used the data science tools), and java. ... aquarius metadata management aquarius [oceanaquarius] python application running the backend that enables metadata management. abstracts access different metadata stores, allowing aquarius integrate different metadata repositories. the oceandb plugin system can integrate different data stores (e.g. elasticsearch, mongodb, bigchaindb) implementing the oceandb interfaces. figure shows the high-level architecture aquarius. figure aquarius high-level architecture ... brizo capabilities for publishers service providers brizo component providing capabilities for publishers. brizo interacts with the publisher's cloud and/or on-premise infrastructure. figure illustrates. the most basic scenario for publisher provide access the assets the publisher owns manages. addition this, other extended services could also offered, e.g. computing top the data without moving the data storage services for newly derived assets gathering service proofs enables different kinds service proofs from different providers. for example allowing the retrieval receipts from cloud providers validate service delivery. on-chain access control brizo charge the on-chain validation, controlling the assets services that consumer entitled get access to. this happens integrating with the keeper from the brizo side. the appendix contains more details how brizo enables publishers provide computing services and related services. figure brizo high-level architecture tier decentralized layer the keeper contracts are solidity smart contracts deployed and running the decentralized ethereum virtual machine (evm). ocean core behavior asset registration figure describes how publisher registers asset. figure asset registration flow with the help squid, did (decentralized identifier) generated, giving the asset unique, cryptographically secure id. the did, url and threshold ("document") are encrypted and stored with the help parity secret store (the appendix has details). then, ddo (did descriptor object) created from the did, asset's metadata, public key, encrypted url, and list services (data compute provided). the publisher now holds did with associated ddo. she publishes this pair marketplace running aquarius, which storing its own database (an oceandb instance) persist the pair, searchable via the metadata. the publisher registers that did the ocean keeper network well. asset consumption now describe how asset registered the ocean network consumed, figure first, consumer (via squid) conducts search marketplace's metadata store (via aquarius interface). he/she finds service offering (sea) for data compute that she likes. she digitally signs the sea. the next few steps, service provider running brizo will execute the agreement that consumer can access (via on-chain access control) and consume the asset after sending the payment keeper smart contract. note that on-chain access control helps resolve the did into the ddo, which includes the metadata for the asset corresponding the same did. figure asset consumption flow emphasize that ocean's access control on-chain. for simplicity, access control implemented using the service execution agreements (seas) building blocks. oep has precise specification on-chain access control with seas [oep_] identity metadata dids ddos each ocean user, asset, and group users (clan) gets identity the form did (decentralized identifier) and associated ddos (did descriptor objects). this improves discoverability and usability data assets. did has its own associated ddo, which includes claims and metadata. some that metadata points specific asset resources. metadata stored oceandb that running marketplaces. with large scale metadata, discoverability indispensable functionality search for needed assets. addition, keeper contracts' service execution agreements (seas) grant access specific assets for specific users clans. figure illustrates. oep has details [oep_] figure each user, asset and clan gets did (identity). assets metadata ontology asset metadata one part ocean ddos. this json object with information about the asset. ocean's assets metadata ontology the schema for the asset metadata, and specified oep [oep_] it's based the public dataset schema from schema.org [schemaorg] oep specifies the common attributes that must included any asset metadata stored the ocean network, such name, datecreated, author, license, price, files (to urls), file checksums, tags, and more. addition, oep recommends some additional attributes for discoverability and normalizes these attributes for curation purpose, which serve common structure for sorting and filtering ddos. life cycle metadata. metadata first created the publisher the asset. during the publishing process, the publisher provides the file urls plaintext, which will encrypted metadata store (aquarius) the backend and stored encrypted urls. service execution agreements (seas) introduction this section introduces seas [dejonghe] they are the fundamental building blocks for provable provenance, dispute resolution, reward mechanics and on. consists various service conditions the ocean network and fulfills specific data services. seas specify the compute dag that ocean orchestrates. unhappy paths service execution lot can wrong when dealing with data services, for example: services might not exist, although the consumer paid for them. services might delivered correctly, however the consumer refuses forgets reward the service provider (sp). sps might not grant access rightful users grant access non-rightful users. the service didn't meet the functional requirements performance expectations the consumer. service response logs got "lost" the network translation. traditional service level agreements our physical world, service-level agreements (slas) are key defining the relationship between end-user client and their service providers (sps). slas are commitments among the multiple involved parties. each sla explicitly states the service that the end-user expects receive from the and clarify the performance metrics used gauge the service quality. case any disagreements around the delivered service, all involved parties must turn sla resolve the dispute. introducing seas ocean service execution agreements (seas) bring the idea slas blockchains. surviving unhappy paths, seas can take advantage not only legal agreements, but also add layers mathematical security (encryption, signatures, hashes, cryptographic proofs attestations) and automation. seas are building blocks for following services: decentralized access control dispute resolution provenance service consumption network rewards and incentives seas are composed of: did, serviceagreementid, referencing the unique agreement between the consumer and publisher; servicedefinitionid eferencing the service the consumer wishes use; and templateid, referencing service agreement template from which the unique service agreement derived. with these components, the transaction can clearly defined and executed. seas are implemented smart contracts running ocean keepers. sea operates decentralized escrow, holding payments. the contract can resolve (i.e. the contract "executes" "aborts" the payments) when sufficient conditions the contract are fulfilled (inspired the interledger protocol [thomas] web web service agreements distributed service networks add layer redundancy the cost replication. the trust level decreases, for example the case (anonymous) permission-less networks, one can explore economic and reputational incentives that use the underlying token. the more critical service within supply chain, the more stringent the levels guarantees that need enforced. figure illustrates. sea templates ocean provides reference implementation sea templates for certain functionalities, such data access on-premise compute. user can create their own seas, top new conditions such as: minimum accuracy requirement trained model satisfied, which triggers the release payment the model creator. consumers hold certain credentials affiliation certain institute. release partial payments the service provider depending the percentage fulfilled services. once the new template created, the user can create new service agreement instantiating the new templateid and submitting the network. once the service agreement has been signed both service provider and consumer, will initialized on-chain, and voila, therefore, new customized service agreement established both parties, which can used fulfill more complex services. figure levels trust increase with more cryptographic verification (going left right) seas example providing data figure illustrates one scenario. the provider willing deliver data service after consumer ensures payment will follow upon delivery. the consumer turn will only pay granted access the service and the performance the service verified verifier. the verifier (or network verifiers) also demands see reward locked escrow (for her) before she puts any verification work. figure three parties engaging service agreement. anatomy modular sea ocean protocol, seas have modular design give flexibility for onboarding and consuming variety web. (cloud/on-premise) and web. services. identify three main parts sea: service identifier, conditions fulfillments, and reward logic. figure illustrates. now describe each more detail. figure components service execution agreement. ... service identifier this cryptographic that uniquely identifies the service consumed. use dids here for that purpose. ... conditions and fulfillments imperfect world, deal with off-chain, on-chain, side-chain and other-chain services and events. these services can either execute correctly, fulfill partially, even fail. some point time the ocean seas will interested knowing the status these services settle resolve disputes. hence introduce conditions and fulfillments: cryptographic and non-cryptographic conditions that can fulfilled. each condition has validation function that returns state: "true", "false" "unknown". "unknown" value implies that condition has not been provably fulfilled. all conditions have the same initial state "unknown". the validation logic executed on-chain. conditions can serve the inputs sea. conditions allow flexibly encode proof-of-service into seas. figure illustrates. conditions are the challenges solved and fulfillments are the solutions. reward logic distributes the rewards depending the fulfilled conditions. conditions can vary from simple cryptographic challenges (e.g. provide pre-image which hashes value with trailing zeros, prove that you have the private key corresponding public key), more complex ones (e.g. starks, compute attestations, proof-of-spacetime, proof-of-retrievability) and more subjective ones (e.g. m-of-n signatures voting curation scenario, stake/slash). when validation events occur non-ocean networks, conditions can simply fed into oracles bridge contracts resolve the dispute. figure from conditions over fulfillments rewards. conditions are the challenges solved and fulfillments are the solutions (green: fulfilled/valid, orange:unfulfilled/unknown, red:invalid). reward logic distributes the rewards (outputs) depending the fulfilled conditions. the actual implementation conditions and fulfillments variant the crypto-conditions internet-draft (of ietf), initiated the interledger protocol [thomas] each condition/fulfillment cryptographic challenge/proof pair such as: hash pre-image: find pre-image that computes given hash. the computation the hash the pre-image happens on-chain. this condition useful for party prove that they have knowledge secret (without revealing that secret). public key message signature: sign given message with private key corresponding given public key. the verification the signature happens on-chain. useful for party prove that they have the private key corresponding the given public key, and the message wasn't altered transit. m-of-n threshold: validates the proof "true" out conditions have been correctly fulfilled. useful for multi-party dispute resolution such voting. query/resolve: link publicly available state value (that recorded with timestamp) and resolve/compare that value upon validation. the query executed on-chain, hence it's limited get operations within the state context the chain (e.g. contractaddress.getvalue). useful bridge services and oraclize off-chain values. conditions can combined build more complex logic express: payment conditions: the amount token submitted the contract equals the predefined asset price. access control: access-control secret has been communicated with the consumer. [oceanblog_accesscontrol] has details. verified compute: network verifiers agrees and verifies that service was correctly delivered not. inevitable that many new conditions will emerge the ecosystem, hence careful auditing and curation are required green-light conditions that can safely added into sea, for example using aragondao [aragon] ... reward logic the outputs sea are rewards that are typically distributed agents that have fulfilled one more conditions. rewards can issued the form payments, royalties, permissions, badges, reputation lottery tickets for network reward function, and on. multiple reward mechanics can envisioned, they are consolidated governed templates, similar the library conditions. basic reward structure implemented ocean protocol the escrow hold token. here tokens are locked sea either: executed all conditions are fulfilled before timeout occurs. executing the payment means that the locked tokens can transferred the receiving party. aborted not all conditions are fulfilled after the timeout occurs. aborting the payment means that the locked tokens are (typically) returned the issuing party. future functionality can include bounties, competitions, royalty schemes, and payment streams. the life service execution agreement having all components sea place, one can start publishing services, attaching service agreement, and interacting marketplaces with consumers. figure illustrates. figure ocean protocol's publishing flow: from resource service execution agreement ... service publication provider can provision services defining metadata (see [oep_] and api calls for access, consumption and monitoring (see [oep_] next, the provider takes the role publisher (or delegates that role) marketplace. publisher chooses sea from the templates and includes the service identity document before publishing marketplace (see "publishing" section [oep_] publishing methods include public metadata store (aquarius), web apis/forums and peer-to-peer messaging. once service published, can discovered consumer. this will enable both parties instantiate sea signing and executing the agreement (see "consuming" section [oep_] now let's explore life cycles few example seas during their operation. ... access control one elementary sea provisioning access control off-chain resources with escrow consumer payment. figure illustrates. here are the relevant events for the sea: sign and execute: oth parties agree and instance the access control sea created. payment: consumer locks the required amount tokens the escrow smart contract. access: the service provider grants access the resource and reports this event on-chain. rewards: the escrow contract will either execute abort, taking the access condition and timeout into consideration. [oceanblog_acesscontrol] has details. figure life simple access control sea after publication ... verified services more complex use case extends the above access control adding verification events the service. here, the resource provider submits one more service proofs attestations verifier network. figure illustrates. the verifier network tasked with resolving disputes about the performance the service (e.g. truebit, fitchain, enigma, filecoin). here, the seas are connected either oracle bridge contract that can queried using query conditions. hence, the sea will able access and resolve the dispute resolution outcome the verifier network. ... combining publishing, consumption, and validation we've done the groundwork put all events the life sea together, figure notice that multiple seas can easily executed parallel. off-/side-chain resources and authorization servers simply need listen for the predefined events emitted from the sea. figure external verifier network used for validating proofs-of-service and bridging the outcome the dispute resolution cycle. initial network deployment physical network ocean includes the launch the initial physical network. ocean's main target users are researchers and data scientists. they demand reasonable network performance and usage costs. for this reason, ocean will initially ship proof-of-authority (poa) network called "pacific" running parity ethereum clients [mcconaghy]. ocean will become permissionless later milestone. maximize clarity, ocean not shipping the rd-party network called "poa network" [poanetworkb] ocean shipping its own dedicated network. figure the life ocean protocol sea from publishing consumption and validation. using poa raises the question: how users pacific get ocean tokens, and exchange with ether? there's good answer: use erc-erc tokenbridge [poanetwork] between pacific and ethereum mainnet. such bridge, users can move tokens back and forth any time. ocean users will use pacific, except for using ethereum mainnet interface with token exchanges. launch will also include parity secret store cluster, allow the sharing secrets among parties. the appendix has more information. launch will also include commons marketplace (provider) with its own aquarius (with oceandb metadata store) and brizo nodes (service provisioning). this will help demonstrate ocean capabilities using free/open assets. finally, launch will include block explorer. network legals one running software that publicly accessible, whether web server poa node, then one subject laws the jurisdiction that they operate and potentially other jurisdictions. these include data processing regulations like the general data protection regulations (gdpr). accordingly, node operators face liability around data protection light these laws. then question for node operators is: want their liability uncertain, clearly delineated? prefer the latter. can take steps make liability clear, even decentralized setting. this creating legal entity that interfaces with the legal world, with appropriate governance. have chosen dutch stichting, which nonprofit foundation with base the netherlands. european, yet has low latency and flexibility changing its articles association. these characteristics will useful evolve ocean governance being permissionless. the stichting lays out contracts for each node operator, including data processing agreements. this clarifies liability. believe that node operator risk low because there little opportunity have personally identifiable information (pii) on-chain: ocean keepers not store data metadata, and even hyperlinks are encrypted via parity secret store. network governance the most important governance decisions are (a) adding and removing node operators, and (b) upgrading the smart contracts running the nodes' virtual machines. make both decisions function node operators: one operator one vote. call "multisig". practice, the stichting makes the decisions, and democratically governed node operators. aim for minimum degree evolvability "software upgrades" framing minimum viable governance the smart contract upgrades are handled zeppelinos [zeppelin] ocean will become permissionless later milestone. proofs service introduction top the seas infrastructure, ocean adds network rewards incentivize data sharing. ocean rewards participants for providing data and other services. avoid being gamed, ocean needs some proofs that verify the work provide data/services was actually done: service integrity proofs. ocean needs service integrity proofs for both data and compute, for both web and web (blockchain) services. the first priority web data, which the most mature and unlocks many use cases. service integrity: data integrity introduction need able prove that actor made the correct file available, versus incorrect one. put another way, how tell the data asset just made available the same the one that was initially uploaded? for clients (verifiers) reliably retrieve data object, storage service (prover) required provide concise proof that data was made available and can recovered its entirety. early work introduces cryptographic building block known proof retrievability (por). por enables user (verifier) determine that archive (prover) "possesses" file data object these proofs rely efficient hash functions while ensuring that the memory and computational requirements for the verifier are independent the (potentially large) size the file other words: data integrity requires that bounded prover can convince clients accept altered falsified data after recovery get operation [filecoin] data availability most clients with access permissions the datum can see then available. for ocean protocol both data integrity and availability are important design constraints. popular datasets should become more available referral while respecting ownership attribution. web data services today, most data not available web services. don't expect there massive amounts data available web services the next few years, and data doesn't want moved. need where the data is. data the cloud, on-premise environments the shape big data lakes, data warehouse bespoke solutions. the two largest cloud providers are amazon aws and azure. ocean supports both. addition, ocean supports standard http on-premise urls. the next question how get proof from these services. web data integrity this tricky web environment. the best scenario, unless you want move the data (not good idea) that you need rely the infrastructure provider (amazon, azure, etc). mitigate this, store data integrity hash on-chain smart contract representing the ground-truth multiple attributes the dataset. compose this hash use the checksum information given the cloud providers. means the owner the file changes the content deletes it, the same checksum can't computed cloud providers again. the hash calculated as: concatenation all the ddo.services['accessservice'].metadata.files.checksum[*] attributes. every file included the asset can have file checksum associated concatenating the previous string the metadata attributes name, author and license concatenating the previous string the did (e.g. did:op:ebedadafdebbfbdfffcceffcecfdacdea) hashing the complete string generated using sha- section "how compute the integrity checksum" [oep_] has details. web-web data integrity oracles bring web data services (off-chain) closer web trust levels (on-chain). [chainlink] requires multiple independent sources for the same data; the replication factor the user. [oraclize] uses tlsnotary proof [tlsnotary] web data integrity availability there are several options here. none are fully mature, and some are still early stages. proof-of-space-time (post) proves that data blob was stored for time interval [filecoin] ethereum swarm [tron] has similar aims, but emphasizes erasure coding. proof-of-retrieval (por) proves that data was retrieved, without worrying about storage [filecoin] por chunks data and pays for each piece sequence, using state channels for efficiency. proof-of-replication (porep). porep proves that data was replicated, using verifiable delay functions challenge-response setting. dedicated pow blockchain with full replication. [teutschb] each miner promises serve every dataset; and voting happens among all miners. challenge-response could delay voting unless there was actually challenge. but full replication means expensive storage. dedicated pow blockchain, with per-block sharding. [arweave] each block points merkleized data blob. stakers can stake any blocks. asked return data, they get reward they succeed and are slashed otherwise. this shards with natural bias replicate the most popular content more. the chain breaks data cannot returned when asked; therefore the community incentivized always maintain every block's blob. provably secure certified mail. with literature going back two decades, [pfitzmann] [schneier] this seems proof-of-data-availability disguise, but would need investigate further confirm. network validators hash merkle subtree. this early-stage research ocean combining blockchain replication and erasure coding. for example, alice claims have served dataset. the network randomly selects validators, from the set validators that have staked that dataset. each validator attempts download merkle branch the dataset from joe (whole dataset not needed). alice gets rewards validators agree that she served up; she gets slashed agree that she didn't; otherwise the validators get slashed. higher-efficiency variant have challenge-response gate before resorting network validators verify. computational integrity on-premise computation, the data consumer needs provably correct model execution the purchased data. hence, service needs provide sufficient proof convince verifier that the code actually ran the dataset computational integrity implies that reported response computation correct with respect request and dataset such that (s) ensuring that prover correctly reports the output rather than more favorable output the prover. high level, the computational integrity represented two parties where there are verifiers and provers. let illustrate figure verifier simply able send task function and input prover will execute the computation behalf then return the output along with short proof. computational integrity defined correctness, soundness and zero knowledge, where correctness means that can convince concerning true statement and soundness means that cannot convince any false statement. figure computational integrity framework each proof system usually relies assumptions. assumptions mean that the prover may have huge computation power which guarantees that the protocol will execute any task the prover cannot solve certain problems. also the verifier might have access all inputs (like public blockchains) not (such confidential transactions). moreover, assumptions can include replication computation (for example proof work), executing tasks trusted hardware (such trusted execution environment "tee"), using multi-party computation (mpc) where single entity has the whole secret the attestation, auditing. there are multiple factors for selecting the suitable protocol proof system including the functionality the protocol, the implementation complexity, the public verifiability, applicability zero-knowledge, the number required messages transferred between prover and verifier, etc. incentives core token design: curated proofs market ... introduction this section addresses: "how incentivize putting data the commons?" and related questions. ocean adds network rewards incentivize data sharing, which will lead commons. this section describes the core incentive model ocean. its heart network rewards function (objective function) implemented curated proofs market. recall that ocean's main goal is: maximize the supply relevant data services this drives ocean's network reward function akin objective function the optimization literature, for example used [mcconaghy] this objective function what ocean optimizing, incentivizing actors its ecosystem contribute it. bitcoin rewards contribution hash rate with bitcoin tokens; ocean rewards contribution relevant data/services with ocean tokens. besides the main goal, had several ancillary questions goals that guided token design. early designs did not meet them. will see later, the chosen design does. they are follows: for priced data, there incentive for supplying more? referring? good spam prevention? for free data, there incentive for supplying more? referring? good spam prevention? does support compute services, including privacy-preserving compute? they have incentives for supplying more, and for referring? does the token give higher marginal value users the network versus external investors? are people incentivized run keepers? simple? onboarding low-friction? ... network rewards incentivize relevant data/services make available network rewards are the key tool incentivize desired behavior, i.e. "get people stuff" [mcconaghy] ocean emits ocean tokens network rewards. want ocean have strong incentives submit, refer, and make available quality data/services. accomplish this, introduce curated proofs market, which combines (a) cryptographic proofs that the data/service was made available, with (b) curation markets rouviere for reputation data/services. does curation cryptographic proofs. uses stake measure the belief the future popularity the data/services, where popularity measured number times that service made available. network rewards for dataset/service are function how much actor has staked that dataset/service, the dataset/services's actual (proofed) popularity, and the actor's serve-versus-use ratio. now elaborate. first describe ideal token allocation approach then describe practical implementation. here the ideal allocation approach, i.e. the approach assuming computational constraints. rij the network rewards for actor dataset/service before eing normalized across all actors and datasets/services. the actual network rewards received are normalized: rij,norm rij log log (dj rij,norm rij rij where actor i's stake dataset/service measured drops. number deliveries dataset/service the block interval global ratio for actor erving vs. accessing dataset; details are below total ocean tokens given during the block interval according the overall token reward schedule (see appendix) the first term rij log reflects the actor's belief the popularity the dataset/service, measured drops. the actor that posts the data/service believes that will popular, then they can stake even more than the minimum posting amount, curation-market style, and receive more drops. additionally, others that believe the future popularity the data/service can stake whatever they would like, curation-market style, and receive drops. these mechanics incentivize participants submit relevant datasets/services, and gives them opportunity make money too. use log curation market stake (drops) level the playing field with respect token whales; and that token whales are incentivized make greater number datasets/services available. this has theoretical roots kelly betting: applying the log the optimal strategy for individual maximize their utility betting [kellycriterion] [simmons] later section elaborates curation markets including stake drops; and another section how manage identities prevent stake whales from creating multiple accounts. the second term, log (dj reflects the popularity the dataset/service; that is, how many times has been (provably) used the time interval. use log incentivize actors stake and make greater number datasets/services available. the first and second term can summarized binding predicted opularity actual popularity. this the core mechanic curated proofs market. the third term, mitigate one particular attack vector for data (it's excluded for services.) "sybil downloading" where actors download files repeatedly increase their network rewards (more this later). uses tit-for-tat approach like bittorrent measuring how much data actor has served up, versus how much accessed, follows: {min(b served,i downloaded,i all data assets served; otherwise} where served,i total number bits that actor served (made available) across all data assets they have staked downloaded,i total number bits that actor accessed, from any data assets actor has staked data asset and they want get rewarded, then they must run keeper node that makes that data asset available. they don't make available when asked (or fail other keeper functionality), they will lose their stake that data asset. it's they retrieve last-minute from another miner; it's more reward cdn (content delivery network) [cdn] opposed proof storage like filecoin [filecoin] for early staker data/service that has since had more stake, they can subsequently pull out their stake profit, curation-market style. it's worth emphasizing: when say "stake" for that dataset/service, mean the amount it's worth terms the derivative token for that dataset/service, called "drops". later section elaborates. ... separation roles vs. one "unified" keeper designing the system, want incentivize the various stakeholder roles: data/service provider, curator, validator, and keeper. one possibility give percentage network rewards each role based their respective actions. however, this raises the concern that keepers take all the rewards for themselves. one solution explicitly couple all the roles into one: you've staked (provider curator) then the only way get network rewards run keeper node. however, this doesn't work poa (proof authority) setting with limited number keeper nodes, because severely restricts who can commons service provider getting network rewards. another solution disincentivize eepers from taking all the rewards for themselves. for example, keepers may have stake slashed the community they act unfairly. another approach nothing. the community believes that keeper behavior unfair, then they will simply leave the network, and the keepers are left with nothing. however, this would only work the cost leaving was lower than malfeasance behalf keeper; that's probably too strong assumption. will defer the final choice until have progressed farther implementation. ... network rewards: towards implementation implement the network rewards described above has complexity and high compute cost because, for each network rewards cycle, need compute the amount stake for each dataset/service made available each actor, and we'd need transaction each actor reward their effort. can address these issues giving keepers the same expected value network reward (though higher variance), with less computation using bitcoin-style strategy (called "probabilistic micropayments" [salomon] bitcoin, every ten minutes, tokens (bitcoins) are awarded single eeper (miner) where the probability reward proportional value added (miner hash rate), compared the rest the network's value added (network hash rate network difficulty). network difficulty updated every two weeks. ocean similar. rather than rewarding fixed time intervals, every time keeper makes dataset/service available consumer, ocean randomly chooses whether give network rewards. the amount awarded based the value added the keeper rij and total network value added. rdif iculty the network difficulty; gets updated every two weeks minutes), i.e. the difficulty interval. rrecent the value added since the last difficulty update. network launch, rdif iculty the beginning each difficulty interval, rrecent here's what happens when actor makes dataset/service vailable consumer. compute value added: rij log log (dj update total recent network value added: rrecent rrecent rij compute the probability getting network reward, wanted one reward average every two weeks, would (). but let's have rewards every minute average. minutes two weeks. so, add the factor minutes)/( minute). the result (). rij rdif iculty rij rdif iculty compute whether actor gets the reward: u[,], i.e. draw random real number between and using e.g. [randao] [syta] then actor ill get the reward this parameter, like many parameters ocean, are subject change. actually wrap each log() expression with max avoid negative values. e.g. max(, log(s the actor get the reward, then compute reward and give it, via transaction with output actor since step has bias reward more often using the factor (/), here need divide the amount awarded that same factor. arrive the fraction rewards for this action this difficulty interval. compute reward scale dif iculty where dif iculty the total ocean tokens given during the two week difficulty period according the overall token reward schedule (see appendix). rij rdif iculty reward dif iculty once every difficulty interval (two weeks), the difficulty will updated with recent. the change limited the previous difficulty value. rdif iculty max(. rdif iculty min(. rdif iculty rrecent ... network rewards: seas-based implementation aim implement network rewards simply special cases service execution agreements (seas). figure the ocean network token dispenser periodically mints new ocean tokens that drip into the treasure chest (top middle). commons provider stakes provide service (left). every time she provides service, she gets "raffle tickets" that service fits within commons template (bottom). raffle occurs among raffle ticket holders, and the winner gets the rewards the treasure chest (top right). figure network rewards commons agreement template curation markets details ... introduction recall that ocean's objective function (network reward function) maximize the supply relevant data/services. ocean uses curation markets [rouviere] signal how relevant dataset service might be. curation markets leverage the wisdom the crowd: people stake datasets/services they believe in. other words, they put their money where their mouth is. traditional curation markets, the main action for actors stake and un-stake means signaling. ocean builds this binding those staking actions with actual work making service available curated proofs market. this section elaborates curation markets. each dataset/service has its own curation market, which turn has its own token called drops, and bonding curve hat relates drops ocean tokens o". ... tokens for services: drops let's elaborate drops first. recall that drops are derivative tokens ocean tokens denoted that measure stake for each dataset/service. for example, drops stake dataset dx". users can get value from drops two ways: network rewards. people earn ocean tokens they bet dataset service and make available when asked (in the model where one unified keeper plays several roles). un-staking. one can un-stake convert from service back ocean tokens. drops are measure user's attention: user cares about dataset the user will stake dataset get drops that is, dx. because there scarcity ocean tokens, there scarcity drops, which mirrors user's scarcity attention. short, are proxy for mindshare because each dataset/service has its own token, user ocean will likely hold not just ocean tokens their crypto wallet; they may also hold dx, dy, general variety drops for the datasets and compute services that they've staked. ... bonding curves bonding curve relates token's drops ocean tokens for given dataset/service. figure shows bonding curve for dataset relates the price buy more drops (y-axis) function the current supply drops (x-axis). people stake more interest its supply goes according the bonding curve. bonding curves can take whatever shape the creator wishes. but reward early adopters, bonding curve typically makes more expensive buy more people stake it; this the positive slope the curve. figure bonding curve for new curation market initialized each time new dataset service made available. with this, the actor has already staked have the dataset service vetted. later section describes vetting. once vetted, this stake goes into the curation market, return for drops measure stake. figure illustrates. we're the far left the bonding curve because have been generated. there, each costs the initial user staked she would gain o/dx dx. the supply for increases from figure increasing supply from here on, anyone can stake further let's say user wants purchase staking more tokens. this would make the supply from dx. that range, the price o/dx)/ o/dx. the user would get for total cost dx/o figure illustrates. figure increasing supply ... un-staking user can sell some their service any time, return for this the exact backwards action compared staking. the supply goes down correspondingly. the ability un-stake for leads the possibility for pump-and-dump behavior. later section, discuss this further, and how mitigate it. ... convergence relevant data/services one can ask: how does the token design lead large supply relevant data/services? overall, each actor has "holdings" terms stake (belief) the relative value different datasets/services. actor early understand the value dataset/service, they will get high relative rewards. this implicitly incentivizes referrals: will refer you datasets/services that have staked in, because then get more network reward. actors get rewarded the most they stake large amounts popular datasets/services the first and second terms the network rewards function, respectively. put another way, they must predict that dataset/service will popular, then see its actual measured popularity (as proxy for relevance). just one alone not enough. over time, this causes convergence towards relevant datasets/services. network rewards schedule this section describes ocean's network rewards schedule. ... bitcoin rewards schedule bitcoin's network rewards schedule is: (h, t/h where (h, the fraction all reward tokens that have been released after years the half-life, years. half-life the time taken for remaining supply released. bitcoin uses half-life years. the bitcoin reward schedule has several nice features, including fixed supply tokens, more benefits early movers, and track record being live for decade. for these reasons, ocean uses bitcoin's approach starting point. ... challenges for ocean the ocean schedule token design must resolve these additional challenges: means pay for the development the network can take several years for internal enterprise processes prepare their data assets for sharing ocean release does not have network rewards; only ocean release does. ocean will only become permissionless there needs strong incentive for the network permissionless. how verify the incentives when the best verification having the system live, because means real actors with real skin the game? this chicken-and-egg concern. ... addressing the challenges address challenge with pre-mining build ocean software (e.g. with developer bounties), incentivize the community, and more. ocean tokens remain for network rewards. address challenge via half-life ten years, i.e. this gives entities wishing share data breathing space. address challenge inserting delay into the network rewards function: only emits tokens once the network rewards infrastructure ocean ready. address challenge only turning full (%) emission once the network permissionless (scheduled for ocean v), maximum years, whichever sooner. address challenge three forms verification: human, software, and economic human verification simply human-based vetting the token design, including whitepaper reviews and security audits. software verification running software simulation and verification tools, done engineering fields. economic verification includes ratcheting the amount rewards over time, until stability reached specifically for ocean, start with multiplier rewards, then after months, then after another months (capping until the next gate), and finally when permissionless (or years hit). considered having more ratchets based milestones other than time. however, each non time-based milestone adds complexity, and must measured with relatively obvious threshold. for these reasons, only have single non-milestone ratchet after the initial network rewards: when the network becomes permissionless. the years hit" part for the final ratchet just case the "permissionless" measure ineffective, after years the priority for being permissionless has diminished. ... ocean network reward equations this section describes the equations network rewards, which implement the goals above. g(t) the overall network rewards schedule. it's piecewise model four exponential curves. where i(t) are pieces the piecewise model chosen depending and are the values (t) the inflection points. and f(t) the value f(h,t) assuming constant f(h,t) the base exponential curve. the units and are years. (h, t/h the pieces the model are function which are parameterized with the units are years. min(, max(t the following parameter values are chosen: years; ... ocean network reward curves this section aims give insight into how network rewards are dispensed over time. the values time interval after network launch for network rewards) and time interval after network launch for permissionless) are dependent implementation timelines. this section assumes years and years; these values may change practice due implementation. figure shows how network rewards will dispensed over the next years. one can readily see the ratchet every months. figure network rewards tokens released over next years figure shows network rewards over years. the curve shaped much like bitcoin's, except the first two years ramp-up and the longer half-life. figure network rewards tokens released over next years table shows network rewards going years. takes years get dispensed; year lag compared ramp-up period. table network rewards released over next years years released left on-chain bounties introduction this section addresses the question "how can the system not only self-sustain but actually improve over the decades, without guidance centralized actor?" the ocean answer this have functionality on-chain bounties. design ocean will utility network serve the public large. therefore must self-sustaining; and the technology and ecosystem need steady improvements, the community and for the community. this end, the ocean network will have on-chain bounties system where portion the network rewards are for (a) technology improvement and (b) ecosystem community development. the dash network has on-chain bounties system part its governance system [dashgovernance] its bounties have been live since september [dashwatch] has: on-chain means for people submit proposals; on-chain means for the community select proposals fund, then fund them; on-chain means track progress and completion each proposal; off-chain interfaces the above, such dash central [dashcentral] and importantly, enables on-chain funding for proposals. dash's network rewards towards its bounties system. any allocated funds not spent are burned this incentivizes proposer not just add value, but add value where the rise the value each dash token due the reduction total dash tokens. dash provides good reference design for ocean on-chain bounties. aim flesh out the ocean design with learnings from dash, other governance-oriented blockchains like [decred] and [tezos] and tooling like [aragon] and zeppelin] hope use off-the-shelf tooling components where possible. clans introduction ocean, "clan" group people, where membership rules for that group are governed aragondao [aragon] clans are motivated the following use cases. contests, hackathons, impromptu collaborations. group hackers data scientists self-organize try solve problem, such kaggle competition hackathon. they want able easily access each others' data and compute services they progress, especially they are working remotely from each other. regulatory sandboxes. government wants give means for organizations run "monitored" regulatory environment that the government can observe. the organizations are vetted the government and may have access specially designated government compute services. enterprise data access. enterprise might make some its data available only its employees, but want able use ocean services available the broader network. sharing autonomous driving data. individuals each membership companies mobi [mobi] need access automotive data from any one the mobi member companies. could time-consuming and error-prone specify permission for each member company individually. furthermore, those permissions will fall out date mobi members are added removed; and updating the permissions one organization time could time-consuming error-prone. this involves two levels permissions: access member companies into mobi, and access individuals each member company (enterprise). sharing medical data. researchers european soil that wish directly access german medical data need demonstrate that they have been accredited appropriate authorities. this will usually through their hospital university. there could thousands researchers accessing the data. with automotive data, will time-consuming and error prone specify and update permissions for each these thousands researchers. this may two levels permissions (hospital/university into authority; individual into hospital/university), may among hospitals and universities more networked fashion. sharing financial data (while preserving privacy). small and medium sized credit unions the u.s. have challenge: they don't have large enough datasets justify using ai. since the credit unions don't compete with each other, they would find great value build models across their collective datasets. clans can help. collective bargaining ("data labour"). rural canada, farmers grouped together into cooperatives like swp [swp] have measurable clout negotiating prices for grain, marketing it, and shipping it. labour unions have done the same for factory workers, teachers, and many other professions. [ibarra] the authors suggest that data creators are currently getting raw deal, and the solution make labour union for data. within ocean, clan would the appropriate organizing body. clans design ocean's baseline permissioning system grants access individual addresses. address the given use cases, ocean needs means grant permissions group people. these people are grouped together via ocean clan infrastructure. given clan needs membership rules: the process select and add new members; and remove existing members. possibilities include: higher-level authority chooses group members off-chain; and grants on-chain clan membership accordingly. example governments enterprises. group uses off-chain mechanisms (e.g. voting) choose group members; and grants on-chain clan membership accordingly. example mobi. group uses on-chain mechanisms (e.g. voting) choose group members. on-chain clan memberships then come directly. example mobi other consortia (in the future). voting can take many forms: one member one vote, one token one vote, one member one vote with minimum staking, etc. on-chain voting gives new options, such token curated registry (tcr) [goldin] member clan may individual, another clan. the latter needed support nested clans, such the use cases medical data and automotive data. aim implement clans ocean using aragondaos. going permissionless overall, there tradeoff among ecentralization, onsistency, and cale the dcs triangle [mcconaghy] however, see this more engineering challenge than fundamental constraint, and are hopeful about efforts improve scaling while retaining sufficient consistency (preventing double spends) and decentralization (sybil tolerance). move from poa permissionless, our technology options include: continuing run dedicated ocean chain, but with permissionless consensus. examples include parity substrate [paritysubstrate] cosmos sdk [cosmossdk] with permissionless algorithm, simply ethereum (v) code running separate network. running party permissionless chain that has achieved both decentralized and scale, e.g. improving the consensus protocol sharding. examples include eth., dfinity [hanke] ouroboros [kiayias] red belly crain] omniledger [kokoris] continuing run dedicated -node poa ocean chain, but giving better optionality for dissatisfied users leave. this includes plasma [poon] and ost [ost] shifting some work "layer state channels like lightning [poon] raiden [raiden] factors influencing these choices include: the ocean keeper code written solidity. the ethereum ecosystem provides decent solidity developer tools, well libraries erc standards. ethereum has the biggest developer community and values well aligned with ocean. ethereum mainnet hosts many useful services that currently only run there, like decentralized exchanges. however "glue technology" like inter-chain networks can mitigate the effect this. for example, aragon moving run polkadot [wood] addition ethereum mainnet. other technologies include cosmos [kwon] interledger [thomas] and truebit [teutsch] outstanding concerns believe this system design reasonable first cut. however, still have concerns. the biggest include: usability. seas are powerful but pretty complex for typical data scientist use. how make them more approachable? templates will part the answer. complexity security. there's lot infrastructure for seas, permissioning, validation proofs, curation markets, and all the interactions. complexity exposes the size the attack surface. incentives fail. believe have reasonable token design incentivize supply data services. however, have not yet thoroughly verified with software simulation. nor have verified mainnet with economic skin the game--when the real evaluation begins. will only feel more comfortable about this once the system live. verifiable data availability maturity issues. technology has all these characteristics: decentralized, fast, general, mature software, achilles heel (e.g. sgx hardware exploits). web approaches rely trusting web cloud providers. filecoin's proof-of-space-time not available yet. teutsch's data availability proof expensive. our challenge-response proof needs vetting. verifiable compute maturity speed issues. technology the combination decentralized, fast, general, mature software, achilles heel. web approaches rely trusting web cloud providers. homomorphic encryption not both general and fast. mpc is, but the software not yet mature. hardware-based trusted execution environments fail soon hardware-level vulnerabilities are found. secure containers have big attack surfaces. concerns described elsewhere. the appendix describes more specific concerns. believe have reasonable answers each concern. however, these answers may not perfect have unforeseen issues. addressing these concerns and others that appear, may turn out that the final designs will quite different than the ones this document. engineers, are perfectly comfortable with this. conclusion this paper presented ocean protocol: protocol and network for data and services. aims spread the benefits ai, unlocking data while preserving privacy. helps power marketplaces buy/sell data services, software publish and access commons data, and ai/data science tools consume data. this paper described four challenges and solution for each: ocean helps make data available for use without losing control the data, via decentralized service agreements and permissioning that bring compute the data. ocean has network rewards incentivize supply commons data and services. ocean uses on-chain bounties continuously fund improvements the network and community. ocean permissionless with governance balance stakeholder groups. references [amazon] "amazon object storage built store and retrieve any amount data from anywhere", amazon.com, last accessed feb. https://aws.amazon.com/s/ [aragon] "aragon: unstoppable organizations", homepage, last accessed feb. https://aragon.one/ [arweave] "arweave: introducing the permaweb," last accessed feb. https://www.arweave.org/ [azevedo] eduardo azevedo, david pennock, and glen weyl, "channel auctions", aug https://papers.ssrn.com/sol/papers.cfm?abstract_id= [banko] banko and brill, "scaling very very large corpora for natural language disambiguation", proc. annual meeting association for computational linguistics, july, http://www.aclweb.org/anthology/p- [bigchaindb_tri_] bigchaindb team, "bigchaindb and tri announce decentralized data exchange for sharing autonomous vehicle data", may, https://blog.bigchaindb.com/bigchaindb-and-tri-announce-decentralized-data-exchange-for-sharing-autonomous-vehicledata-dbde [connectedlife] connectedlife homepage, last accessed feb. https://connectedlife.io [cdn] "content delivery network", wikipedia, last accessed feb. https://en.wikipedia.org/wiki/content_delivery_network [chainlink] "chainlink: your smart contracts connected real world data, events and payments", homepage, last accessed feb. https://chain.link/ [cosmossdk] "cosmos sdk documentation", homepage, https://cosmos.network/docs/ [crain] tyler crain al., "(leader/randomization/signature)-free byzantine consensus for consortium blockchains", may https://arxiv.org/abs/. ("red belly" blockchain) [daostack] "daostack: operating system for collective intelligence." homepage. last accessed feb. https://daostack.io/ [dashcentral] "dash central masternode monitoring and budget voting." homepage. last accessed feb. https://www.dashcentral.org [dashgovernance] "dash decentralized governance system." homepage. last accessed feb. https://www.dash.org/governance/ [dashwatch] "dash watch: discover more about dash's funded proposals." homepage. last accessed feb. https://dashwatch.org/ [decred] decred autonomous digital currency. homepage. last accessed feb. https://decred.org [dejonghe] dimitri jonghe, "exploring the sea: service execution agreements", ocean protocol blog, nov. https://blog.oceanprotocol.com/exploring-the-sea-service-execution-agreements-fde [did] "decentralized identifiers(dids) v.: data model and syntaxes for decentralized identifiers", community group, draft community group report february last accessed feb. https://wc-ccg.github.io/did-spec/ [dns] "domain name system", wikipedia, last accessed feb. https://en.wikipedia.org/wiki/domain_name_system [economist] "the world's most valuable resource longer oil, but data", the economist, may https://www.economist.com/news/leaders/-data-economy-demands-new-approach-antitrust-rules-worlds-most -valuable-resource [filecoin] protocol labs, "filecoin: decentralized storage network", august https://filecoin.io/filecoin.pdf [grigg] ian grigg, "the ricardian contract", proc. ieee international workshop electronic contracting, http://iang.org/papers/ricardian_contract.html [goldin] mike goldin, "token-curated registries .", medium.com, sep. https://medium.com/@ilovebagels/token-curated-registries---afdac [halevy] alon halevy, peter norvig, and fernando pereira, "the unreasonable effectiveness data", ieee intelligent systems (), march-april https://research.google.com/pubs/archive/.pdf [hanke] timo hanke, mahnush movahedi and dominic williams, "dfinity technology overview series: consensus system", jan. https://dfinity.org/pdf-viewer/pdfs/viewer?file=../library/dfinity-consensus.pdf [hewitt] carl hewitt, peter bishop, and richard steiger, universal modular actor formalism for artificial intelligence", proc. intl. joint conf. artificial intelligence, [hintjens] hintjens, social architecture: building online communities, https://www.amazon.com/social-architecture-building-line-communities/dp/ [ibarra] imanol arrieta ibarra al, "should treat data labor? moving beyond 'free'", aea papers and proceedings, -.doi: ./pandp., https://www.aeaweb.org/conference//preliminary/paper/ynna [iexec] iexec team, "the iex.ec project: blueprint for blockchain-based fully distributed cloud infrastructure", march, https://iex.ec/app/uploads///iexec-wpv.-english.pdf [imagenet] "imagenet", wikipedia, last accessed feb. https://en.wikipedia.org/wiki/imagenet [information] "information wants free", wikipedia, last accessed feb. https://en.wikipedia.org/wiki/information_wants_to_be_free [infura] "infura: scalable blockchain infrastructure", infura homepage, last accessed feb. https://infura.io [ipfs] "ipfs: ipfs the distributed web", ipfs homepage, last accessed feb. https://ipfs.io/ [ipld] "ipld: ipld the data model the content-addressable web", ipld homepage, last accessed feb. https://ipld.io/ [kalra] nidhi kalra and susan paddock, "driving safety: how many miles driving would take demonstrate autonomous vehicle reliability?", rand corporation, apr https://www.rand.org/pubs/research_reports/rr.html [kellycriterion] "kelly criterion", wikipedia, last accessed feb. https://en.wikipedia.org/wiki/kelly_criterion [kiayias] aggelos kiayias al., "ouroboros: provably secure proof-of-stake blockchain protocol", proc. annual international cryptology conference, august https://eprint.iacr.org//.pdf [kokoris] eleftherios kokoris-kogias al., "omniledger: secure, scale-out, decentralized ledger via sharding", proc. ieee symposium security privacy, (preprint https://eprint.iacr.org//.pdf [konecny] jakub konecny al., "federated learning: strategies for improving communication efficiency", oct. https://arxiv.org/abs/. [kwon] jae kwon and ethan buchmann, "cosmos: network distributed ledgers", https://github.com/cosmos/cosmos/blob/master/whitepaper.md [lamport] leslie lamport al., "the byzantine generals problem", acm trans. programming languages and systems (), pp. july http://research.microsoft.com/en-us/um/people/lamport/pubs/byz.pdf [libpp] "libpp: modular peer-to-peer networking stack (used ipfs and others)", last accessed feb. https://github.com/libpp [mattereum], "mattereum: smart contracts for the real world", mattereum homepage, last accessed feb. https://mattereum.com/ [mcconaghy] trent mcconaghy and georges g.e. gielen, "globally reliable variation-aware sizing analog integrated circuits via response surfaces and structural homotopy," ieee trans. computer-aided design (), nov. http://trent.st/content/-tcad-sangria.pdf [mcconaghy] trent mcconaghy, "the dcs triangle: decentralized, consistent, scalable", july https://blog.bigchaindb.com/the-dcs-triangle-ceeefdc [mcconaghy] trent mcconaghy, "token design optimization design", blockchain meetup, feb. berlin, germany, https://www.slideshare.net/trentmcconaghy/token-design-as-optimization-design [mcconaghy] trent mcconaghy, "ocean poa vs. ethereum mainnet?", feb. https://blog.oceanprotocol.com/ocean-on-poa-vs-ethereum-mainnet-decdacc [mobi] "mobi mobility open blockchain initiative". homepage. last accessed feb. https://dlt.mobi/ [modifiedghost] "ethereum whitepaper: modified ghost implementation", ethereum wiki, last accessed feb. https://github.com/ethereum/wiki/wiki/white-paper#modified-ghost-implementation [monegro] joel monegro, "the price and value governance", video, recorded token engineering global gathering, may [mongodb] "mongodb atlas: database service", last accessed feb. https://www.mongodb.com/cloud/atlas [moor] james moor, ed. the turing test: the elusive standard artificial intelligence. vol. springer science business media, [nakamoto] satoshi nakamoto, "bitcoin: peer-to-peer electronic cash system", oct https://bitcoin.org/bitcoin.pdf [nucypher] "nucypher: proxy re-encryption for distributed systems", nucypher homepage, last accessed feb. https://www.nucypher.com [oceanaquarius] "aquarius: provides off-chain database store for metadata about data assets. it's part the ocean protocol software stack", last accessed feb. https://github.com/oceanprotocol/aquarius [oceanbrizo] "brizo: helping publishers expose their services", last accessed feb. https://github.com/oceanprotocol/brizo [oceanblog_accesscontrol] "secure on-chain access control for ocean protocol", ocean protocol blog, dec. https://blog.oceanprotocol.com/secure-on-chain-access-control-for-ocean-protocol-dcaafc [oceanblog_roadmap] "ocean roadmap setting sail ocean protocol blog, feb. https://blog.oceanprotocol.com/ocean-roadmap-setting-sail-to--ca [oceandocs] "ocean protocol software documentation", last accessed feb. https://docs.oceanprotocol.com/ [oceandocs_secretstore] "secret store: how ocean protocol integrates with parity secret store", ocean protocol software documentation, last accessed feb. https://docs.oceanprotocol.com/concepts/secret-store/ [oceandatascience] "manta ray: data science powered ocean protocol", documentation, last accessed feb. https://datascience.oceanprotocol.com/ [oceanpleuston] "pleuston: web app for consumers explore, download, and publish data assets", github repository, last accessed feb. https://github.com/oceanprotocol/pleuston [oeps_] "ocean enhancement proposals (oeps)," last accessed feb. https://github.com/oceanprotocol/oeps [oep_] "ocean enhancement proposal ocean protocol network architecture," last accessed feb. https://github.com/oceanprotocol/oeps/tree/master/ [oep_] "ocean enhancement proposal decentralized identifiers," last accessed feb. https://github.com/oceanprotocol/oeps/tree/master/ [oep_] "ocean enhancement proposal assets metadata ontology," last accessed feb. https://github.com/oceanprotocol/oeps/tree/master/ [oep_] "ocean enhancement proposal on-chain access control using service agreements," last accessed feb. https://github.com/oceanprotocol/oeps/tree/master/ [openmined] "openmined: building safe artificial intelligence", homepage, last accessed feb. [oraclize] "oraclize: blockchain oracle service, enabling data-rich smart contracts", homepage, last accessed feb. http://www.oraclize.it/ [orbitdb] "orbitdb: peer-to-peer database for the decentralized web", github repository, last accessed feb. https://github.com/orbitdb/orbit-db [ost] "ost: powering blockchain economies for forward thinking businesses", homepage, last accessed feb. https://ost.com/ [ostrom] elinor ostrom, governing the commons: the evolution institutions for collective action. cambridge, uk: cambridge university press, [parityeth] "parity ethereum: the fastest and most advanced ethereum client", last accessed feb. https://www.parity.io/ethereum/ [paritysecret] "parity secret store wiki" (parity secret store documentation), last accessed feb. https://wiki.parity.io/secret-store [paritysubstrate] "parity substrate: build your own blockchains", last accessed feb. [parno] bryan parno al., "pinocchio: nearly practical verifiable computation", security and privacy (sp)", proc. ieee symposium security privacy, https://eprint.iacr.org//.pdf [pfitzmann] birgit pfitzmann al., "provably secure certified mail", ibm research report (#) //, https://www.researchgate.net/publication/_provably_secure_certified_mail [poanetwork] "the tokenbridge interoperability", homepage, https://poa.network/bridge [poanetworkb] "poa network: public ethereum sidechain with proof autonomy consensus independent validators", homepage, https://poa.network [poon] joseph poon and thaddeus dryja, "the bitcoin lightning network: scalable off-chain instant payments", january https://lightning.network/lightning-network-paper.pdf [poon] joseph poon and vitalik buterin, "plasma: scalable autonomous smart contracts", august https://plasma.io/plasma.pdf [porep] proof replication technical report (wip) protocol labs, https://filecoin.io/proof-of-replication.pdf [raiden] "what the raiden network?", last accessed feb. https://raiden.network/.html [randao] "randao: dao working rng ethereum", last accessed feb. https://github.com/randao/randao [rouviere] simon rouviere, "introducing curation markets: trade popularity memes information (with code)!", medium, may https://medium.com/@simondlr/introducing-curation-markets-trade-popularity-of-memes-information-with-code-bffe [salomon] david salomon al, "orchid: enabling decentralized network formation and probabilistic micro-payments", january version .., https://orchidprotocol.com/whitepaper.pdf [swp] "saskatchewan wheat pool", wikipedia, last accessed mar. https://en.wikipedia.org/wiki/saskatchewan_wheat_pool [schemaorg] "schema.org dataset schema", last accessed feb. https://schema.org/dataset [schneier] schneier and riordan, certified email protocol", ibm, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=.... [simmons] andrew simmons, "prediction markets: how betting markets can used create robust ai", sept. http://dstil.ghost.io/prediction-markets/amp/ [squidapi] "squid api specification", last accessed feb. https://github.com/oceanprotocol/dev-ocean/blob/master/doc/architecture/squid.md [squidjs] "squid-js: javascript client library for ocean protocol", last accessed feb. https://github.com/oceanprotocol/squid-js [squidpy] "squid-py: python client library for ocean protocol", last accessed feb. https://github.com/oceanprotocol/squid-py [syta] ewa syta al., "scalable bias-resistant distributed randomness", proc. ieee symposium security privacy, https://eprint.iacr.org//.pdf [teutsch] jason teutsch and christian reitwiessner, scalable verification solution for blockchains", november https://people.cs.uchicago.edu/~teutsch/papers/truebit.pdf [teutschb] jason teutsch, "on decentralized oracles for data availability", dec. http://people.cs.uchicago.edu/~teutsch/papers/decentralized_oracles.pdf [tezos] tezos. homepage. last accessed feb. https://tezos.com/ [thomas] stefan thomas and evan schwartz, protocol for interledger payments", ripple inc., https://interledger.org/interledger.pdf [tlsnotary] "tlsnotary mechanism for independently audited https sessions", sept https://tlsnotary.org/tlsnotary.pdf [tron] viktor tron al, "swarm documentation", jan. https://media.readthedocs.org/pdf/swarm-guide/latest/swarm-guide.pdf [webjs] "web.js ethereum javascript api", last accessed feb. https://github.com/ethereum/web.js/ [wood] gavin wood, "polkadot: vision for heterogenous multi-chain framework", https://github.com/polkadot-io/polkadotpaper/raw/master/polkadotpaper.pdf [zeppelin] "zeppelinos: develop, deploy and operate any smart contract project securely", homepage, last accessed feb. https://zeppelinos.org/ [zurrer] ryan zurrer, "keepers workers that maintain blockchain networks", aug. https://medium.com/@rzurrer/keepers-workers-that-maintain-blockchain-networks-ab appendix: secret store introduction ocean needs secure, scalable way share secrets among parties such publishers, consumers, providers, and smart contracts. parity secret store feature included the parity ethereum client [parityeth] allows users store fragmented ecdsa key the blockchain, such that retrievals are controlled permissioned smart contract. the secret store implements threshold retrieval system, individual secret store nodes are unable reconstruct the keys decrypt documents themselves. secret store node only saves portion the ecdsa key. the decryption key can only generated consensus reached amount secret store nodes bigger than the threshold that the publisher the secret chooses. from the parity secret store documentation [paritysecret] the parity secret store core technology that enables: distributed elliptic curve (ec) key pair generation key generated several parties using special cryptographic protocol, that: private key portion remains unknown every single party; public key portion could computed every party and could safely exposed external entities; every party hold the 'share' the private key; any subset parties could unite restore the private portion the key; any subset less than parties could not restore the private portion the key; distributed key storage private key shares are stored separately every party and are never exposed neither another parties, nor external entities; threshold retrieval according blockchain permissions all operations that are requiring private key, require least parties agree 'permissioning contract' state. this last point can enable ocean protocol have solid mechanism distribute encrypted contents between parties (consumers and publishers), and these contents can only decrypted after on-chain authorization phase. architecture the integration the secret could have the following characteristics: all the negotiation required encrypt decrypt resource happening without writing any information on-chain this integration only requires standard http requests between the clients (consumer, publisher) and the parity evm parity secret store api's (no gas, quick) requires the usage the parity evm secret store api existing the parity ethereum clients requires the usage the parity secret store api. this software part the parity ethereum client it's based the usage permissioned secret store cluster. this cluster would deployed/governed the ocean protocol foundation network launch part the base ocean core capabilities. additional parties (user/companies/organizations) could added members decentralize the responsibility running this infrastructure. publishers and consumers could use their own parity secret store cluster. the cluster use part the squid configuration used the different users ocean. figure shows the high-level architecture using secret store. figure high-level architecture using secret store encryption ocean implements the parity secret store publishing flow [paritysecret] various ocean protocol secret store clients (javascript, python, java) [oceandocs_secretstore] abstracted part the encryptdocument squid method. publisher (such alice the diagram above) can use encryptdocument encrypt document and store the decryption key the secret store cluster. the type document encrypt flexible: could one many urls, access tokens external resource, and on. typically ocean, during the asset access phase, are decrypting the url get access asset that stored cloud provider. this could extended the future allowing configuration access policies runtime. this would allow the granting access specific addresses get access assets, after the authorization phase. figure shows how secret store integrates into the ocean publishing flow. result this flow, the encrypted document can shared with the potential consumers (e.g. part the metadata via libpp [libpp] the action granting permissions on-chain specific user not part this flow. figure ocean secret store publishing flow decryption this logic for the standard parity secret store consuming flow [paritysecret] encapsulated part the decryptdocument quid method [oceandocs_secretstore] this method allows consumer, given resource unique and encrypted document (shared for the publisher via metadata libpp), decrypt this document using the secret store cluster capabilities. decryption only can achieved the secret store cluster achieves the quorum specified the publisher during the publishing process with the threshold attribute. the secret store checks the user's authorization permissions on-chain determine they are authorized decrypt document. the secret store won't allow decryption the user doesn't have authorization. figure illustrates how secret store integrates into the ocean asset consumption flow. ... authorization oep- [oep_] describes how understand how service agreements interact with secret store manage access control. ... deployment secret store functionality provided permissioned cluster that will executed part the ocean protocol basic infrastructure. nodes can only added this cluster changing the configuration the members the cluster, third-party malicious user can't join the cluster without changing the configuration the other nodes. figure ocean secret store consuming flow appendix: computing services details (brizo) this section describes how ocean protocol (with brizo) enables publishers provide computing services and related services. motivation the most basic scenario for publisher provide access the datasets they own manage. addition that, publisher could offer other data-related services. some possibilities are: service execute some computation top their data. this has some benefits: the data never leaves the publisher enclave. it's not necessary move the data; the algorithm sent the data. having only one copy the data and not moving makes easier compliant with data protection regulations. service store newly-derived datasets. result the computation existing datasets, new dataset could created. publishers could offer storage service make use their existing storage capabilities. this optional; users could also download the newly-derived datasets. architecture enabling publisher services (brizo) the direct interaction with the infrastructure where the data resides requires the execution component handled publishers. this component will charge interacting with users and managing the basics publisher's infrastructure provide these additional services. the business logic supporting these additional publisher capabilities the responsibility this new technical component. the key component introduced support these additional publisher services named brizo. brizo the technical component executed the publishers, which provides extended data services. brizo, part the publisher ecosystem, includes the credentials interact with the infrastructure (initially cloud providers, but could on-premise). because these credentials, the execution brizo should not delegated third-party. figure illustrates how brizo relates keeper contracts and cloud providers. figure brizo high-level architecture responsibilities the main responsibilities brizo are: expose http api allowing for the execution some extended, data-related services (compute and/or storage). authorize the user on-chain using the proper service agreement. that is, validate that the user requesting the service allowed use that service. interact with the infrastructure (cloud/on-premise) using the publisher's credentials. start/stop/execute computing instances with the algorithms provided users. retrieve the logs generated during executions. register newly-derived assets arising from the executions (i.e. new ocean assets). provide proofs computation the ocean network. figure shows the sequence diagram for computing services. involves the following components/actors: data scientists the end users who need use some computing/storage services offered the same publisher the data publisher. ocean keeper charge enforcing the service agreement tracing conditions. publisher infrastructure the storage and computing systems where the publisher's data resides (in cloud provider on-premise). publisher agent (brizo) orchestrates/provides additional publisher services. publisher the actor running brizo and other software. the publisher often the same the data owner. flow figure sequence diagram for computing services before the flow can begin, the following pre-conditions must met: the algorithm must implemented one the languages/frameworks supported the publisher. the asset ddo must specify the brizo endpoint exposed the publisher. the service agreement template must already predefined and whitelisted on-chain. the following describes the steps the flow. set the service agreement the game begins with data scientist looking for data asset and finding one where the data attached its own compute service. they pick the asset, sign the agreement, then send signature, templateid the asset publisher. the publisher sets new service agreement instance on-chain, using the data scientist's signature. fulfill the lock payment condition the data scientist listens for the executeagreement on-chain event, lock the payment the paymentcondition.sol smart contract and fulfills the condition. fulfill the upload algorithm condition the data scientist parses the ddo (using squid) see how the publisher exposes computing service. the computing service defines how upload algorithm the publisher side and how consume output. the data scientist uploads the algorithm off-chain directly the data set publisher's algorithm endpoint. once the algorithm files are uploaded, the data scientist calculates the files' hash, signs this hash and then submits only the signature on-chain. the publisher receives the algorithm files, calculates the hash (algorithm files) and submits this hash fulfill the uploadalgorithm condition. the keeper contracts automatically verify that both parties see the same files using the hash which submitted the publisher; the signature submitted the data scientist; and, the data scientist's address. the brizo software component responsible for running the algorithm, attaching data assets, and running the container. fulfill the grant access condition meanwhile, the publisher starts computation, trains the model, collects logs and generates outputs (the derived assets). finally, the publisher archives and uploads the outputs. the publisher registers the derived assets (outputs) dids/ddos and assigns ownership the data scientist. the publisher grants access the data scientist using the secret store and fulfills the grant access condition. release payment once the grant access condition fulfilled (within timeout), the publisher can release payment. the data scientist can consume the derived assets (outputs) calling the secret store which turn checks the permissions on-chain, and then downloads the outputs using the decryption keys. cancel payment the payment can cancelled only if: the access the derived assets (outputs) was not delivered within timeout period, payment was never locked. agreement fulfillment the service agreement will accepted fulfilled only the payment released the service provider. appendix: addressing key goals incentive design the main goal ocean network deliver large supply relevant data/services: "commons" data, priced data, and compute services. bringing compute the data and incentives are two complementary ways drive this. the latter, previously discussed, developed criteria compare candidate token incentive designs against. table describes the question criteria (left column) and how the token design addresses those criteria (right column). table how ocean incentive design addresses key goals key question for priced data, there incentive for supplying more? network rewards are function stake dataset/service. actors are incentivized stake relevant data/services soon possible because curation market pricing. the most obvious way get the best price then supply it. for priced data, there incentive for referring? curation markets incentivize data referrals, because they signal which high quality data. i've bet dataset, i'm incentivized tell others about it, they'll download stake it, both which reward me. for priced data, there good spam prevention? few will download low-quality data, i.e. spam. therefore even fewer are incentivized stake curation market. while can exist the system there incentive stake it. for free data, there incentive for supplying more? referring? good spam prevention? same priced data. does support compute services, including privacy-preserving compute? they have incentives for supplying more, and for referring? ocean's core construction curated proofs market, binding cryptographic proofs with curation markets. for data, the main proof for data availability. but this can replaced with other proofs for compute, including privacy-preserving compute. doing so, all the benefits data supply and curation extend compute services supply and curation. does the token give higher marginal value users the network versus external investors? yes. token owners can use the tokens stake the system, and get network rewards based the amount staked (and other factors). this means that participating the system, they're getting their tokens "work" for them. are people incentivized run keepers? yes. one only gets network rewards for data they've staked they also make available when requested; making data available key role keepers. simple? the system conceptually simple: its simple network reward function implemented binding cryptographic proofs *curation market, form curated proofs market. adds ancillary affordances needed, though those are changeable new ideas emerge. onboarding low-friction? on-boarding for the actors registry, and for each dataset might high because each case there token-curated registry that asks for staking and vetting period. however, each case have explicitly added low-friction alternative paths, such risk-staking. appendix: faqs and concerns this section addresses frequently asked questions and concerns about ocean. data storage where does data get stored? ocean itself does not store the data. instead, links data that stored, and provides mechanisms for access control. the most sensitive data (e.g. medical data) should behind firewalls, on-premise. via its services framework, ocean can bring the compute the data, using on-premise compute. other data may centralized cloud (e.g. [amazon] decentralized cloud (e.g. filecoin [filecoin] both cases should encrypted. this means that the ocean blockchain does not store data itself. this also means that data providers can remove the data, it's not decentralized and immutable substrate. marketplaces would de-list the data accordingly. this highly useful feature for data protection regulations, hate speech regulations, and copyrighted materials. arbitration what buy data, and the seller gives garbage? have recourse? ocean seas allow for inclusion traditional legally-binding contracts, which turn often have clauses arbitration, such "if the parties disagree, the matter will taken arbitration court jurisdiction x". makes sense for data and services marketplaces include such contracts (and corresponding clauses) their terms service, and each specific sea template that they use. anticipate that different marketplaces will gain specialties serving different jurisdictions and data service asset classes. the legal contracts can linked the network well, e.g. putting into ipfs and including the hash the legal contract the sea instance [grigg] rights violation paid data when someone registers work ocean, they are claiming have the appropriate rights make that data available others. what they don't actually have those rights, and make money selling the data? given that it's paid data, then there will almost certainly legal contract (provided the marketplace). this contract will include recourse such violations, such party arbitration. rights violation free data (elsa anna attack) concern: actor stakes and publishes that they clearly don't own, such the disney movie "frozen" featuring elsa anna. any parent young girls would recognize, the asset quickly becomes extremely popular. other words, elsa anna have bombarded the network. because the actor has staked the asset and served when requested, they would have quickly earned tremendous amount network reward tokens. finally, the actor leaves the network, taking their earnings with them (and none the rights holder). it's commons data, there might not legal contract, just sea. the baseline recourse for this similar how modern websites handle copyrighted material today. with the usa's digital millennium copyright act, copyright holders have the onus identify material that breaches copyright. they identify that material, the host needs remove de-index it. it's similar ocean: the marketplace will de-index the infringing work. recall that the ocean network itself stores metadata actual data, it's just pass-through. furthermore, ocean has day delay before network rewards are given service providers, add friction "hit and run" jobs. data escapes concern: you're trying sell data. but, someone else downloads and then posts for cheaper, even for free. fact the system incentivizes "free" network rewards, because there will more downloads for something that's free, versus paid. data rightsholder especially worried, they can also set permissions such that the data never leaves the premises; rather, compute must done locally. they this the cost virality due the data's openness, course. curation clones concern: you've published new unique dataset into the commons, and have kicked off curation market. it's popular, you earn many network rewards and many others stake too, raising the price staking. someone else sees the dataset's popularity, they re-publish the dataset and started brand new curation market, where they get significant stake the market because they were the early adopter. then, others repeat this. short time, have curation markets for the same dataset, hindering discoverability not mention being unfair the first publisher. the main tactic the design the bonding curve: make the price stake the curation market flat start with, rather than rising immediately. this way the first actors longer have incentive start their own curation market; rather they are incentivized grow the popularity that collectively-owned curation market. sybil downloads concern: actor puts high stake one data asset, then downloads many times themselves get more mining rewards. this could from their own single account, from many accounts they create, ring the actor and their buddies. this bad for second reason: it's giant waste bandwidth. this issue analogous the "click fraud" problem online ads. don't make rewards function based only the number files accessed. instead, make function the number bits accessed versus registered and the price paid for the data. the ability short bonding curves will another solution. rich get richer concern: long-standing concern with proof-of-stake (pos) systems that stakeholders get wealthier simply having more tokens. result, many pos systems have changed where stake needed simply participate the network; and perhaps higher staking gives more active role like being keeper node cosmos [kwon] "rich get richer" less concern for ocean because curation markets. recall that stake data curation market not "just" the amount you initially staked, but also how many tokens you would receive you withdrew. therefore, early adopters popular data service asset will get rewarded. for ocean, it's not rich get richer, it's "curate data well" "get richer". secondary equalizing factor using log stake. pump-and-dump drops concern: recall that each dataset/service has its own token its drops which are staked and un-staked the context its corresponding curation market. this scenario, "pumping-and-dumping" concern. for instance, someone stakes early curation market get then promotes that dataset/service others (pumps), and finally sells their big profit and leaves everyone else hanging (dumps). overall, this may not significant concern because "active" holders are actually earning ocean tokens making that service available when asked; they are getting positive marginal benefits for staking assuming efficient market, over time we'd end with only active holders. that said, might still see this type behaviour from bad actors. possible mitigations include: have one bonding curve for buying and second one for selling where the sell price lower than buy price. when selling, use dutch auction: the sell price the previous buy price, not the current price. have minimum staking periods. for example, the requirement hold any for least week. general, simply want add friction the sell side way that good actors won't mind, and bad actors will leave. overall, it's clear that pumping-and-dumping becomes real issue, have tools address it. network rewards for on-premise data concern: data asset on-premise, then only the actor storing that data asset can "keep" and earn network rewards for making available. others who might believe that it's valuable may stake curation market (and sell that stake gain later); but they cannot make available and therefore cannot get network rewards for it. this also means there automatic cdn functionality, retrieving that data will become bottleneck becomes popular. the answer twofold: privacy and markets. privacy. the reason store the data on-premise privacy, then should stay that way. privacy trumps access convenience and cdn scaling. markets. the actor storing that data asset sees that becomes popular, they are incentivized spread its usage more. the way that remove themselves bottleneck, letting other actors store the data and make available cdn scaling fashion. there's variant this concern when bring on-premise compute. with on-premise compute, anyone play keeper for data that on-premise, where they intermediary between the party that's hosting the data on-premise and the buyer the data. however the keeper won't able make the data available the data host doesn't make available. this case, the discussion the previous section still holds: the host won't either make available because privacy, they will make available because market forces. appendix: one-token vs. two-token systems recall that ocean tokens are used the medium exchange for buying/selling services, addition staking and network rewards. contemplated two-token design, where the unit exchange was the dai stablecoin (or something similar). the reasoning that buyers and sellers are less subject fluctuations the value ocean. however, that less issue with just-in-time exchange. the single-token approach has advantages. first, it's simpler. but the biggest advantage help create data economy where the everyone has equal access the benefits assets which grow wealth over time. cash typically depreciates over time. binding assets cash helps ensure that those who rely day-to-day cash have greater exposure upside opportunities [monegro] appendix: data pricing strategies ocean protocol enables many types data pricing strategies. here sampling. free market fixed price. here, the data provider fixes price and let's the market feedback buy not. "here data, take leave it". market feedback might influence pricing setting provider willing listen. believe other strategies will likely more effective. auctions. people bid and the winning bidder gets the dataset/service. there are variants: english (lower bound price that increases), dutch (upper bound price that decreases), channel (both lower and upper bound price) [azevedo] incentivized commons. put this data the commons (free consume), and when gets used get network rewards". this built into ocean's incentives system. data provider could further mandating that network rewards reinvested the system. royalties. "you use data and get every time you create value." simple but powerful, this how music and many other industries work. can bring this data. prize. there's prize which competitor does the best according some performance indicator. mechanics are like kaggle the netflix prize. here, there fixed finish line; the best result (or results) get the reward. usually, this time-bound. bounty. whoever solves the problem first, gets past some performance threshold first, wins the prize. there fixed finish line. the competition ends soon competitor crosses the finish line. for some pricing schemes, like fixed price and auctions, the data vendor may need create digital scarcity justify price. this possible, eg. limiting access single dataset parties over time interval.