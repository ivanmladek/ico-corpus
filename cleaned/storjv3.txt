storj: decentralized cloud storage network framework storj labs, inc. october https://github.com/storj/whitepaper copyright (c) storj labs, inc. and subsidiaries this work licensed under creative commons attribution-sharealike license (cc by-sa .). all product names, logos, and brands used cited this document are property their respective owners. all company, product, and service names used herein are for identification purposes only. use these names, logos, and brands does not imply endorsement. contents abstract contributors introduction storj design constraints security and privacy decentralization marketplace and economics amazon compatibility durability, device failure, and churn latency bandwidth object size byzantine fault tolerance coordination avoidance framework framework overview storage nodes peer-to-peer communication and discovery redundancy metadata encryption audits and reputation data repair payments concrete implementation definitions peer classes storage node node identity peer-to-peer communication node discovery redundancy structured file storage metadata satellite encryption authorization audits data repair storage node reputation payments bandwidth allocation satellite reputation garbage collection uplink quality control and branding walkthroughs upload download delete move copy list audit data repair payment future work hot files and content delivery improving user experience around metadata selected calculations object repair costs audit false positive risk choosing erasure parameters distributed consensus attacks primary user benefits contents abstract decentralized cloud storage represents fundamental shift the efficiency and economics large-scale storage. eliminating central control allows users store and share data without reliance third-party storage provider. decentralization mitigates the risk data failures and outages while simultaneously increasing the security and privacy object storage. also allows market forces optimize for less expensive storage greater rate than any single provider could afford. although there are many ways build such system, there are some specific responsibilities any given implementation should address. based our experience with petabyte-scale storage systems, introduce modular framework for considering these responsibilities and for building our distributed storage network. additionally, describe initial concrete implementation for the entire framework. contributors this paper represents the combined efforts many individuals. contributors affiliated with storj labs, inc. include but are not limited to: tim adams, kishore aligeti, cameron ayer, atikh bana, alexander bender, stefan benten, maximillian von briesen, paul cannon, gina cooley, dennis coyle, egon elbre, nadine farah, patrick gerbes, john gleeson, ben golub, james hagans, jens heimburge, faris huskovic, philip hutchins, brandon iglesias, viktor ihnatiuk, jennifer johnson, kevin leffew, alexander leitner, richard littauer, dylan lott, olio, kaloyan raev, garrett ransom, matthew robinson, jon sanderson, benjamin sirb, dan sorensen, helene unland, natalie villasana, bryan white, and shawn wilkinson. we'd also like thank the other authors and contributors the previous storj and metadisk white papers: tome boshevski, josh brandoff, vitalik buterin, braydon fuller, gordy hall, jim lowry, chris pollard, and james prestwich. we'd like especially thank petar maymounkov, anand babu periasamy, tim kosse, roberto galoppini, steven willoughby, and aaron boodman for their helpful review and contributions early draft this paper. would like acknowledge the efforts, white papers, and communications others the distributed computing, blockchain, distributed storage, and decentralized storage space, whose work has informed our efforts. more comprehensive list sources the bibliography, but would like provide particular acknowledgement for the guidance and inspiration provided the teams that designed and built allmydata, ceph, coralcdn, ethereum, farsite, filecoin, freenet, gluster, gfs, hadoop, ipfs, kademlia, lustre, maidsafe, minio, mojonation, oceanstore, scality, siacoin, and tahoe-lafs. finally, extend huge thank you everyone talked during the design and architecture this system for their valuable thoughts, feedback, input, and suggestions. please address correspondence paper@storj.io. introduction the internet massive decentralized and distributed network consisting billions devices which are not controlled single group entity. much the data currently available through the internet quite centralized and stored with handful technology companies that have the experience and capital build massive data centers capable handling this vast amount information. few the challenges faced data centers are: data breaches, periods unavailability grand scale, storage costs, and expanding and upgrading quickly enough meet user demand for faster data and larger formats. decentralized storage has emerged answer the challenge providing performant, secure, private, and economical cloud storage solution. decentralized storage better positioned achieve these outcomes the architecture has more natural alignment the decentralized architecture the internet whole, opposed massive centralized data centers. news coverage data breaches over the past few years has shown that the frequency such breaches has been increasing much factor between and []. decentralized storage's process protecting data makes data breaches more difficult than current methods used data centers while, the same time, costing less than current storage methods. this model can address the rapidly expanding amount data for which current solutions struggle. with anticipated zettabytes data expected exist and market that will grow billion usd the same time frame [], have identified several key market segments that decentralized cloud storage has the potential address. decentralized cloud storage capabilities evolve, will able address much wider range use cases from basic object storage content delivery networks (cdn). decentralized cloud storage rapidly advancing maturity, but its evolution subject specific set design constraints which define the overall requirements and implementation the network. when designing distributed storage system, there are many parameters optimized such speed, capacity, trustlessness, byzantine fault tolerance, cost, bandwidth, and latency. propose framework that scales horizontally exabytes data storage across the globe. our system, the storj network, robust object store that encrypts, shards, and distributes data nodes around the world for storage. data stored and served manner purposefully designed prevent breaches. order accomplish this task, we've designed our system modular, consisting independent components with taskspecific jobs. we've integrated these components implement decentralized object storage system that not only secure, performant, and reliable but also significantly more economical than either on-premise traditional, centralized cloud storage. chapter introduction have organized the rest this paper into six additional chapters. chapter discusses the design space which storj operates and the specific constraints which our optimization efforts are based. chapter covers our framework. chapter proposes simple concrete implementation the framework, while chapter explains what happens during each operation the network. chapter covers future work. finally, chapter covers selected calculations. storj design constraints before designing system, it's important first define its requirements. there are many different ways design decentralized storage system. however, with the addition few requirements, the potential design space shrinks significantly. our design constraints are heavily influenced our product and market fit goals. carefully considering each requirement, ensure the framework choose universal possible, given the constraints. security and privacy any object storage platform must ensure both the privacy and security data stored regardless whether centralized decentralized. decentralized storage platforms must mitigate additional layer complexity and risk associated with the storage data inherently untrusted nodes. because decentralized storage platforms cannot take many the same shortcuts data center based approaches can (e.g. firewalls, dmzs, etc.), decentralized storage must designed from the ground support not only end-toend encryption but also enhanced security and privacy all levels the system. certain categories data are also subject specific regulatory compliance. for example, the united states legislation for the health insurance portability and accountability act (hipaa) has specific requirements for data center compatibility. european countries have consider the general data protection regulation (gdpr) regarding how individual information must protected and secured. many customers outside the united states may feel they have significant geopolitical reasons consider storing data way that limits the ability for us-based entities impact their privacy []. there are many other regulations other sectors regarding user's data privacy. customers should able evaluate that our software implemented correctly, resistant attack vectors (known unknown), secure, and otherwise fulfills all the customers' requirements. open source software provides the level transparency and assurance needed prove that the behaviors the system are advertised. decentralization informally, decentralized application service that has single operator. furthermore, single entity should solely responsible for the cost associated with running the service able cause service interruption for other users. one the main motivations for preferring decentralization drive down infrastructure costs for maintenance, utilities, and bandwidth. believe that there are significant underutilized resources the edge the network for many smaller operators. our ex- chapter storj design constraints perience building decentralized storage networks, have found long tail resources that are presently unused underused that could provide affordable and geographically distributed cloud storage. conceivably, some small operator might have access less-expensive electricity than standard data centers another small operator could have access less-expensive cooling. many these small operator environments are not substantial enough run entire datacenter-like storage system. for example, perhaps small business home network attached storage (nas) operator has enough excess electricity run ten hard drives but not more. have found that aggregate, enough small operator environments exist such that their combination over the internet constitutes significant opportunity and advantage for less-expensive and faster storage. our decentralization goals for fundamental infrastructure, such storage, are also driven our desire provide viable alternative the few major centralized storage entities who dominate the market present. believe that there exists inherent risk trusting single entity, company, organization with significant percentage the world's data. fact, believe that there implicit cost associated with the risk trusting any third party with custodianship personal data. some possible costly outcomes include changes the company's roadmap that could result the product becoming less useful, changes the company's position data collection that could cause sell customer metadata advertisers, even the company could out business otherwise fail keep customer data safe. creating equivalent better decentralized system, many users concerned about single-entity risk will have viable alternative. with decentralized architecture, storj could cease operating and the data would continue available. have decided adopt decentralized architecture because, despite the tradeoffs, believe decentralization better addresses the needs cloud storage and resolves many core limitations, risks, and cost factors that result from centralization. within this context, decentralization results globally distributed network that can serve wide range storage use cases from archival cdn. however, centralized storage systems require different architectures, implementations, and infrastructure address each those same use cases. marketplace and economics public cloud computing, and public cloud storage particular, has proven attractive business model for the large centralized cloud providers. cloud computing estimated billion dollar market and expected reach billion []. the public cloud storage model has provided compelling economic model end users. not only does enable end users scale demand but also allows them avoid the significant fixed costs facilities, power, and data center personnel. public chapter storj design constraints cloud storage has generally proven economical, durable, and performant option for many end users when compared on-premise solutions. however, the public cloud storage model has, its nature, led high degree concentration. fixed costs are born the network operators, who invest billions dollars building out network data centers and then enjoy significant economies scale. the combination large upfront costs and economies scale means that there extremely limited number viable suppliers public cloud storage (arguably, fewer than five major operators worldwide). these few suppliers are also the primary beneficiaries the economic return. believe that decentralized storage can provide viable alternative centralized cloud. however, encourage partners customers bring data the network, the price charged for storage and bandwidth--combined with the other benefits decentralized storage--must more compelling and economically beneficial than competing storage solutions. our design storj, seek create economically advantageous situation for four different groups: end users must provide the same economically compelling characteristics public cloud storage with upfront costs and scale demand. addition, end users must experience meaningfully better value for given levels capacity, durability, security, and performance. storage node operators must economically attractive for storage node operators help build out the network. they must paid fairly, transparently, and able make reasonable profit relative any marginal costs they incur. should economically advantageous storage node operator not only utilizing underused capacity but also creating new capacity, that can grow the network beyond the capacity that currently exists. since node availability and reliability has large impact network availability, cost, and durability, required that storage node operators have sufficient incentive maintain reliable and continuous connections the network. demand providers must economically attractive for developers and businesses drive customers and data onto the storj network. must design the system fairly and transparently deliver margin partners. believe that there unique opportunity provide open-source software (oss) companies and projects, which drive over two-thirds the public cloud workloads today without receiving direct revenue, source sustainable revenue. network operator sustain continued investment code, functionality, network maintenance, and demand generation, the network operator, currently storj labs, inc., must able retain reasonable profit. the operator must maintain this profit while not only charging end users less than the public cloud providers but also margin sharing with storage node operators and demand providers. additionally, the network must able account for ensuring efficient, timely billing and payment processes well regulatory compliance for tax and other reporting. globally versatile possible with payments, our network must robust accom- chapter storj design constraints modate several types transactions (such cryptocurrency, bank payments, and other forms barter). lastly, the storj roadmap must aligned with the economic drivers the network. new features and changes the concrete implementations framework components must driven applicability specific object storage use cases and the relationship between features and performance the price storage and bandwidth relative those use cases. amazon compatibility the time this paper's publication, the most widely deployed public cloud amazon web services []. amazon web services not only the largest cloud services ecosystem but also has the benefit first mover advantage. amazon's first cloud services product was amazon simple storage service, amazon for short. public numbers are hard come but amazon likely the most widely deployed cloud storage protocol existence. most cloud storage products provide some form compatibility with the amazon application program interface (api) architecture. our objective aggressively compete the wider cloud storage industry and bring decentralized cloud storage into the mainstream. until decentralized cloud storage protocol becomes widely adopted, amazon compatibility creates graceful transition path from centralized providers alleviating many switching costs for our users. achieve this, the storj implementation allows applications previously built against amazon work with storj with minimal friction changes. compatibility adds aggressive requirements for feature set, performance, and durability. bare minimum, this requires the methods described figure implemented. bucket operations createbucket bucketname deletebucket bucketname listbuckets object operations getobject bucketname objectpath offset length putobject bucketname objectpath data metadata deleteobject bucketname objectpath listobjects bucketname prefix startkey limit delimiter figure minimum api durability, device failure, and churn storage platform useless unless also functions retrieval platform. for any storage platform valuable, must careful not lose the data was given, even the chapter storj design constraints presence variety possible failures within the system. our system must store data with high durability and have negligible risk data loss. for all devices, component failure guarantee. all hard drives fail after enough wear and servers providing network access these hard drives will also eventually fail. network links may die, power failures could cause havoc sporadically, and storage media become unreliable over time. data must stored with enough redundancy recover from individual component failures. perhaps more importantly, data can left single location indefinitely. such environment, redundancy, data maintenance, repair, and replacement lost redundancy must considered inevitable, and the system must account for these issues. furthermore, decentralized systems are susceptible high churn rates where participants join the network and then leave for various reasons, well before their hardware has actually failed. for instance, rhea al. found that many real world peer-to-peer systems, the median time participant lasts the network ranges from hours mere minutes []. maymounkov al. found that the probability node staying connected decentralized network for additional hour increasing function uptime (figure []). other words, nodes that have been online for long time are less likely contribute overall node churn. churn could caused any number factors. storage nodes may offline due hardware software failure, intermittent internet connectivity, power loss, complete disk failure, software shutdown removal. the more network churn that exists, the more redundancy required make for the greater rate node loss. the more redundancy that required, the more bandwidth needed for correct operation the system. fact, there tight relationship between network churn, additional redundancy, and bandwidth availability []. keep background bandwidth usage and redundancy low, our network must have low network churn and strong incentive favor long-lived, stable nodes. see section and blake al. for discussion how repair bandwidth varies function node churn. latency decentralized storage systems can potentially capitalize massive opportunities for parallelism. some these opportunities include increased transfer rates, processing capabilities, and overall throughput even when individual network links are slow. however, parallelism cannot, itself, improve latency. individual network link utilized part operation, its latency will the lower bound for the overall operation. therefore, any distributed system intended for high performance applications must continuously and aggressively optimize for low latency not only individual process scale but also for the system's entire architecture. chapter storj design constraints figure probability remaining online additional hour function uptime. the axis represents minutes. the axis shows the fraction nodes that stayed online least minutes that also stayed online least minutes. source: maymounkov al. bandwidth global bandwidth availability increasing year after year. unfortunately, access highbandwidth internet connections unevenly distributed across the world. while some users can easily access symmetric, high-speed, unlimited bandwidth connections, others have significant difficulty obtaining the same type access. the united states and other countries, the method which many residential internet service providers (isps) operate presents two specific challenges for designers decentralized network protocol. the first challenge the asymmetric internet connections offered many isps. customers subscribe internet service based advertised download speed, but the upload speed potentially order magnitude two slower. the second challenge that bandwidth sometimes "capped" the isp fixed amount allowed traffic per month. for example, many markets, the isp comcast imposes one terabyte per month bandwidth cap with stiff fines for customers who over this limit []. internet connection with cap tb/month cannot average more than kb/s over the month without exceeding the monthly bandwidth allowance, even the isp advertises speeds mb/s higher. such caps impose significant limitations the bandwidth available the network any given moment. with device failure and churn guaranteed, any decentralized system will have corresponding amount repair traffic. result, important account for the bandwidth required not only for data storage and retrieval but also for data maintenance and repair []. designing storage system that careless with bandwidth usage would not only give undue preference storage node operators with access unlimited high-speed bandwidth but also centralize the system some degree. order keep the storage system decentralized possible and working many environments possible, bandwidth usage must aggressively minimized. chapter storj design constraints please see section for discussion how bandwidth availability and repair traffic limit usable space. object size can broadly classify large storage systems into two groups average object size. differentiate between the two groups, classify "large" file few megabytes greater size. database the preferred solution for storing many small pieces information, whereas object store file system ideal for storing many large files. the initial product offering storj labs designed function primarily decentralized object store for larger files. while future improvements may enable database-like use cases, object storage the predominant initial use case described this paper. made protocol design decisions with the assumption that the vast majority stored objects will larger. while smaller files are supported, they may simply more costly store. worth noting that this will not negatively impact use cases that require reading lots files smaller than megabyte. users can address this with packing strategy aggregating and storing many small files one large file. the protocol supports seeking and streaming, which will allow users download small files without requiring full retrieval the aggregated object. byzantine fault tolerance unlike centralized solutions like amazon storj operates untrusted environment where individual storage providers are not necessarily assumed trustworthy. storj operates over the public internet, allowing anyone sign become storage provider. adopt the byzantine, altruistic, rational (bar) model discuss participants the network. byzantine nodes may deviate arbitrarily from the suggested protocol for any reason. some examples include nodes that are broken nodes that are actively trying sabotage the protocol. general, byzantine node bad actor, one that optimizes for utility function that independent the one given for the suggested protocol. inevitable hardware failures aside, altruistic nodes are good actors and participate proposed protocol even the rational choice deviate. rational nodes are neutral actors and participate deviate only when their net best interest. some distributed storage systems (e.g. datacenter-based cloud object storage systems) operate environment where all nodes are considered altruistic. for example, absent chapter storj design constraints hardware failure security breaches, amazon's storage nodes will not anything besides what they were explicitly programmed do, because amazon owns and runs all them. contrast, storj operates environment where every node managed its own independent operator. this environment, can expect that majority storage nodes are rational and minority are byzantine. storj assumes altruistic nodes. must include incentives that encourage the network ensure that the rational nodes the network (the majority operators) behave similarly possible the expected behavior altruistic nodes. likewise, the effects byzantine behavior must minimized eliminated. note that creating system that robust the face byzantine behavior does not require byzantine fault tolerant consensus protocol--we avoid byzantine consensus. see sections and appendix for more details. coordination avoidance growing body distributed database research shows that systems that avoid coordination wherever possible have far better throughput than systems where subcomponents are forced coordinate achieve correctness [-]. use bailis al.'s informal definition that coordination the requirement that concurrently executing operations synchronously communicate otherwise stall order complete []. this observation happens all scales and applies not only distributed networks but also concurrent threads execution coordinating within the same computer. soon coordination needed, actors the system will need wait for other actors, and waiting--due coordination issues--can have significant cost. while many types operations network may require coordination (e.g., operations that require linearizability ]), choosing strategies that avoid coordination (such highly available transactions []) can offer performance gains two three orders magnitude over wide area networks. fact, carefully avoiding coordination much possible, the anna database able times faster than both cassandra and redis their corresponding environments and times faster than performance-focused in-memory databases such masstree intel's tbb []. not all coordination can avoided, but new frameworks (such invariant confluence the calm principle allow system architects understand when coordination required for consistency and correctness. evidenced anna's performance successes, most efficient avoid coordination where possible. systems that minimize coordination are much better scaling from small large workloads. adding more resources coordination-avoidant system will directly linearizable operations are atomic operations specific object where the order operations equivalent the order given original "wall clock" time. chapter storj design constraints crease throughput and performance. however, adding more resources coordinationdependent system (such bitcoin even raft []) will not result much additional throughput overall performance. get exabyte scale, minimizing coordination one the key components our strategy. surprisingly, many decentralized storage platforms are working towards architectures that require significant amounts coordination, where most not all operations must accounted for single global ledger. for achieve exabyte scale, fundamental requirement limit hotpath coordination domains small spheres which are entirely controllable each user. this limits the applicability blockchain-like solutions for our use case. framework after having considered our design constraints, this chapter outlines the design framework consisting only the most fundamental components. the framework describes all the components that must exist satisfy our constraints. long our design constraints remain constant, this framework will, much feasible, describe storj both now and ten years from now. while there will some design freedom within the framework, this framework will obviate the need for future rearchitectures entirely, independent components will able replaced without affecting other components. framework overview all designs within our framework will the following things: store data when data stored with the network, client encrypts and breaks into multiple pieces. the pieces are distributed peers across the network. when this occurs, metadata generated that contains information where find the data again. retrieve data when data retrieved from the network, the client will first reference the metadata identify the locations the previously stored pieces. then the pieces will retrieved and the original data will reassembled the client's local machine. maintain data when the amount redundancy drops below certain threshold, the necessary data for the missing pieces regenerated and replaced. pay for usage unit value should sent exchange for services rendered. improve understandability, break the design into collection eight independent components and then combine them form the desired framework. the individual components are: storage nodes peer-to-peer communication and discovery redundancy metadata encryption audits and reputation data repair payments chapter framework storage nodes the storage node's role store and return data. aside from reliably storing data, nodes should provide network bandwidth and appropriate responsiveness. storage nodes are selected store data based various criteria: ping time, latency, throughput, bandwidth caps, sufficient disk space, geographic location, uptime, history responding accurately audits, and forth. return for their service, nodes are paid. because storage nodes are selected via changing variables external the protocol, node selection explicit, non-deterministic process our framework. this means that must keep track which nodes were selected for each upload via small amount metadata; can't select nodes for storing data implicitly deterministically system like dynamo []. with gfs [], hdfs [], lustre [], this decision implies the requirement metadata storage system keep track selected nodes (see section .). peer-to-peer communication and discovery all peers the network communicate via standarized protocol. the framework requires that this protocol: provides peer reachability, even the face firewalls and nats where possible. this may require techniques like stun [], upnp [], nat-pmp [], etc. provides authentication s/kademlia [], where each participant cryptographically proves the identity the peer with whom they are speaking avoid man-inthe-middle attacks. provides complete privacy. cases such bandwidth measurement (see section .), the client and storage node must able communicate without any risk eavesdroppers. the protocol should ensure that all communications are private default. additionally, the framework requires way look peer network addresses unique identifier that, given peer's unique identifier, any other peer can connect it. this responsibility similar the internet's standard domain name system (dns) [], which mapping identifier ephemeral connection address, but unlike dns, there can centralized registration process. achieve this, network overlay, such chord [], pastry [], kademlia [], can built top our chosen peer-to-peer communication protocol. see section for implementation details. redundancy assume that any moment, any storage node could offline permanently. our redundancy strategy must store data way that provides access the data with high chapter framework probability, even though any given number individual nodes may offline state. achieve specific level durability (defined the probability that data remains available the face failures), many products this space use simple replication. unfortunately, this ties durability the network expansion factor, which the storage overhead for reliably storing data. this significantly increases the total cost relative the stored data. for example, suppose certain desired level durability requires replication strategy that makes eight copies the data. this yields expansion factor this data then needs stored the network, using bandwidth the process. thus, more replication results more bandwidth usage for fixed amount data. discussed the protocol design constraints (section and blake al. [], high bandwidth usage prevents scaling, this undesirable strategy for ensuring high degree file durability. alternative simple replication, erasure codes provide much more efficient method achieve redundancy. erasure codes are well-established use for both distributed and peer-to-peer storage systems [-]. erasure codes are encoding scheme for manipulating data durability without tying bandwidth usage, and have been found improve repair traffic significantly over replication []. importantly, they allow changes durability without changes expansion factor. erasure code often described two numbers, and block data encoded with (k, erasure code, there are total generated erasure shares, where only any them are required recover the original block data. block data bytes, each the erasure shares roughly s/k bytes. besides the case when (replication), all erasure shares are unique. interestingly, the durability erasure code better than erasure code, even though the expansion factor (x) the same for both. this because the risk spread across more nodes the case. these considerations make erasure codes important part our general framework. better understand how erasure codes increase durability without increasing expansion factors, the following table shows various choices and along with the expansion factor and associated durability: exp. factor p(d chapter framework contrast, replication requires significantly higher expansion factors for the same durability. the following table shows durability with replication scheme: exp. factor p(d see how these tables were calculated, we'll start with the simplifying assumption that the monthly node churn rate (that is, the fraction nodes that will offline month average). mathematically, time-dependent processes are modeled according the poisson distribution, where assumed that events are observed the given unit time. result, model durability the cumulative distribution function (cdf) the poisson distribution with mean pn, where expect pieces the file lost monthly. estimate durability, consider the cdf looking the probability that most pieces the file are lost month and the file can still rebuilt. the cdf given by: p(d) e-l n-k the expansion factor still plays big role durability, seen the following table: exp. factor p(d being able tweak the durability independently the expansion factor, erasure coding allows very high durability achieved with surprisingly low expansion factors. because how limited bandwidth resource, completely eliminating replication strategy and using erasure codes only for redundancy causes drastic decrease bandwidth footprint. erasure coding also results storage nodes getting paid more. high expansion factors dilute the incoming funds per byte across more storage nodes; therefore, low expansion factors, such those provided erasure coding, allow for much more direct passthrough income storage node operators. chapter framework erasure codes' effect streaming erasure codes are used many streaming contexts such audio cds and satellite communications [], it's important point out that using erasure coding general does not make our streaming design requirement (required amazon compatibility, see section more challenging. whatever erasure code chosen for our framework, with cds, streaming can added top encoding small portions time, instead attempting encode file all once. see section for more details. erasure codes' effect long tails erasure codes enable enormous performance benefit, which the ability avoid waiting for "long-tail" response times []. long-tail response occurs situations where needed server has unreasonably slow operation time due confluence unpredictable factors. long-tail responses are so-named due their rare average rate occurrence but highly variable nature, which probability density graph looks like "long tail." aggregate, long-tail responses are big issue distributed system design. mapreduce, long-tail responses are called "stragglers." mapreduce executes redundant requests called "backup tasks" make sure that specific stragglers take too long, the overall operation can still proceed without waiting. the backup task mechanism disabled mapreduce, basic operations can take longer complete, even though the backup task mechanism causing duplicated work []. using erasure codes, are position create mapreduce-like backup tasks for storage for uploads, file can encoded higher (k, ratio than necessary for desired durability guarantees. during upload, after enough pieces have uploaded gain required redundancy, the remaining additional uploads can canceled. this cancellation allows the upload continue fast the fastest nodes set, instead waiting for the slowest nodes. downloads are similarly improved. since more redundancy exists than needed, downloads can served from the fastest peers, eliminating wait for temporarily slow offline peers. the outcome that every request satisfiable the fastest nodes participating any given transaction, without needing wait for slower subset. focusing operations where the result only dependent the fastest nodes random subpopulation turns what could potential liability (highly variable performance from individual actors) into great source strength for distributed storage network, while still providing great load balancing characteristics. this ability over-encode file greatly assists dynamic load balancing popular content the network. see section for discussion how plan address load balancing very active files. chapter framework figure various outcomes during upload and download metadata once split object with erasure codes and select storage nodes which store the new pieces, now need keep track which storage nodes selected. allow users choose storage based geographic location, performance characteristics, available space, and other features. therefore, instead implicit node selection such scheme using consistent hashing like dynamo [], must use explicit node selection scheme such directory-based lookups []. additionally, maintain amazon compatibility, the user must able choose arbitrary key, often treated like path, identify this mapping data pieces node. these features imply the necessity metadata storage system. amazon compatibility once again imposes some tight requirements. should support: hierarchical objects (paths with prefixes), per-object key/value storage, arbitrarily large files, arbitrarily large amounts files, and forth. objects should able stored and retrieved arbitrary key; addition, deterministic iteration over those keys will required allow for paginated listing. every time object added, edited, removed, one more entries this metadata storage system will need adjusted. result, there could heavy churn this metadata system, and across the entire userbase the metadata itself could end being sizable. for example, suppose few years the network stores one total exabyte data, where the average object size and our erasure code selected such that one exabyte objects billion objects. this metadata system will need keep track which nodes were selected for each object. each metadata element roughly chapter framework bytes (info for each selected node plus the path and some general overhead), there are over terabytes metadata which keep track. fortunately, the metadata can heavily partitioned the user. user storing terabytes megabyte objects will only incur metadata overhead gigabytes. it's worth pointing out that these numbers vary heavily with object size: the larger the average object size, the less the metadata overhead. additional framework focus enabling this component--metadata storage--to interchangeable. specifically, expect the platform incorporate multiple implementations metadata storage that users will allowed choose between. this greatly assists with our design goal coordination avoidance between users (see section .). aside from scale requirements, implement amazon compatibility, the desired api straightforward and simple: put (store metadata given path), get (retrieve metadata given path), list (paginated, deterministic listing existing paths), and delete (remove path). see figure for more details. encryption regardless storage system, our design constraints require total security and privacy. all data metadata will encrypted. data must encrypted early possible the data storage pipeline, ideally before the data ever leaves the source computer. this means that amazon s-compatible interface appropriate similar client library should run colocated the same computer the user's application. encryption should use pluggable mechanism that allows users choose their desired encryption scheme. should also store metadata about that encryption scheme allow users recover their data using the appropriate decryption mechanism cases where their encryption choices are changed upgraded. support rich access management features, the same encryption key should not used for every file, having access one file would result access decryption keys for all files. instead, each file should encrypted with unique key. this should allow users share access certain selected files without giving encryption details for others. because each file should encrypted differently with different keys and potentially different algorithms, the metadata about that encryption must stored somewhere manner that secure and reliable. this metadata, along with other metadata about the file, including its path, will stored the previously discussed metadata storage system, encrypted deterministic, hierarchical encryption scheme. hierarchical encryption scheme based bip will allow subtrees shared without sharing their parents and will allow some files shared without sharing other files. see section for discussion our path-based hierarchical deterministic encryption scheme. chapter framework audits and reputation incentivizing storage nodes accurately store data paramount importance the viability this whole system. essential able validate and verify that storage nodes are accurately storing what they have been asked store. many storage systems use probabilistic per-file audits, called proofs retrievability, way determining when and where repair files are extending the probabilistic nature common per-file proofs retrievability range across all possible files stored specific node. audits, this case, are probabilistic challenges that confirm, with high degree certainty and low amount overhead, that storage node wellbehaved, keeping the data claims, and not susceptible hardware failure malintent. audits function "spot checks" help calculate the future usefulness given storage node. our storage system, audits are simply mechanism used determine node's degree stability. failed audits will result storage node being marked bad, which will result redistributing data new nodes and avoiding that node altogether the future. storage node uptime and overall health are the primary metrics used determine which files need repair. the case with proofs retrievability this auditing mechanism does not audit all bytes all files. this can leave room for false positives, where the verifier believes the storage node retains the intact data when has actually been modified partially deleted. fortunately, the probability false positive individual partial audit easily calculable (see section .). when applied iteratively storage node whole, detection missing altered data becomes certain within known and modifiable error threshold. reputation system needed persist the history audit outcomes for given node identities. our overall framework has flexible requirements the use such system, but see section for discussion our initial approach. data repair data loss ever-present risk any distributed storage system. while there are many potential causes for file loss, storage node churn (storage nodes joining and leaving the network) the largest leading risk significant degree compared other causes. discussed section network session time many real world systems range from hours mere minutes []. while there are many other ways data might get lost, such corruption, malicious behavior, bad hardware, software error, user initiated space reclamation, these issues are less serious than full node churn. expect node churn the dominant cause data loss our network. because audits are validating that conforming nodes store data correctly, all that re- chapter framework mains detect when storage node stops storing data correctly goes offline and then repair the data had new nodes. repair the data, will recover the original data via erasure code reconstruction from the remaining pieces and then regenerate the missing pieces and store them back the network new storage nodes. vital our system incentivize storage node participants remain online for much longer than few hours. encourage this behavior, our payment strategy will involve rewarding storage node operators that keep their nodes participating for months and years time. payments payments, value attribution, and billing decentralized networks are critical part maintaining healthy ecosystem both supply and demand. course, decentralized payment systems are still their infancy number ways. for our framework achieve low latency and high throughput, must not have transactional dependencies blockchain (see section .). this means that adequately performant storage system cannot afford wait for blockchain operations. when operations should measured milliseconds, waiting for cluster nodes probabilistically come agreement shared global ledger non-starter. our framework instead emphasizes game theoretic models ensure that participants the network are properly incentivized remain the network and behave rationally get paid. many our decisions are modeled after real-world financial relationships. payments will transferred during background settlement process which well-behaved participants within the network cooperate. storage nodes our framework should limit their exposure untrusted payers until confidence gained that those payers are likely pay for services rendered. addition, the framework also tracks and aggregates the value the consumption those services those who own the data stored the network. charging for usage, the framework able support the end-to-end economics the storage marketplace ecosystem. although the storj network payment agnostic and the protocol does not require specific payment type, the network assumes the ethereum-based storj token the default mechanism for payment. while intend for the storj token the primary form payment, the future other alternate payment types could implemented, including bitcoin, ether, credit debit card, ach transfer, even physical transfer live goats. concrete implementation believe the framework we've described relatively fundamental given our design constraints. however, within the framework there still remains some freedom choosing how implement each component. this section, lay out our initial implementation strategy. expect the details contained within this section change gradually over time. however, believe the details outlined here are viable and support working implementation our framework capable providing highly secure, performant, and durable production-grade cloud storage. with our previous version [], will publish changes this concrete architecture through our storj improvement proposal process []. definitions the following defined terms are used throughout the description the concrete implementation that follows: actors client user application that will upload download data from the network. peer class cohesive collection network services and responsibilities. there are three different peer classes that represent services our network: storage nodes, satellites, and uplinks. storage node this peer class participates the node discovery system, stores data for others, and gets paid for storage and bandwidth. uplink this peer class represents any application service that implements libuplink and wants store and/or retrieve data. this peer class not expected remain online like the other two classes and relatively lightweight. this peer class performs encryption, erasure encoding, and coordinates with the other peer classes behalf the customer/client. libuplink library which provides all necessary functions interact with storage nodes and satellites directly. this library will available number different programming languages. gateway service which provides compatibility layer between other object storage services such amazon and libuplink exposing amazon s-compatible api. uplink cli command line interface for uploading and downloading files from the network, managing permissions and sharing, and managing accounts. satellite this peer class participates the node discovery system, caches node address information, stores per-object metadata, maintains storage node reputation, aggre- chapter concrete implementation gates billing data, pays storage nodes, performs audits and repair, and manages authorization and user accounts. users have accounts and trust specific satellites. any user can run their own satellite, but expect many users elect avoid the operational complexity and create account another satellite hosted trusted third party such storj labs, friend, group, workplace. figure the three different peer classes data bucket bucket unbounded but named collection files identified paths. every file has unique path within bucket. path path unique identifier for file within bucket. path arbitrary string bytes. paths contain forward slashes access control boundaries. forward slashes (referred the path separator) separate path components. example path might videos/carlsagan/gloriousdawn.mp, where the path components are videos, carlsagan, and gloriousdawn.mp. unless otherwise requested, encrypt paths before they ever leave the customer's application's computer. file object file (or object) the main data type our system. file referred path, contains arbitrary amount bytes, and has minimum maximum size. file represented ordered collection one more segments. segments have fixed maximum size. file also supports limited amount key/value userdefined fields called extended attributes. like paths, the data contained file encrypted before ever leaves the client computer. extended attribute extended attribute user defined key/value field that associated with file. like other per-file metadata, extended attributes are stored encrypted. segment segment represents single array bytes, between and user-configurable maximum segment size. see section for more details. remote segment remote segment segment that will erasure encoded and distributed across the network. remote segment larger than the metadata re- chapter concrete implementation figure files, segments, stripes, erasure shares, and pieces chapter concrete implementation quired keep track its bookkeeping, which includes information such the ids the nodes that the data stored on. inline segment inline segment segment that small enough where the data represents takes less space than the corresponding data remote segment will need keep track which nodes had the data. these cases, the data stored "inline" instead being stored nodes. stripe stripe further subdivision segment. stripe fixed amount bytes that used encryption and erasure encoding boundary size. erasure encoding happens stripes individually, whereas encryption may happen small multiple stripes time. all segments are encrypted, but only remote segments erasure encode stripes. stripe the unit which audits are performed. see section for more details. erasure share when stripe erasure encoded, generates multiple pieces called erasure shares. only subset the erasure shares are needed recover the original stripe. each erasure share has index identifying which erasure share (e.g., the first, the second, etc.). piece when remote segment's stripes are erasure encoded into erasure shares, the erasure shares for that remote segment with the same index are concatenated together, and that concatenated group erasure shares called piece. there are erasure shares after erasure encoding stripe, then there are pieces after processing remote segment. the ith piece the concatenation all the ith erasure shares from that segment's stripes. see section for more details. pointer pointer data structure that either contains the inline segment data, keeps track which storage nodes the pieces remote segment were stored on, along with other per-file metadata. peer classes our overall strategy extends from our previous version and also heavily mirrors distributed storage systems such the google file system (and other gfs-like systems and the lustre distributed file system []. every case, there are three major actors the network: metadata servers, object storage servers, and clients. object storage servers hold the bulk the data stored the system. metadata servers keep track per-object metadata and where the objects are located object storage servers. clients provide coherent view and easy access files communicating with both the metadata and object storage servers. lustre's architecture proven for high performance. the majority the top fastest supercomputers use lustre for their high-performance, scalable storage []. while don't expect achieve equal performance over wide-area network, expect dramatically better performance than other architectures. any limitation, any, experience performance will due factors besides our overall architecture. our previous version used different names for each component. what previously chapter concrete implementation referred storj share, now refer simply storage nodes. our formerly centralized single bridge instance can now run anyone and referred satellite. our libstorj library will made backwards compatible where possible, but now refer client software uplinks. storage node the main duty the storage node reliably store and return data. node operators are individuals entities that have excess hard drive space and want earn income renting their space others. these operators will download, install, and configure storj software locally, with account required anywhere. they will then configure disk space and per-satellite bandwidth allowance. during node discovery, storage nodes will advertise how much bandwidth and hard drive space available, and their designated storj token wallet address. simplify lifecycle management for ephemeral files, storage nodes also keep track optional per-piece "time-to-live", ttl, designations. pieces may stored with specific ttl expiry where data expected deleted after the expiration date. ttl provided, data expected stored indefinitely. this means storage nodes have database expiration times and must occasionally clear out old data. storage nodes must additionally keep track signed bandwidth allocations (see section send satellites for later settlement and payment. this also requires small database. both ttl and bandwidth allocations are stored sqlite database. storage nodes can choose with which satellites work. they work with multiple satellites (the default behavior), then payment may come from multiple sources varying payment schedules. storage nodes are paid specific satellites for returning data when requested the form egress bandwidth payment, and for storing data rest. storage nodes are expected reliably store all data sent them and are paid with the assumption that they are faithfully storing all data. storage nodes that fail random audits will removed from the pool, can lose funds held escrow cover additional costs, and will receive limited future payments. storage nodes are not paid for the initial transfer data store (ingress bandwidth). this discourage storage nodes from deleting data only paid for storing more, which became problem with our previous version []. while storage nodes are paid for repair egress bandwidth usage, some satellites may opt pay less than normal retrieval egress bandwidth usage. storage nodes are not paid for node discovery any other maintenance traffic. storage nodes will support three methods: get, put, and delete. each method will take piece id, satellite id, signature from the associated satellite instance, and bandwidth allocation (see section .). the satellite forms namespace. identical piece with different satellite refers different piece. registration with us- tax form service may required. see section chapter concrete implementation the put operation will take stream bytes and optional ttl and store the bytes such that any subrange bytes can retrieved again via get operation. get operations are expected work until the ttl expires (if ttl was provided) until delete operation received, whichever comes first. storage nodes will allow administrators configure maximum allowed disk space and per-satellite bandwidth usage over the last rolling days. they will keep track how much remaining both, and reject operations that not have valid signature from the appropriate satellite. the storage node being developed and will released open source software. node identity during setup, storage nodes, satellites, and uplinks all generate their own identity and certificates for use the network. this node used for node discovery and routing. each node will operate its own certificate authority, which requires public/private key pair and self-signed certificate. the certificate authority's private key will ideally kept cold storage prevent key compromise. it's important that the certificate authority private key managed with good operational security because key rotation for the certificate authority will require brand new node id. figure the different keys and certificates that compose storage node's overall identity. each row represents private/public key pair. the public key the node's certificate authority determines its node id. s/kademlia [], the node will the hash the public key and will serve proof work for joining the network. unlike bitcoin's proof work [], the proof work will dependent how many trailing zero bits one can find the hash output. this means that the node (which may end with number trailing zero bits) will still usable bal- chapter concrete implementation anced kademlia tree. this cost meant make sybil attacks prohibitively expensive and time consuming. each node will have revocable leaf certificate and key pair that signed the node's certificate authority. nodes use the leaf key pair for communication. each leaf has signed timestamp that satellites keep track per node. should the leaf become compromised, the node can issue new leaf with later timestamp. interested peers will make note newly seen leaf timestamps and reject connections from nodes with older leaf certificates. optimized special case, peers will not need make note when the leaf certificate and certificate authority share the same timestamp. peer-to-peer communication initially, are using the grpc protocol top the transport layer security protocol (tls) top the utp transport protocol with added session traversal utilities for nat (stun) functionality []. stun provides nat traversal; utp provides reliable, ordered delivery (like tcp would) with ledbat functionality; tls provides privacy and authentication; and grpc provides multiplexing and convenient programmer interface. ledbat allows competing internet traffic take priority, providing more graceful user experience home operators with less network usage interference. over time, will replace tls with more flexible secure transport framework (such the noise protocol framework []) reduce round trips due connection handshakes situations where the data already encrypted and forward secrecy isn't necessary. when using authenticated communication such tls noise, every peer can ascertain the the node with which speaking validating the certificate chain and hashing its peer's certificate authority's public key. can then estimated how much work went into constructing the node considering the number trailing zero bits the end the id. satellites can configure minimum proof work required pass audit (section such that, over time, the network will require greater proofs work due natural user intervention. for the few cases where node cannot achieve successful connection through nat firewall (via stun [], upnp [], natpmp [], similar techniques), manual intervention and port forwarding will required. the future, nodes unable create connection through their firewalls may rely traffic proxying from other, more available nodes, for fee. nodes can also provide assistance other nodes for initial stun setup, public address validation, and forth. node discovery this point, have storage nodes and have means identify and communicate with them know their address. must account for the fact that storage nodes will chapter concrete implementation often consumer internet connections and behind routers with constantly changing addresses. therefore, the node discovery system's goal provide means look node's latest address node id, somewhat similar the role dns provides for the public internet. the kademlia distributed hash table (dht) key/value store with built-in node lookup protocol. utilize kademlia our primary source truth for dns-like functionality for node lookup, while ignoring the key/value storage aspects kademlia. using only kademlia for node lookup eliminates the need for some other functionality kademlia would otherwise require such owner-based key republishing, neighbor-based key republishing, storage and retrieval values, and forth. furthermore, avoid number other known attacks using the s/kademlia extensions where appropriate. unfortunately, dhts such kademlia require multiple network round trips for many operations, which makes difficult achieve millisecond-level response times. solve this problem, add basic decentralized caching service top kademlia. the caching service will live independently each satellite and attempt talk every storage node the network ongoing basis, perhaps once per hour. the caching service will then cache the last known good address for each node, and evict nodes that hasn't talked after certain period time. storage nodes will not need extended know about these caching services. expect this scale for the reasonable future, ping operations are inexpensive, but admit new solution may ultimately necessary. fortunately, space requirements are negligible. for instance, caching addresses for network nodes can done with only memory. based this design, each satellite's cache will not expected primary source truth, and results the cache may stale. however, due our redundant storage strategy, the storage network will resilient against expected degree node churn and staleness. therefore, the system will robust even some lookups the cache fail return incorrect addresses. furthermore, because our peer-to-peer communication system already provides peer authentication, node discovery cache that sometimes returns faulty deliberately misleading address lookup responses can only cause loss performance but not correctness. although the satellite caches are not the primary source truth, because repair (section requires rapid determination whether node online offline, lookups our system will stop with the cache lookup and will not attempt another lookup using kademlia. only after cases failed audit requests will fallback, nonconcurrent lookup kademlia performed correct for potentially stale cache information. addition being included every satellite, plan host and help set some well-known community-run node discovery caches. these caches will perform the duty quickly returning address information for given node the node has been online recently. this assuming ordered in-memory list -tuples node bytes), address bytes for ipv), port bytes), and timestamp bytes). .mb chapter concrete implementation kademlia messages will use our peer-to-peer communication protocol (section .), which includes confidentiality and peer identification. because this requires cryptographic setup, connections kademlia neighbors and frequent contacts will cached where possible. with each kademlia message shared the network, nodes will include their available disk space, per-satellite bandwidth availability, storj wallet address, and any other metadata the network needs. the node discovery cache will collect this information provided the nodes, allowing faster lookups for it. mitigating sybil attacks while we've adopted the proof-of-work scheme s/kademlia proposes partially address sybil attacks, extend kademlia with application specific integration further defend our network. given two storage nodes, and storage node not allowed enter storage node a's routing table until storage node can present signed message from satellite that storage node trusts claiming that has passed enough audits that trusts (sections and .). this ensures that only nodes with verified disk space have the opportunity participate the routing layer. node that allowed enter routing tables considered vetted and lookups only progress through vetted nodes. make sure unvetted nodes can still found, vetted nodes keep unbounded lists their unvetted neighbors provided that the xor distance all unvetted neighbors farther than the farthest the k-closest vetted neighbors. unvetted nodes keep their k-nearest vetted nodes up-to-date. redundancy use the reed-solomon erasure code []. implement our solution for reducing the effects long-tails (see section ..), choose numbers for each object that store, and such that path encryption enabled default but otherwise optional, encrypted paths make efficient sorted path listing challenging. when path encryption use perbucket feature), objects are sorted their encrypted path name, which deterministic but otherwise relatively unhelpful when the client application interested sorted, unencrypted paths. for this reason, users can opt out path encryption. when path encryption disabled, unencrypted paths are only revealed the user's chosen satellite, but not the storage nodes. storage nodes continue have information about the path and metadata the pieces they store. authorization encryption protects the privacy data while allowing for the identification tampering, but authorization allows for the prevention tampering disallowing clients from making unauthorized edits. users who are authorized will able add, remove, and edit files, while users who are not authorized will not have those abilities. metadata operations will authorized. users will authenticate with their satellite, which will allow them access various operations according their authorization configuration. our initial metadata authorization scheme uses macaroons []. macaroons are type bearer token that authorizes the bearer some restricted resources. macaroons are especially interesting that they allow for rich contextual decentralized delegation. other words, they provide the property that anyone can add restrictions way which those restrictions cannot later removed, without coordination with central party. use macaroons restrict which operations can applied and which encrypted paths they can applied. this way, macaroons provide mechanism restrict delegated access specific encrypted path prefixes, specific files, and specific operations, such read only access perhaps append only access. each account has root macaroon and operations are validated against supplied macaroon's set caveats. our macaroons are further caveated with optional expirations and revocation tokens, which allow users revoke macaroons programmatically. because want restrict satellite operations, and satellites only have access en- chapter concrete implementation crypted paths, our authorization scheme must work encrypted paths. for access delegation specific path prefixes, path separation boundaries between path components must remain across encryption. this implies reduced functionality and/or performance for path delimiters other than forward slash. once the uplink authorized with the satellite, the satellite will approve and sign for operations storage nodes, including bandwidth allocations (section .). the uplink must retrieve valid signatures from the satellite prior operations with storage nodes. all operations storage node require specific satellite and associated signature. storage node will reject operations not signed the appropriate satellite id. storage nodes will not allow operations signed one satellite apply objects owned another, unless explicitly granted the owning satellite. our initial implementation does not detect attempt mitigate unexpected file removal rollback misbehaving satellite. our trust model expects that user's satellite well-behaved and stores and repairs data reliably. satellite cannot trusted, unlikely repair data client's behalf anyway. however, future implementation could add more thorough detection for satellite-based file system tampering, via scheme systems such sundr, sirius, plutus [-]. audits network with untrusted nodes, validating that those nodes are returning data accurately and otherwise behaving expected vital ensuring properly functioning system. audits are way confirm that nodes have the data they claim have. auditors, such satellites, will send challenge storage node and expect valid response. challenge request the storage node order prove has the expected data. some distributed storage systems, including the previous version storj [], discuss merkle tree proofs, which audit challenges and expected responses are generated the time storage form proof retrievability []. using merkle tree [], the amount metadata needed store these challenges and responses negligible. proofs retrievability can broadly classified into limited and unlimited schemes []. the merkle tree variety used our previous version one such limited scheme. unfortunately, such scheme, the challenges and expected responses must pregenerated. learned with our previous version, without periodic regeneration these challenges, storage node can begin pass most audits without storing all the requested data keeping track which challenges exist and then saving only the expected responses. during our previous version, began consider reed-solomon erasure coding help solve this problem. assumption our storage system that most storage nodes behave rationally, and incentives are aligned such that most data stored faithfully. long that assumption holds, reed-solomon able detect errors and even correct them, via mechanisms chapter concrete implementation such the berlekamp-welch error correction algorithm are already using reed-solomon erasure coding small ranges (stripes), discussed the hail system [], use erasure coding read single stripe time challenge and then validate the erasure share responses. this allows run arbitrary audits without pre-generated challenges. perform audit, first choose stripe. request that stripe's erasure shares from all storage nodes responsible. then run the berlekamp-welch algorithm across all the erasure shares. when enough storage nodes return correct information, any faulty missing responses can easily identified. given specific storage node, audit might reveal that offline incorrect. the case node being offline, the audit failure may due the address the node discovery cache being stale, another, fresh kademlia lookup will attempted. the node still appears offline, the satellite places the node containment mode. this mode, the satellite will calculate and save the expected response, then continue try the same audit with that node until the node either responds successfully, actively fails the audit, disqualified for being offline too long. once the node responds successfully, leaves containment mode. all audit failures will stored and saved the reputation system. audits additionally serve opportunity test storage node latency, throughput, responsiveness, and uptime. this data will also saved the reputation system. important that every storage node has frequent set random audits gain statistical power how well-behaved that storage node operating. however, discussed section not requirement that audits are performed every byte, even every file. additionally, important that every byte stored the system has equal probability being checked for future audit every other byte the system. see section for discussion how many audits are required confident data stored correctly. data repair storage nodes offline--taking their pieces with them--it will necessary for the missing pieces rebuilt once each segment's pieces fall below the predetermined threshold, node goes offline, the satellite will mark that nodes' file pieces missing. the node discovery system's caches have reasonably accurate and up-to-date information about which storage nodes have been online recently. when storage node changes state from recently online offline, this can trigger lookup reverse index within user's metadata database, identifying all segment pointers that were stored that node. for every segment that drops below the appropriate minimum safety threshold, the segment will downloaded and reconstructed, and the missing pieces will re- chapter concrete implementation generated and uploaded new nodes. finally, the pointer will updated include the new information. users will choose their desired durability with their satellite which may impact price and other considerations. this desired durability (along with statistics from ongoing audits) will directly inform what reed-solomon erasure code choices will made for new and repaired files, and what thresholds will set for when uploads are successful and when repair needed. see sections and for how calculate these values given user inputs. direct implication this design that, for now, the satellite must constantly stay running. the user's satellite stops running, repairs will stop, and data will eventually disappear from the network due node churn. this similar the design how value storing and republishing works kademlia [], which requires the owner stay online. the ingress (or inbound) bandwidth demands the audit and repair system are large, but given standard configuration, the egress (or outbound) demands are relatively small. large amount data comes into the system for audits and repairs, but only the formerly missing pieces are sent back out. while the repair and audit system can run anywhere, the bandwidth usage asymmetry means that hosting providers which offer free ingress make for especially attractive hosting location for users this system. piece hashes data repair ongoing, costly operation that will use significant bandwidth, memory, and processing power, often impacting single operator. result, repair resource usage should aggressively minimized much possible. for repairing segment effective minimizing bandwidth usage, only few pieces needed for reconstruction should downloaded. unfortunately, reed-solomon insufficient its own for correcting errors when only few redundant pieces are provided. instead, piece hashes provide better way confident that we're repairing the data correctly. solve this problem, hashes every piece will stored alongside each piece each storage node. validation hash that the set hashes correct will stored the pointer. during repair, the hashes every piece can retrieved and validated for correctness against the pointer, thus allowing each piece validated its entirety. this allows the repair system correctly assess whether not repair has been completed successfully without using extra redundancy for the same task. chapter concrete implementation storage node reputation reputation metrics decentralized networks are critical part enabling cooperation between nodes where progress would challenging otherwise. reputation metrics are used ensure that bad actors within the network are eliminated participants, improving security, reliability, and durability. storage node reputation can divided into four subsystems. the first subsystem proof work identity system, the second subsystem the initial vetting process, the third subsystem filtering system, and finally, the fourth system preference system. the goal the first system require short proof that the storage node operator invested, through time, stake, resources. initially, are using proof work. mentioned section storage nodes require proof work part identity generation. this helps the network avoid some sybil attacks [], but glossed over how proof work difficulty set. will let satellite operators set per-satellite minimum difficulty required for new data storage. storage node has identity generated with lower difficulty than the satellite's configured minimum, that storage node will not candidate for new data. expect satellite operators naturally increase the minimum proof work difficulty requirements over time until reasonable balance found. the case changing difficulty configuration, satellites will leave existing data existing nodes where possible. other investment proof schemes are possible, such form proof stake proposed our previous work []. the second subsystem slowly allows nodes join the network. when storage node first joins the network, its reliability unknown. result, will placed into vetting process until enough data known about it. propose the following way gather data about new nodes without compromising the integrity the network. every time file uploaded, the satellite will select small number additional unvetted storage nodes include the list target nodes. the reed-solomon parameters will chosen such that these unvetted storage nodes will not affect the durability the file, but will allow the network test the node with small fraction data until are sure the node reliable. after the storage node has successfully stored enough data for long enough period (at least one payment period), the satellite will then start including that storage node the standard selection process used for general uploads. will also give the node signed message claiming that the vetting process completed, and that the storage node may now enter other nodes' routing tables (section ..). importantly, storage nodes get paid during this vetting period, but don't receive much data. the filtering system the third subsystem; blocks bad storage nodes from participating. addition simply not having done sufficient proof work, certain actions storage node can take are disqualifying events. the reputation system will used filter these nodes out from future uploads, regardless where the node the vetting process. actions that are disqualifying include: failing too many audits; failing return data, with reasonable speed; and failing too many uptime checks. chapter concrete implementation storage node disqualified, that node will longer selected for future data storage and the data that node stores will moved new storage nodes. likewise, client attempts download piece from storage node that the node should have stored and the node fails return it, the node will disqualified. importantly, storage nodes will allowed reject and fail put operations without penalty, nodes will allowed choose which satellite operators work with and which data store. it's worth reiterating that failing too many uptime checks disqualifying event. storage nodes can taken down for maintenance, but storage node offline too much, can have adverse impact the network. node offline during audit, that specific audit should retried until the node responds successfully disqualified, prevent nodes from selectively failing respond audits. after storage node disqualified, the node must back through the entire vetting process again. the node decides start over with brand-new identity, the node must restart the vetting process from the beginning (in addition generating new node via the proof work system). this strongly disincentivizes storage nodes from being cavalier with their reputation. the last subsystem preference system. after disqualified storage nodes have been filtered out, remaining statistics collected during audits will used establish preference for better storage nodes during uploads. these statistics include performance characteristics such throughput and latency, history reliability and uptime, geographic location, and other desirable qualities. they will combined into load-balancing selection process, such that all uploads are sent qualified nodes, with higher likelihood uploads preferred nodes, but with non-zero chance for any qualified node. initially, we'll load balancing with these preferences via randomized scheme, such the power two choices [], which selects two options entirely random, and then chooses the more qualified between those two. the storj network, preferential storage node reputation only used select where new data will stored, both during repair and during the upload new files, unlike disqualifying events. storage node's preferential reputation decreases, its file pieces will not moved repaired other nodes. there process planned our system for storage nodes contest their reputation scores. the best interest storage nodes have good uptime, pass audits, and return data. storage nodes that don't these things are not useful the network. storage nodes that are treated satellites unfairly will not accept future data from those satellites. see section about quality control for how plan ensure satellites are incentivized treat storage nodes fairly. initially, storage node reputation will individually determined each satellite. node disqualified one satellite, may still store data for other satellites. reputation will not initially shared between satellites. over time, reputation will determined globally. chapter concrete implementation payments the storj network, payments are made clients who store data the platform the satellite they utilize. the satellites then pays storage nodes for the amount storage and bandwidth they provide the network. payments clients may through any mechanism (storj, credit card, invoice, etc.), but payments storage nodes are via the ethereum-based erc storj token. previous distributed systems have handled payments hard-coded contracts. for example, the previous storj network utilized -day contracts maintain data the network. after that period time, the file was deleted. other distributed storage platforms use -day renewable contracts that delete data the user does not login every days. others use -day contracts. believe that the most common use case indefinite storage. best solve this use case, our network will longer use contracts manage payments and file storage durations. the default assumption that data will last indefinitely. satellites will pay storage nodes for the data they store and for piece downloads. storage nodes will not paid for the initial transfer data, but they will paid for storing the data month-by-month. the end the payment period, satellite will calculate earnings for each its storage nodes. provided the storage node hasn't been disqualified, the storage node will paid the satellite for the data has stored over the course the month, per the satellite's records. satellites have strong incentive prefer long-lived storage nodes. storage node churn too high, satellites will escrow portion storage node's payment until the storage node has maintained good participation and uptime for some minimum amount time, the order greater than half year. storage node leaves the network prematurely, the satellite will reclaim escrowed payments it. storage node misses delete command due the node being offline, will storing more data than the satellite credits for. storage nodes are not paid for storing such file pieces, but will eventually cleaned through the garbage collection process (see section .). this means that storage nodes who maintain higher availability can maximize their profits deleting files request, which minimizes the amount garbage data they store. the satellite maintains database all file pieces responsible for and the storage nodes believes are storing these pieces. each day, the satellite adds another day's worth accounting each storage node for every file piece will storing. satellites will track utilized bandwidth (see section .). the end the month, each satellite adds all bandwidth and storage payments each storage node has earned and makes the payments the appropriate storage nodes. satellites will also earn revenue from account holders for executing audits, repairing segments, and storing metadata. satellites charge per-segment and per-byte cost, addition charging for access and retrieval. per-segment charges cover the cost pointer chapter concrete implementation metadata, whereas per-byte charges cover the cost data maintenance the network. every day, each satellite will execute number audits across all its storage nodes the network. the satellite will charge for both completing audits and repairs, once segments fall below the piece threshold needed for repair. when detected that storage node acts maliciously and does not store files properly maintain sufficient availability, will not paid for the services rendered, and the funds allocated will instead used repair any missing file pieces and pay new storage nodes for storing the data. reduce transaction fees and other overhead much possible, payments will recipient-initiated and must worth least some minimum value. certain satellites may elect use portion the storage nodes' payout cover transaction fees part whole. see the satellite reputation section (section for details how storage nodes will know trust satellites. bandwidth allocation core component our system requires knowing how much bandwidth used between two peers. our previous version [,], used exchange reports gather information about what transpired between two peers. the end operation, both peers would send reports central collection service for settlement. when both peers mutually agreed, was straightforward determine how much bandwidth had been used. when they disagreed, however, resorted data analysis and regression determine which peer had greater propensity for dishonesty effort catch "cheaters" (or, rational nodes). with our new version, want make cheating impossible from the protocol level. solve this problem, turn neuman's proxy-based authorization and accounting for distributed systems []. this accounting protocol more correctly measures resource usage delegated and decentralized way. neuman's accounting protocol, account holder has enough funds cover the operation, account server will create signed, digital check and transfer the account holder. the protocol refers this check proxy, but refer bandwidth allocation. this check contains information identifying the account server, the payer, the payee, the maximum amount resources available used the operation, check number prevent any double spending problems [], and expiration date. our case, the account server the satellite, the payer the uplink, the payee the storage node, and the resource question bandwidth. the satellite will only create bandwidth allocation the uplink authorized for the request. the beginning storage operation, the uplink can transfer the bandwidth allocation storage node. the chapter concrete implementation storage node can validate the satellite's signature and perform the requested operation the allowed bandwidth limit, storing and later sending the bandwidth allocation the satellite for payment. we're further inspired filecoin's off-chain retrieval market, wherein only small amounts data are transferred time []. instead allowing the storage node cheat and save the bandwidth allocation without performing the requested operation, break each operation into smaller requests such that either the storage node uplink stop participating the protocol prematurely, neither peer class exposed too much loss. this similar optimistic, gradual-release, fair-exchange protocol []. support this with neuman's accounting protocol and little satellite overhead, use restricted bandwidth allocations (referred restricted proxies []). neuman's restricted proxies work much like macaroons which further caveats can added way that can't removed, limiting the capabilities the proxy. proxies can use public/private key cryptography, which means that anyone can validate the proxy, instead just the original issuer. because each uplink already has key pair part its identity (section .), use the existing key pair instead creating new key pair for every restriction. restricted bandwidth allocations, our case, are restricted the uplink limit the bandwidth allocation's value only what has transferred far. this way, the storage node will only keep the largest bandwidth allocation has received that point, and the uplink will only send bandwidth allocations that are slightly larger than what has received. the storage node has incentive keep more than the largest allocation, they all share the same "check number," which can only cashed once. the case get operation, assume the satellite-signed bandwidth allocation allows bytes total. the uplink will start sending restricted allocation for some small amount bytes), perhaps only few kilobytes, the storage node can verify the uplink's authorization. the allocation signed correctly, the storage node will transfer the amount listed the restricted allocation bytes) before awaiting another allocation. the uplink will then send another allocation where larger, continuing send allocations for data until has grown the full value. for each transaction, the storage node only sends previously-unsent data, that the storage node only sends bytes total. seen figure pipeline these requests avoid pipeline stall performance penalties. the request terminated any time, either planned unexpectedly, the storage node will keep the largest restricted bandwidth allocation has received. this largest restricted bandwidth allocation the signed confirmation the uplink that the uplink agreed bandwidth usage bytes, along with the satellite's confirmation the uplink's bandwidth allowance the storage node will periodically send the largest restricted bandwidth allocations has received appropriate satellites, which point satellites will pay the storage node for that bandwidth. the uplink can't afford the bandwidth usage, the satellite will not sign bandwidth allocation, protecting the satellite's reputation. likewise, the uplink tries use more chapter concrete implementation figure diagram put operation figure diagram get operation chapter concrete implementation bandwidth than allocated, the storage node will decline the request. the storage node can only get paid for the maximum amount client has agreed to, otherwise has valid bandwidth allocations return for payment. before, don't measure all peer-to-peer traffic. this bandwidth traffic measurement system only tracks bandwidth used during storage operations (storage and retrievals pieces). however, does not apply node discovery traffic (kademlia dht) other generic maintenance overhead. satellite reputation whenever satellite the storj network has less than stellar payment, demand generation, performance history, there strong incentive for the storage nodes avoid accepting its data. when new satellite joins the network, the participating storage nodes will commence their own vetting process. this process limits their exposure the new and unknown satellite, while building trust over time highlight which the satellites have the best payment record. storage nodes will able configure the maximum amount data they will store for untrusted satellite, and will build historical data whether that satellite will trusted further the future. storage node operators will also retain manual control what satellites they will trust, won't trust, desired. storage node operators can elect automatically trust storj labs provided collection recommended satellites that adhere strict set quality controls and payment service level agreements (slas). protect storage node operators, satellite operator wants included the "tardigrade" approved list, the satellite operator may required adhere set operating, payment, and pricing parameters and sign business arrangement with storj labs. see section for more details. garbage collection when clients move, replace, delete data, satellites, clients behalf satellites, will notify storage nodes that they are longer required store that data. configurations where delete messages are issued the client, the metadata system (and thus satellite, with satellite reputation the line) will require proof that deletes were issued configurable minimum number storage nodes. this means that every time data deleted, storage nodes that are online and reachable will receive notifications right away. storage nodes will sometimes temporarily unavailable and will miss delete messages. these cases, unneeded data considered garbage. satellites only pay for data that they expect stored. storage nodes with lots garbage will earn less than they chapter concrete implementation otherwise would unless garbage collection system employed. for this reason, introduce garbage collection free space storage nodes. garbage collection algorithm method for freeing no-longer used resources. precise garbage collector collects all garbage exactly and leaves additional garbage. conservative garbage collector, the other hand, may leave some small proportion garbage around given some other trade-offs, often with the aim improving performance. long conservative garbage collector used our system, the payment for storage owed storage node will high enough amortize the cost storing the garbage. for the nodes that miss initial delete messages, our first release will start with conservative garbage collection strategy, though anticipate precise strategy the near future. periodically, storage nodes will request data structure detect differences. the simplest form, can hash stored keys, which allows efficient detection outof-sync state. after detecting out-of-sync state, collection can use another structure, such bloom filter [], find out what data has not been deleted. returning data structure tailored each node periodic schedule, satellite can give storage node the ability clean garbage data configurable tolerance. satellites will reject overly frequent requests for these data structures. uplink uplink the term which use identify any software service that invokes libuplink order interact with satellites and storage nodes. comes few forms: libuplink libuplink library that provides access storing and retrieving data the storj network. gateways gateways act compatibility layers between service application and the storj network. they run service co-located with wherever data generated, and will communicate directly with storage nodes avoid central bandwidth costs. the gateway simple service layer top libuplink. our first gateway amazon gateway. provides s-compatible, drop-in interface for users and applications that need store data but don't want bother with the complexities distributed storage directly. uplink cli the uplink cli command line application which invokes libuplink, allowing its user upload and download files, create and remove buckets, manage file permissions, and other related tasks. aims provide experience familiar what you might expect when using linux/unix tools such scp rsync. like storage nodes and satellites, the uplink software all three forms being developed and will released open source software. chapter concrete implementation quality control and branding the storj network has two major product focuses that serve two distinct target markets. these focal points are: creating storage supply for the network via recruiting storage node operators and creating demand for cloud storage with paying users. storj will differentiate these focuses and the experience design for each market segment separating the supply side our business from the demand side through two brands, storj and tardigrade. the supply side the market will served the storj brand. will retain storj.io the place for learning how contribute extra storage and bandwidth the storj network. this includes storage node setup, documentation, frequently asked questions (faqs), and tutorials. users both brands will also able access our source code and community through storj.io. the demand side our business will served the tardigrade brand and will directed through tardigrade.io. this experience will focused toward our partners and customers who purchased decentralized storage and bandwidth from the network with the expectation high durability, resilience, and reliability, backed industry-leading service level agreement (sla). this includes any offers, free trials, satellite selection, documentation, faqs, tutorials, and forth. the "tardigrade" brand will additionally serve satellite quality credentialing system. anyone can set satellite via storj.io, but have satellite listed official tardigrade satellite, considered "tardigrade quality," and benefit directly from storj labs' demand generation activities, operator must pass certain compliance and quality requirements. these quality controls will continuously audit and rank satellites their behavior, durability, compliance, and performance. addition, the satellite operator will have adhere particular business policies around pricing, storage node recruitment, slas, storage node payments, and forth. satellite operators the tardigrade network will have business relationship with storj labs that defines, among other things, franchise fees and revenue sharing between the entities. storj labs will also assume responsibilities including demand generation, brand enforcement, satellite operator support, end user support, united states form tax filing compliance, insurance, and maintenance overall network quality. these compliance and quality controls will implemented ensure that storage nodes are paid fairly and satellites are able continuously meet all slas the tardigrade products. form required law for any payments individual given year exceeding total walkthroughs the following collection common use case examples different types transactions data through the system. upload when user wants upload file, the user first begins transferring data instance the uplink. the uplink chooses encryption key and starting nonce for the first segment and begins encrypting incoming data with authenticated encryption flows the network. the uplink buffers data until knows whether the incoming segment short enough inline segment remote segment. inline segments are small enough stored the satellite itself. the rest this walkthrough will assume remote segment because remote segments involve the full technology stack. the uplink sends request the satellite prepare for the storage this first segment. the request object contains api credentials, such macaroons, and identity certificates. upon receiving the request, the satellite will: confirm that the uplink has appropriate authorization and funds for the request. the uplink must have account with this specific satellite already. make selection nodes with adequate resources that conform the bucket's configured durability, performance, geographic, and reputation requirements. return list nodes, along with their contact information and unrestricted bandwidth allocations, and chosen root piece id. next, the uplink will take this information and begin parallel connections all the chosen storage nodes while measuring bandwidth (section .). the uplink will begin breaking the segment into stripes and then erasure encode each stripe. the generated erasure shares will concatenated into pieces they transfer each storage node parallel. the erasure encoding will configured over-encode more pieces than needed. this will eliminate the long tail effect and lead significant improvement visible performance allowing the uplink cancel the slowest uploads. chapter walkthroughs the data will continue transfer until the maximum segment size hit the stream ends, whichever sooner. all the hashes every piece will written the end each piece stream. after that, the storage node will store: the largest restricted bandwidth allocation; the ttl the segment, one exists; and the data itself. the data will identified the storage node-specific piece and the delegating satellite id. the upload aborted for any reason, the storage node will keep the largest restricted bandwidth allocation received from the client uplink behalf the satellite, but will throw away all other relevant request data. assuming success: the uplink encrypts the random encryption key chose for this file, utilizing deterministic hierarchical key. the uplink will upload pointer object back the satellite, which contains the following information: which storage nodes were ultimately successful what encrypted path was chosen for this segment which erasure code algorithm was used the chosen piece the encrypted encryption key and other metadata the hash the piece hashes signature finally, the uplink will then proceed with the next segment, continuing process segments until the entire stream has completed. each segment gets new encryption key, but the segment's starting nonce monotonically increases from the previous segment. the last segment stored the stream will contain additional metadata: how many segments the stream contained how large the segments are, bytes the starting nonce the first segment extended attributes and other metadata periodically, the storage nodes will later send the largest restricted bandwidth allocation they received part the upload the appropriate satellite for payment. upload happens via the amazon multipart-upload interface, each part uploaded segment individually. chapter walkthroughs download when user wants download file, first the user sends request for data the uplink. the uplink then tries reduce the number round trips the satellite speculatively requesting the pointers for the first few segments addition the pointer for the last segment. the uplink needs the last segment pointer learn the size the object, the size and number segments, and how decrypt the data. future release, the uplink may just tell the satellite which byte ranges are needed and the satellite can respond with the appropriate segment pointers. for every segment pointer requested, the satellite will: validate that the uplink has access download the segment pointer and has enough funds pay for the download. generate unrestricted bandwidth allocation for each piece that makes the segment. look the contact information for the storage nodes listed the pointer. return the requested segment pointer, the bandwidth allocations, and node contact info for each piece. the uplink will determine whether more segments are necessary for the data request received, and will request the remaining segment pointers needed. once all necessary segment pointers have been returned, the requested segments are not inline, the uplink will initiate parallel requests while measuring bandwidth (section all appropriate storage nodes for the appropriate erasure share ranges inside each stored piece. because not all erasure shares are necessary for recovery, long tails will eliminated and significant and visible performance improvement will gained allowing the uplink cancel the slowest downloads. the uplink will combine the retrieved erasure shares into stripes and decrypt the data. the download aborted for any reason, each storage node will keep the largest restricted bandwidth allocation received, but will throw away all other relevant request data. either way, the storage nodes will later send the largest restricted bandwidth allocation they received part the download the appropriate satellite for later payment. delete when user wants delete file, the delete operation first received the uplink. the uplink then requests all the segment pointers for the file. for every segment pointer, the satellite will: chapter walkthroughs validate that the uplink has access delete the segment pointer. generate signed agreement for the deletion the segment, the storage node knows the satellite expecting the delete proceed. look the contact information for the storage nodes listed the pointer. return the segments, the agreements, and contact information. for all the remote segments, the uplink will initiate parallel requests all appropriate storage nodes signal that the pieces are being removed. the storage nodes will return signed message indicating either that the storage node received the delete operation and will delete both the file and its bookkeeping information that was already removed. the uplink will upload all the signed messages that received from working storage nodes back the satellite. the satellite will require adjustable percent the total storage nodes successfully sign messages ensure that the uplink did its part notifying the storage nodes that the object was deleted. the satellite will remove the segment pointers and stop charging the customer and stop paying the storage nodes for them. the uplink will return success status. periodically, storage nodes will ask the satellite for generated garbage collection messages that will update storage nodes who were offline during the main deletion event. satellites will reject requests for garbage collection messages that happen too frequently. see section for more details. move when user wants move file, first, the uplink receives request for moving the file new path. then, the uplink requests all the segment pointers that file. for every segment pointer, the satellite: validates that the uplink has access download it. returns the requested segment metadata. for every segment pointer, the uplink: decrypts the metadata with encryption key derived from the path. calculates the path the new destination. re-encrypts the metadata with new encryption key derived from the new path. the uplink requests that the satellite add all modified segment pointers and remove all old segment pointers atomic compare-and-swap operation. the satellite will validate that: chapter walkthroughs the uplink has appropriate authorization remove the old path and create the new path. the content the old path hasn't changed since the overall operation started. the validation successful, the satellite will perform the operation. storage node will receive any request related the file move. because the complexity around atomic pointer batch modifications, efficient move operations may not implemented the first release this network. copy when user wants copy file, first, the uplink receives request for copying file new path. then the uplink requests all the segment pointers the file. for every segment pointer, the satellite: validates that the uplink has access download it. looks the contact information for the storage nodes listed the pointer. returns the requested segment metadata, new root piece id, and contact information. for every segment pointer, the uplink: decrypts the metadata with encryption key derived from the path. changes the path the new destination. invokes copy operation each the storage nodes from the pointer duplicate the piece with new piece id. waits for the storage nodes respond that they have duplicated the data and removes nodes that were unsuccessful. re-encrypts the metadata with the new piece and new encryption key derived from the new path. finally, the uplink uploads all modified segment pointers the satellite. importantly, okay the storage nodes de-duplicate storage, only store one actual copy the data. all that matters that the storage node can identify the data both the old and new piece id. one the piece ids receives delete operation, the other piece will continue working. only after both pieces are deleted will the node free the space. list when user wants list files: chapter walkthroughs first, request for listing page objects received the uplink. then, the uplink will translate the request unencrypted paths encrypted paths. next, the uplink will request from the satellite the appropriate page encrypted paths. after that, the satellite will validate that the uplink has appropriate access and then return the requested list page. finally, the uplink will decrypt the results and return them. audit each satellite has queue segment stripes that will audited across set storage nodes. the queue filled via two mechanisms. the first mechanism, the satellite populates the queue periodically selecting segments randomly, and then stripes within those segments also random. because segments have maximum size, this sufficiently approximates our goal choosing byte audit uniformly random. the second mechanism, the satellite chooses stripe audit identifying storage nodes that have had fewer recent audits than other storage nodes. the satellite will select stripe random from the data contained that storage node. satellites will then work process the queue and report errors. for each stripe request, the satellite will perform the entire download operation for that small stripe range, filtering out nodes that are containment mode. unlike standard downloads, the stripe request does not need performant. the satellite will attempt download all the erasure shares for the stripe and will wait for slow storage nodes. after receiving many shares possible within generous timeout, the erasure shares will analyzed discover which, any, are wrong. satellites will take note storage nodes that return invalid data, and storage node returns too much invalid data, the satellite will disqualify the storage node from future exchanges. the case disqualification, the satellite will not pay the storage node going forward, and will not select the storage node for new data. for storage nodes that did not respond, cryptographic checksum the expected audit result will created and stored, placing the unresponsive nodes containment. while containment, node will continue given only the audit was unresponsive for until passes disqualified. data repair the repair process has two parts. the first part detects unhealthy files, and the second part repairs them. detection straightforward. chapter walkthroughs each satellite will periodically ping every storage node knows about, either part the audit process via standard node discovery ping operations. the satellite will keep track nodes that fail respond and mark them down. when node marked down marked bad via the audit process, the pointers that point that storage node will considered for repair. pointers keep track their minimum allowable redundancy. pointer not stored enough good, online storage nodes, will added the repair queue. worker process will take segment pointers off the repair queue. when segment pointer taken off the repair queue: the worker will download enough pieces reconstruct the entire segment, along with the piece hashes stored with those pieces (see section ..). unlike audits, only enough pieces for accurate repair are needed. unlike streaming downloads, the repair system can wait for the entire segment before starting. the piece hashes are validated against the signature the pointer, and then the downloaded pieces are validated against the validated piece hashes. incorrect pieces are thrown away and count against the source failed audits. once enough correct pieces are recovered, the missing pieces are regenerated. the satellite selects some new nodes and uploads the new pieces those new nodes via the normal upload process. the satellite updates the pointer's metadata. payment the payment process works follows: first, the satellite will choose rollup period. this period time--defaulting day--that payment for data rest calculated. this purely period chosen for accounting; actual payments will happen less frequent schedule. during each roll-up period, satellite will consider all the files believes are currently stored each storage node. satellites will keep track payments owed each storage node for each rollup period, based the data kept each storage node. finally, storage nodes will periodically--defaulting monthly--send bandwidth allocation reports. when satellite receives bandwidth allocation report, calculates the owed funds along with the outstanding data rest calculations. then sends the funds the storage node's requested wallet address. future work storj work progress, and many features are planned for future versions. this chapter, discuss few especially interesting areas which want consider improvements our concrete implementation. hot files and content delivery occasionally, users our system may end delivering files that are more popular than anticipated. while storage node operators might welcome the opportunity paid for more bandwidth usage for the data they already have, demand for these popular files might outstrip available bandwidth capacity, and form dynamic scaling needed. fortunately, satellites already authorize all accesses pieces, and can therefore meter and rate limit access popular files. file's demand starts grow more than current resources can serve, the satellite has opportunity temporarily pause accesses necessary, increase the redundancy the file over more storage nodes, and then continue allowing access. reed-solomon erasure coding has very useful property. assume (k, encoding, where any pieces are needed total. for any non-negative integer number the first pieces (k, encoding are the exact same pieces (k, encoding. this means that redundancy can easily scaled with little overhead. practical example, suppose file was encoded via scheme, and satellite discovers that needs double bandwidth resources meet demand. the satellite can download any pieces the generate just the last pieces new scheme, store the new pieces new nodes, and--without changing any data the original nodes--store the file scheme, where any out pieces are needed. this allows all requests adequately load balance across the pieces. demand outstrips supply again, only pieces are needed generate even more redundancy. this manner, satellite could temporarily increase redundancy where requests are load balanced across nodes, such that every piece all are unique, and any those pieces are all that required regenerate the original file. one hand, the satellite will need pay storage nodes for the increased redundancy, content delivery this manner has increased at-rest costs during high demand, addition bandwidth costs. the other hand, content delivery often desired highly geographically redundant, which this scheme provides naturally. chapter future work improving user experience around metadata our initial concrete implementation, place significant burdens the satellite operator maintain good service level with high availability, high durability, regular payments, and regular backups. expect large degree variation quality satellites, which led implement our quality control program (see section .). over time, clients satellites will want reduce their dependence satellite operators and enjoy more efficient data portability between satellites besides downloading and uploading their data manually. plan spend significant time improving this user experience number ways. the short term, plan build metadata import/export system, users can make backups their metadata their own and transfer their metadata between satellites. the medium term, plan reduce the size these exports considerably and make much this backup process automatic and seamless possible. expect build system periodically back the major portion the metadata directly the network. the long term, plan architect the satellite out the platform. hope eliminate satellite control the metadata entirely via viable byzantine-fault tolerant consensus algorithm, should one arise. the biggest challenge this finding the right balance between coordination avoidance and byzantine fault tolerant consensus, where storage nodes can interact with one another and share encoded pieces files while still operating within the performance levels users will expect from platform that competing with traditional cloud storage providers. our team will continue research viable means achieve this end. see section and appendix for discussions why aren't tackling the byzantine fault tolerant consensus problem right away. selected calculations object repair costs fundamental challenge our system how not only choose the system parameters that keep the expansion factor and repair bandwidth minimum but also provide acceptable level durability. fortunately, are not alone wondering about this, and there good amount prior research the problem. "peer-to-peer storage systems: practical guideline lazy" excellent guide, and much our work follows from their conclusions. the end result mathematical framework which determines network durability and repair bandwidth given reed-solomon parameters, average node lifetime, and reconstruction rate. the following summary results and explanation their implications. variable mttf mrt -lr bwr description mean time failure /mttf mean reconstruction time /mrt total bytes the network total number pieces per segment (rs encoding) pieces needed rebuild segment (rs encoding) repair threshold loss rate durability expansion factor ratio data that repair bandwidth total repair bandwidth the network ln(n/m) m-k+ n/k a(n ln(n/m) bwr the equations demonstrate that repair bandwidth impacted node churn linearly, which expected. lower mean time node failure triggers more frequent rebuilds and, chapter selected calculations therefore, more bandwidth usage. loss rate much more sensitive high node churn, increases exponentially with this necessitates very stable nodes, with lifetimes several months, achieve acceptable network durability. see section for more in-depth discussion how node churn affects erasure code parameters. bandwidth limits usable space repair affects storage nodes' participation beyond their bandwidth usage; also constrains the amount usable disk space. consider storage node with available space, with stated monthly bandwidth limit gb. it's known (via the above framework) that storage node can expect repair its data given month, and assuming each stored object served least once, then can store more than this node since anything more than that causes more bandwidth than allowed. other words, paid bandwidth plus repair bandwidth must always less than equal the bandwidth limit. higher repair rates equal lower effective storage size, but nodes serving paid data more frequently are more sensitive this effect. practice, the paid bandwidth rate will vary with the type data being stored each node. these ratios must monitored closely determine appropriate usable space limits the network evolves over time. chapter selected calculations audit false positive risk rely bayesian approach determine the probability that storage node maintaining stored pieces faithfully. high level, seek answer the following question: how consecutive successful audits change our estimate the probability that node will continue return successful audits? model the audit process being binomial random variable with unknown probability success with each audit being independent bernoulli trial. well-known that the conjugate prior the binomial distribution the beta distribution b(a, b), and that the posterior also follows the beta distribution. [], use the mean the posterior distribution our bayes estimator, which given x)/(a where are the parameters the prior distribution, and the number successes observed audits. under our assumption that each audit successful, arrive the bayes estimate the success probability n)/(a n). jeffrey's prior probability estimate audit success probability estimate audit success probability number audits, each assumed successful figure jeffrey's prior, see the estimate for audit success probability heavily weighted near near uniform prior audit success probability estimate probability estimate audit success probability number audits, each assumed successful figure using uniform prior, there assumption placed the estimated audit success probability, and all probabilities are assumed equally likely. now choose prior derive numerical estimate the audit success probability based the number audits performed. there are many reasonable choices bayesian priors, but restrict our attention two popular choices: the uniform prior and jeffrey's chapter selected calculations prior []. using the uniform prior b(, initializes the experiment assigning equal probability all possible outcomes; that is, the probability success drawn from the uniform distribution under jeffrey's prior b(., .), assumed that the probability success falls towards either extreme, that node will return successful audit either with probability near with probability near number audits audit success estimate given uniform prior audit success estimate given jeffrey's prior table estimate audit success probability number audits, each assumed successful. find that the estimated probability success begins when there information known about the node (no audits have been performed), with the estimate quickly jumping above few audits using jeffrey's prior. table present results obtained from using both priors. remark that the well-established bayesian approach allows rapidly gain more confidence node's ability return successful audit, given that the success probability estimate tends closer with each consecutive audit success. chapter selected calculations choosing erasure parameters the context storing erasure-coded segment decentralized network, consider the loss piece from two different perspectives. direct piece loss with direct piece loss, assume that for specific segment, its erasure pieces are lost according certain rate. point out that modeling this straightforward: pieces are lost rate aln(m/n) ln(-p/a) how long expect segment last between repairs. indirect piece loss when modeling indirect piece loss, suppose that fixed rate nodes drop out the network each month, whether not they are holding pieces the segment under consideration. describe the probability that the dropped nodes were each storing one the pieces specific segment, turn the hypergeometric probability distribution. suppose nodes are replaced per month out total nodes the network. then the probability that nodes were each storing piece the segment given c-n p(x cc-d (.) which has mean nc/c. then determine how long will take for the number pieces fall below the desired threshold iterating, holding the overall churn fixed but reducing the number existing pieces the distribution's mean each iteration and counting the number iterations required. for example, after one iteration, the number existing pieces reduced nc/c, instead pieces the network (as the parameter (.)), there are nc/c pieces, changing both the parameter and the mean for (.) iteration may extend this model considering multiple checks per month (as the direct piece loss case), assuming that c/a nodes are lost every /a-th month instead assuming that nodes are lost per month, where the number checks per month. this yields initial hypergeometric probability distribution with mean nc/ac. assume proportion pieces are lost per month, given months. the rate may taken over any desired time interval. though chapter selected calculations either these two cases (single multiple segment integrity checks per month), track the number iterations until the number available pieces fall below the repair threshold. this number may then used determine the expected number rebuilds per month for any given segment. numerical simulations for indirect piece loss produce decision tables (table showcasing worst-case mean segment rebuild outcomes based simulating piece loss for segments encoded with varying reed-solomon parameters. assume (k, encoding scheme, where pieces are generated, with pieces needed for reconstruction, using three different values for also assume that segment undergoes the process repair when less than pieces remain the network, using three different values for each for the initial table, use simplifying assumption that pieces the network are lost constant rate per month, which may due node churn, data corruption, other problems. arrive the value for mean rebuilds per month, consider single segment that encoded with pieces which are distributed uniformly randomly nodes the network. simulate conditions leading rebuild, uniformly randomly select subset nodes from the total population and designate them failed. this multiple times per (simulated) month, scaling the piece loss rate linearly according the number segment integrity checks per month. once enough nodes have failed bring the number pieces above the repair threshold the segment rebuilt, and track the number rebuilds over the course months. repeat this simulation for iterations, simulating two-year periods for single segment. then take the number rebuilds the percentile (or higher) the number rebuilds occurring over these iterations. other words, choose the value for which the value the observed cumulative distribution function (cdf), describing the number rebuilds over this two-year period, least this value then divided the number months arrive the mean rebuilds/month value. example the approach shown figure perform the experiment network nodes, observing that the network size will not directly impact the mean rebuilds/month value for single segment under our working assumption constant rate loss per month. forming the decision tables, consider part our calculations how different choices and mean time failure affect durability and repair bandwidth. what this constant rate may viewed the mean the poisson distribution modeling piece loss per month. for example, the monthly network piece loss rate assumed the network size (or %), and segment integrity checks are performed per month, assume that, average, pieces are lost between checks. represent piece loss proportion nodes selected uniformly randomly from the total network. the proportion scales directly with network size, the overall number pieces lost stays the same for networks different sizes. chapter selected calculations rebuilds over months rebuilds over months cumulative observed density iterations observed density iterations number rebuilds number rebuilds figure left: density for the number rebuilds over month period, repeated for iterations. right: cdf the number rebuilds. this case, the mean rebuilds/month value would taken with there being chance that segment rebuilt most times over the course months. are looking for the lowest repair bandwidth that also meets our durability requirements. mttf (months) repair bandwidth ratio durability nines) .() table decision tables showing the relationship between churn (mttf), reed-solomon parameters (k, m), repair bandwidth ratio, and durability conclusion conclude observing that these models may tuned target specific network scenarios and requirements. one network may require one set reed-solomon parameters, while different network may require another. general, the closer m/n the more rebuilds per month should expected under fixed churn rate. while having larger ratio for m/n increases file durability for any given churn rate, comes the expense more bandwidth used since repairs are triggered more often. maintain low mean rebuilds/month value while also maintaining higher file durability, the aim should increase the value much feasible given other network conditions (latency, download speed, etc.), which allows for lower relative value while still not jeopardizing file durability. chapter selected calculations informally, takes longer lose more pieces under given fixed network size and churn rate. therefore, maximize durability while minimizing repair bandwidth usage, should large existing network conditions allow. this allows for value that relatively closer reducing the mean rebuilds/month value, which turn lowers the amount repair bandwidth used. for example, assume have network with mean time failure six months. suppose consider the same file encoded with two different parameters: one under schema and the other schema. set that for both cases, observe from the above table that the bandwidth repair ratio the case and the case. both encoding schemes have similar durability, repair both cases triggered when there are pieces left; even though the mean rebuilds per month empirically and theoretically lower for the case using distributed consensus explain why are not trying solve byzantine distributed consensus, it's worth brief discussion the history distributed consensus. non-byzantine distributed consensus computerized data storage systems began necessity with single computers storing and retrieving data their own. unfortunately, environments where the system must continue operating all times, single computer failure can grind important process halt. result, researchers have often sought ways enable groups computers manage data without any specific computer being required for operation. spreading ownership data across multiple computers could increase uptime the face failures, increase throughput spreading work across more processors, and forth. this research field has been long and challenging; but, fortunately, has led some really exciting technology. the biggest issue with getting group computers agree that messages can lost. how this impacts decision making succinctly described the "two generals' problem" [], which two armies try communicate the face potentially lost messages. both armies have already agreed attack shared enemy, but have yet decide time. both armies must attack the same time else failure assured. both armies can send messengers, but the messengers are often captured the enemy. both armies must know what time attack and that the other army has also agreed this time. ultimately, generic solution the two generals' problem with finite number messages impossible, engineering approaches have had embrace uncertainty necessity. many distributed systems make trade-offs deal with this uncertainty. some systems embrace consistency, which means that the system will choose downtime over inconsistent answers. other systems embrace availability, which means that the system chooses potentially inconsistent answers over downtime. the widely-cited cap theorem states that every system must choose only two consistency, availability, and partition tolerance. due the inevitability network failures, partition tolerance nonnegotiable, when partition happens, every system must choose sacrifice either consistency availability. many systems sacrifice both (sometimes accident). the cap theorem, consistency (specifically, linearizability) means that every read receives the most recent write error, inconsistent answer means the system returned something besides the most recent write without obviously failing. more generally, there are number other consistency models that may acceptable making various trade-offs. linearizability, sequential consistency, causal consistency, pram consis earlier described problem between groups gangsters chapter distributed consensus tency, eventual consistency, read-after-write consistency, etc., are all models for discussing how history events appears various participants distributed system. amazon generally provides read-after-write consistency, though some cases will provide eventual consistency instead []. many distributed databases provide eventual consistency default, such dynamo and cassandra []. linearizability distributed system often much more desirable than more weakly consistent models, useful building block for many higher level data structures and operations (such distributed locks and other coordination techniques). initially, early efforts build linearizable distributed consensus centered around two-phase commit, then three-phase commit, which both suffered due issues similar the two generals' problem. the flp-impossibility paper proved that algorithm could reach linearizable consensus bounded time. then barbara liskov and brian oki published the viewstamped replication algorithm which was the first linearizable distributed consensus algorithm. unaware the publication, leslie lamport set out prove linearizable distributed consensus was impossible [], but instead proved was possible publishing his own paxos algorithm [], which became significantly more popular, even though wasn't officially published journal until ultimately, both algorithms have large amount common. despite lamport's claims that paxos simple [], many papers have been published since then challenging that assertion. google's description their attempts implement paxos are described paxos made live [], and paxos made moderately complex attempt try and fill all the details the protocol. the entire basis the raft algorithm rooted trying wrangle and simplify the complexity paxos []. ultimately, after upsetting few decades, reliable implementations paxos, raft, viewstamped replication [], chain replication [], and zab now exist, with ongoing work improve the situation further arguably, part google's early success was spending the time build their internal paxos-as-a-service distributed lock system, chubby []. most google's famous early internal data storage tools, such bigtable [], depend chubby for correctness. spanner []--perhaps one the most incredible distributed databases the world--is largely just two-phase commit top multiple paxos groups. byzantine distributed consensus mentioned our design constraints, expect most nodes rational and some byzantine, but few-to-none altruistic. unfortunately, all the previous algorithms discussed assume collection altruistic nodes. reliable distributed consensus algorithms have been game-changing for many applications requiring fault-tolerant differing consistency models are new you, may worth reading about them kyle kingbury's excellent tutorial []. you're wondering why computers can't just use the current time order events, keep mind exceedingly difficult get computers even agree that []. chapter distributed consensus storage. however, success has been much more mixed the byzantine fault tolerant world. there have been number attempts solve the byzantine fault tolerant distributed consensus problem. the field exploded after the release bitcoin [], and still its early stages. note, are particularly interested pbft (barbara liskov once again with the first solution), q/u [], fab (but see []), bitcoin, zyzzyva (but also see []), rbft [], tangaroa [], tendermint [], aliph [], hashgraph [], honeybadgerbft [], algorand [], casper [], tangle [], avalanche [], parsec [], and others []. each these algorithms make additional trade-offs, that non-byzantine distributed consensus algorithms don't require, deal with the potential for uncooperative nodes. for example, pbft causes significant amount network overhead. pbft, every client must attempt talk majority participants, which must all individually reply the client. bitcoin intentionally limits the transaction rate with changing proof-ofwork difficulty. many other post-bitcoin protocols require all participants keep full copy all change histories. why we're avoiding byzantine distributed consensus ultimately, all the existing solutions fall short our goal minimizing coordination (see section .). flexible paxos does significantly better than normal paxos the steadystate for avoiding coordination, but completely unusable byzantine environment. distributed ledger "tangle-like" approaches suffer from inability prune history and retain significant global coordination overhead. are excited about and look forward fast, scalable byzantine fault tolerant solution. the building blocks one may already listed the previous discussion. until clear that one has arisen, are reducing our risk avoiding the problem entirely. attacks with any distributed system, variety attack vectors exist. many these are common all distributed systems. some are storage-specific and will apply any distributed storage system. spartacus spartacus attacks, identity hijacking, are possible unmodified kademlia []. any node may assume the identity another node and receive some fraction messages intended for that node simply copying its node id. this allows for targeted attacks against specific nodes and data. spartacus attack mitigation addressed s/kademlia implementing node ids public key hashes and requiring messages signed. spartacus attacker this system would unable generate the corresponding private key, and thus unable sign messages and participate the network. sybil sybil attacks involve the creation large amounts nodes attempt disrupt network operation hijacking dropping messages. while kademlia vulnerable sybil attacks, our adoption s/kademlia proof work identity generation (section reduces the vulnerability degree. further, our storage node reputation system involves prolonged initial vetting period nodes must complete before they are trusted with significant amounts data membership kademlia routing tables. this vetting system, discussed more sections and prevents large influx new nodes from taking incoming data from existing reputable storage nodes without first proving their longevity. eclipse eclipse attack attempts isolate node set nodes the network graph ensuring that all outbound connections reach malicious nodes. eclipse attacks can hard identify, malicious nodes can made function normally most cases, only eclipsing certain important messages information. storj addresses eclipse attacks using public key hashes node ids, signatures based those public keys, and multiple disjoint network lookups prescribed s/kademlia []. the larger the network is, the harder will prevent node from finding portion the network uncontrolled attacker. long storage node satellite has chapter attacks been introduced portion the network that not controlled the attacker any point, the public key hashes and signatures ensure that man-in-the-middle attacks are impossible, and multiple disjoint network lookups ensure that kademlia routing prohibitively expensive bias. avoid eclipse attack, all that remains make sure new nodes are appropriately introduced least one well-behaved node the network during the bootstrapping process. that end, storj labs will run some well-known, verified bootstrap nodes. honest geppetto this attack, the attacker operates large number "puppet" storage nodes the network, accumulating reputation and data over time. once certain threshold reached, she pulls the strings each puppet execute hostage attack with the data involved, simply drops each storage node from the network. the best defense against this attack create network sufficient scale that this attack ineffective. the meantime, this can partially prevented relatedness analysis storage nodes. bayesian inference across downtime, latency, network route, and other attributes can used assess the likelihood that two storage nodes are operated the same organization. satellites can and should attempt distribute pieces across many unrelated storage nodes possible. hostage bytes the hostage byte attack storage-specific attack where malicious storage nodes refuse transfer pieces, portions pieces, order extort additional payments from clients. the reed-solomon encoding ought sufficient defeat attacks this sort (as the client can simply download the necessary number pieces from other nodes) unless multiple malicious nodes collude gain control many pieces the same file. the same mitigations discussed under the honest geppetto attack can apply here help avoid this situation. cheating storage nodes, uplinks, satellites measuring bandwidth with signatures minimizes the risk for uplink and storage nodes. the uplink can only interact with the storage node sending signed restricted allocation. the restriction limits the risk very low level. the storage node has comply with the protocol expected order get more restricted allocations. storage nodes and satellites will commence vetting process that limits their exposure. storage nodes are allowed decline requests from untrusted satellites. chapter attacks faithless storage nodes and satellites while storage nodes and satellites are built require authentication via signatures before serving download requests, reasonable imagine modification the storage node satellite that will provide downloads any paying requestor. even network with faithless satellite, data privacy not significantly compromised. strong client-side encryption protects the contents the file from inspection. storj not designed protect against compromised clients. defeated audit attacks typical merkle proof verification requires pre-generated challenges and responses. without periodic regeneration these challenges, storage node can begin pass most audits without storing all the requested data. instead, request random stripe erasure shares from all storage nodes. run the berlekamp-welch algorithm across all the erasure shares. when enough storage nodes return correct information, any faulty missing responses can easily identified. new storage nodes will placed into vetting process until enough audits have passed. see section for more details. primary user benefits have designed the storj network provide users better security, availability, performance, and economics--across wide variety use cases--than either on-premise storage solutions traditional, centralized cloud storage. while the bulk this paper describes the design considerations overcome the challenges highly decentralized system, this appendix describes why the end result should significant improvement over traditional approaches. security have designed our system the equivalent spreading encrypted sand encrypted beach. all data encrypted client-side before reaching our system. data sharded and distributed across large number independently operated disk drives which are part much larger network independently operated storage nodes. typical scenario (with reed-solomon setup), each file distributed across different disk drives global network over independently operated nodes. (the previous version the storj network had over independently operated nodes.) compromise individual file, would-be bad actor would have locate and compromise roughly different drives, each operated different provider, network over drives. even the actor were somehow able compromise those drives, reconstruct the file, the would-be bad actor would then have decrypt -bit aes encrypted data, with keys that are only held the end user. and, the wouldbe bad actor would have repeat this process with entirely different set potential drives for the next file they wish obtain. design, not possible for storj, satellite operators, storage node operators, would-be bad actors mine compromise end user data. the level decentralization the network creates powerful disincentives for malicious actors, there centralized trove data target. availability while most centralized cloud providers employ various strategies provide protection against individual drive failures, they are not immune system-wide events. storms, power outages, floods, earthquakes, operator error, design flaws, network overload, attacks can compromise entire data centers. while the centralized providers may calculate and publish theoretically high availability numbers, these calculations depend drive failures being uncorrelated. fact, any chapter primary user benefits data center, the chances individual drive failing highly correlated with the chances another drive failing. decentralized system, contrast, each node operated different individual, different location, with separate personnel, power, network access, and forth. therefore, the chance individual node failing almost entirely uncorrelated with the chances other drives failing. result, the kinds availability obtain are not subject storms, power outages, other "black swan" events. even the chance individual drive failing the storj network higher than centralized cloud, the chance collective failure (e.g. losing out independent drives) vanishingly small. addition, the chance losing one file not correlated with the chances losing second file. performance for read-intensive use cases, the storj network can deliver superior performance taking advantage parallelism. the storage nodes are located close "the edge," reducing the latency experienced when recipients data are physically far from the data center that houses the data. read performance benefits from parallelism. the particular erasure coding scheme that use ensures that slow drives, slow networks, networks and drives experiencing temporarily high load not limit throughput. can adjust the k/n ratio that dramatically improve download and streaming speeds, without imposing the kinds high costs associated with cdn networks. economics while the amount data created around the world has doubled every year, the price cloud storage has only declined about per year over the last three years. there are number potential explanations, both the supply and demand side. public cloud storage operators must make large capital investments building out network data centers and must incur significant costs for power, personnel, security, fire suppression, and forth. their pricing structure must allow them recoup those costs. moreover, the structure the industry such that inherently oligopolistic: there are only handful public cloud companies, and they comprise the largest companies market cap the planet (microsoft, google, amazon, alibaba). any price decreases one provider are quickly matched the other providers, there has been little incentive for providers drop prices gain market share. decentralized network, contrast, there little marginal cost being storage node operator. our experience, the vast majority operators are using existing live equipment with significant spare capacity. there additional cost storage node operator terms capital personnel. running drive full capacity does not chapter primary user benefits consume significantly more power than running drive with excess space. and, with careful management relative caps, most operators should not experience increased bandwidth costs. consequently, operating node represents nearly pure margin, and these supply cost savings can passed end users. have designed market mechanisms the demand side well, prevent any satellite operator from cornering the market. even after providing healthy margin farmers, demand partners, and satellite operators, believe should able provide profitable storage services fraction the cost equivalent centralized cloud storage providers. bibliography identity theft resource center and cyberscout. annual number data breaches and exposed records the united states from (in millions). https://www.statista.com/statistics//data-breaches-recorded-in-the-unitedstates-by-number-of-breaches-and-records-exposed/, knowledge sourcing intelligence llp. cloud storage market forecasts from https://www.researchandmarkets.com/research/lfwbx/cloud_storage, dan shearer. eu-us cloud privacy crash. https://kopano.com/kopano-documents/eu-us-cloud-privacy.pdf, gartner inc. gartner forecasts worldwide public cloud revenue grow percent https://www.gartner.com/en/newsroom/press-releases/--gartner-forecasts-worldwide-public-cloud-revenue-to-grow--percent-in-, synergy research group. cloud growth rate increased again amazon maintains market share dominance. https://www.srgresearch.com/articles/cloudgrowth-rate-increased-again-q-amazon-maintains-market-share-dominance, backblaze inc. how long hard drives last: hard drive stats. https://www.backblaze.com/blog/hard-drive-stats-for-q-/, sean rhea, dennis geels, timothy roscoe, and john kubiatowicz. handling churn dht. proceedings the annual conference usenix annual technical conference, atec page berkeley, ca, usa, usenix association. petar maymounkov and david mazieres. kademlia: peer-to-peer information system based the xor metric. revised papers from the first international workshop peer-to-peer systems, iptps pages london, uk, springer-verlag. charles blake and rodrigo rodrigues. high availability, scalable storage, dynamic peer networks: pick two. proceedings the conference hot topics operating systems volume hotos', page berkeley, ca, usa, usenix association. comcast inc. xfinity data usage center-faq. https://dataplan.xfinity.com/faq/, amitanand aiyer, lorenzo alvisi, allen clement, mike dahlin, jean-philippe martin, and carl porth. bar fault tolerance for cooperative services. proceedings the twentieth acm symposium operating systems principles, sosp pages new york, ny, usa, acm. bibliography seth gilbert and nancy lynch. brewer's conjecture and the feasibility consistent, available, partition-tolerant web services. sigact news, ():-, june seth gilbert and nancy lynch. perspectives the cap theorem. computer, ():-, february daniel abadi. consistency tradeoffs modern distributed database system design: cap only part the story. computer, ():-, february peter bailis, aaron davidson, alan fekete, ali ghodsi, joseph hellerstein, and ion stoica. highly available transactions: virtues and limitations. proc. vldb endow., ():-, november peter bailis, alan fekete, michael franklin, ali ghodsi, joseph hellerstein, and ion stoica. coordination avoidance database systems. proc. vldb endow., ():-, november chenggang wu, jose faleiro, yihan lin, and joseph hellerstein. anna: kvs for any scale. icde, joseph hellerstein. the declarative imperative: experiences and conjectures distributed logic. sigmod rec., ():-, september peter alvaro, neil conway, joseph hellerstein, and william marczak. consistency analysis bloom: calm and collected approach. cidr, kyle kingsbury. consistency models clickable map. https://jepsen.io/consistency, paolo viotti and marko vukolic. consistency non-transactional distributed storage systems. acm comput. surv., ()::-:, june joseph hellerstein. anna: crazy fast, super-scalable, flexibly consistent kvs. https://rise.cs.berkeley.edu/blog/anna-kvs/, satoshi nakamoto. bitcoin: peer-to-peer electronic cash system. https://bitcoin.org/bitcoin.pdf, diego ongaro and john ousterhout. search understandable consensus algorithm. proceedings the usenix conference usenix annual technical conference, usenix atc', pages berkeley, ca, usa, usenix association. giuseppe decandia, deniz hastorun, madan jampani, gunavardhan kakulapati, avinash lakshman, alex pilchin, swaminathan sivasubramanian, peter vosshall, and werner vogels. dynamo: amazon's highly available key-value store. proceedings twenty-first acm sigops symposium operating systems principles, sosp pages new york, ny, usa, acm. sanjay ghemawat, howard gobioff, and shun-tak leung. the google file system. proceedings the nineteenth acm symposium operating systems principles, sosp pages new york, ny, usa, acm. bibliography konstantin shvachko, hairong kuang, sanjay radia, and robert chansler. the hadoop distributed file system. proceedings the ieee symposium mass storage systems and technologies (msst), msst pages washington, dc, usa, ieee computer society. lustre. introduction lustre architecture. http://wiki.lustre.org/images///lustrearchitecture-v.pdf, rosenberg, mahy, matthews, and wing. session traversal utilities for nat (stun). rfc rfc editor, october http://www.rfc-editor.org/rfc/rfc.txt. iso. iso/iec -:: information technology upnp device architecture, cheshire and krochmal. nat port mapping protocol (nat-pmp). rfc rfc editor, april http://www.rfc-editor.org/rfc/rfc.txt. ingmar baumgart and sebastian mies. s/kademlia: practicable approach towards secure key-based routing. icpads, pages ieee computer society, mockapetris. domain names implementation and specification. std rfc editor, november http://www.rfc-editor.org/rfc/rfc.txt. ion stoica, robert morris, david karger, frans kaashoek, and hari balakrishnan. chord: scalable peer-to-peer lookup service for internet applications. proceedings the conference applications, technologies, architectures, and protocols for computer communications, sigcomm pages new york, ny, usa, acm. antony rowstron and peter druschel. pastry: scalable, decentralized object location, and routing for large-scale peer-to-peer systems. proceedings the ifip/acm international conference distributed systems platforms heidelberg, middleware pages london, uk, springer-verlag. frederic giroire, julian monteiro, and stephane perennes. peer-to-peer storage systems: practical guideline lazy. ieee global communications conference (globecom), shawn wilkinson, tome boshevski, josh brandoff, james prestwich, gordon hall, patrick gerbes, philip hutchins, and chris pollard. storj: peer-to-peer cloud storage network v.. https://storj.io/storjv.pdf, vijay bhargava, stephen wicker, ieee communications society., and ieee information theory society. reed-solomon codes and their applications edited stephen wicker, vijay bhargava ieee communications society and ieee information theory society, co-sponsors. ieee press piscataway, nj, jeff wendling and olds. introduction reed-solomon. https://innovation.vivint.com/introduction-to-reed-solomon-bcdf, bibliography netanel raviv, yuval cassuto, rami cohen, and moshe schwartz. erasure correction scalar codes the presence stragglers. corr, abs/., kevin bowers, ari juels, and alina oprea. hail: high-availability and integrity layer for cloud storage. proceedings the acm conference computer and communications security, ccs pages new york, ny, usa, acm. zooko wilcox. zfec: filefec.py's encode_file. https://github.com/tahoe-lafs/zfec/commit/dddcd, jeffrey dean and luiz andre barroso. the tail scale. communications the acm, :-, jeffrey dean and sanjay ghemawat. mapreduce: simplified data processing large clusters. commun. acm, ():-, january paiva and rodrigues. policies for efficient data replication systems. international conference parallel and distributed systems, pages dec peter wuille. bip: hierarchical deterministic wallets. https://github.com/bitcoin/bips/blob/master/bip-.mediawiki, ari juels and burton kaliski, jr. pors: proofs retrievability for large files. proceedings the acm conference computer and communications security, ccs pages new york, ny, usa, acm. hovav shacham and brent waters. compact proofs retrievability. proceedings the international conference the theory and application cryptology and information security: advances cryptology, asiacrypt pages berlin, heidelberg, springer-verlag. kevin bowers, ari juels, and alina oprea. proofs retrievability: theory and implementation. proceedings the acm workshop cloud computing security, ccsw pages new york, ny, usa, acm. shawn wilkinson. sip: sip purpose and guidelines, (). https://github.com/storj/sips/blob/master/sip-.md. michael ovsiannikov, silvius rus, damian reeves, paul sutter, sriram rao, and jim kelly. the quantcast file system. proc. vldb endow., ():-, august salman niazi, mahmoud ismail, seif haridi, jim dowling, steffen grohsschmiedt, and mikael ronstrom. hopsfs: scaling hierarchical file system metadata using newsql databases. proceedings the usenix conference file and storage technologies, fast', pages berkeley, ca, usa, usenix association. richard hipp al. sqlite. https://www.sqlite.org/, bibliography google inc. what grpc? https://grpc.io/docs/guides/index.html, accessed dierks and rescorla. the transport layer security (tls) protocol version rfc rfc editor, august http://www.rfc-editor.org/rfc/rfc.txt. arvid norberg. utorrent transport protocol. http://www.bittorrent.org/beps/bep_.html, shalunov, hazel, iyengar, and kuehlewind. low extra delay background transport (ledbat). rfc rfc editor, december http://www.rfc-editor.org/rfc/rfc.txt. trevor perrin. the noise protocol framework. https://noiseprotocol.org/noise.pdf, irving reed and gustave solomon. polynomial codes over certain finite fields. journal the society for industrial and applied mathematics, ():-, amazon inc. amazon simple storage service object metadata. https://docs.aws.amazon.com/amazons/latest/dev/usingmetadata.html, accessed tao ma. ext: add inline data support. https://lwn.net/articles//, bram cohen. the bittorrent protocol specification. http://www.bittorrent.org/beps/bep_.html, avinash lakshman and prashant malik. cassandra: decentralized structured storage system. sigops oper. syst. rev., ():-, april james corbett, jeffrey dean, michael epstein, andrew fikes, christopher frost, furman, sanjay ghemawat, andrey gubarev, christopher heiser, peter hochschild, wilson hsieh, sebastian kanthak, eugene kogan, hongyi li, alexander lloyd, sergey melnik, david mwaura, david nagle, sean quinlan, rajesh rao, lindsay rolig, dale woodford, yasushi saito, christopher taylor, michal szymaniak, and ruth wang. spanner: google's globally-distributed database. osdi, eugen rochko and others. mastodon: your self-hosted, globally interconnected microblogging community. https://github.com/tootsuite/mastodon, daniel bernstein. cryptography nacl. https://cr.yp.to/highspeed/naclcrypto-.pdf, daniel bernstein. nacl: validation and verification. https://nacl.cr.yp.to/valid.html, arnar birgisson, joe gibbs politz, ulfar erlingsson, ankur taly, michael vrable, and mark lentczner. macaroons: cookies with contextual caveats for decentralized authorization the cloud. network and distributed system security symposium, bibliography jinyuan li, maxwell krohn, david mazieres, and dennis shasha. secure untrusted data repository (sundr). proceedings the conference symposium opearting systems design implementation volume osdi', pages berkeley, ca, usa, usenix association. eu-jin goh, hovav shacham, nagendra modadugu, and dan boneh. sirius: securing remote untrusted storage. ndss, volume pages mahesh kallahalla, erik riedel, ram swaminathan, qian wang, and kevin fu. plutus: scalable secure file sharing untrusted storage. proceedings the usenix conference file and storage technologies, fast pages berkeley, ca, usa, usenix association. ralph merkle. digital signature based conventional encryption function. carl pomerance, editor, advances cryptology crypto pages berlin, heidelberg, springer. lloyd welch and elwyn berlekamp. error correction for algebraic block codes. patent usa, john douceur. the sybil attack. revised papers from the first international workshop peer-to-peer systems, iptps pages london, uk, springer-verlag. shawn wilkinson and james prestwich. sip: bounding sybil attacks with identity cost, (). https://github.com/storj/sips/blob/master/sip-.md. michael mitzenmacher. the power two choices randomized load balancing. ieee trans. parallel distrib. syst., ():-, october fabian vogelsteller and vitalik buterin. erc- token standard, (). https://github.com/ethereum/eips/blob/master/eips/eip-.md. braydon fuller. sip: bandwidth reputation and accounting, (). https://github.com/storj/sips/blob/master/sip-.md. neuman. proxy-based authorization and accounting for distributed systems. the international conference distributed computing systems, pages may bruce schneier. applied cryptography (nd ed.): protocols, algorithms, and source code john wiley sons, inc., new york, ny, usa, protocol labs. filecoin: decentralized storage network. https://filecoin.io/filecoin.pdf, burton bloom. space/time trade-offs hash coding with allowable errors. commun. acm, ():-, july asit basu, david gaylor, and james chen. estimating the probability occurrence tumor for rare cancer with zero occurrence sample. regulatory toxicology and pharmacology, (): bibliography harold jeffreys. invariant form for the prior probability estimation problems. proceedings the royal society london. series mathematical and physical sciences, ():-, jim gray. notes data base operating systems. operating systems, advanced course, pages london, uk, springer-verlag. akkoyunlu, ekanadham, and huber. some constraints and tradeoffs the design network communications. proceedings the fifth acm symposium operating systems principles, sosp pages new york, ny, usa, acm. kyle kingsbury. strong consistency models. https://aphyr.com/posts/-strong-consistency-models, justin sheehy. there now. queue, ()::-:, march amazon inc. amazon simple storage service data consistency model. https://docs.aws.amazon.com/amazons/latest/dev/ introduction.html#consistencymodel, accessed michael fischer, nancy lynch, and michael paterson. impossibility distributed consensus with one faulty process. acm, ():-, april brian oki and barbara liskov. viewstamped replication: new primary copy method support highly-available distributed systems. proceedings the seventh annual acm symposium principles distributed computing, podc pages new york, ny, usa, acm. leslie lamport. the part-time parliament website note. https://www.microsoft.com/en-us/research/publication/part-time-parliament/, accessed leslie lamport. the part-time parliament. acm trans. comput. syst., ():-, may leslie lamport. paxos made simple. https://www.microsoft.com/en-us/research/publication/paxos-made-simple/, tushar deepak chandra, robert griesemer, and joshua redstone. paxos made live engineering perspective invited talk). proceedings the annual acm symposium principles distributed computing, robbert van renesse and deniz altinbuken. paxos made moderately complex. acm comput. surv., ()::-:, february barbara liskov and james cowling. viewstamped replication revisited. technical report mit-csail-tr--, mit, july robbert van renesse and fred schneider. chain replication for supporting high throughput and availability. proceedings the conference symposium operating systems design implementation volume osdi', page berkeley, ca, usa, usenix association. bibliography flavio paiva junqueira, benjamin reed, and marco serafini. zab: high-performance broadcast for primary-backup systems. ieee/ifip international conference dependable systems networks (dsn), pages iulian moraru, david andersen, and michael kaminsky. there more consensus egalitarian parliaments. proceedings the twenty-fourth acm symposium operating systems principles, sosp pages new york, ny, usa, acm. howard, malkhi, and spiegelman. flexible paxos: quorum intersection revisited. arxiv e-prints, august mike burrows. the chubby lock service for loosely-coupled distributed systems. proceedings the symposium operating systems design and implementation, osdi pages berkeley, ca, usa, usenix association. fay chang, jeffrey dean, sanjay ghemawat, wilson hsieh, deborah wallach, mike burrows, tushar chandra, andrew fikes, and robert gruber. bigtable: distributed storage system for structured data. usenix symposium operating systems design and implementation (osdi), pages miguel castro and barbara liskov. practical byzantine fault tolerance. proceedings the third symposium operating systems design and implementation, osdi pages berkeley, ca, usa, usenix association. michael abd-el-malek, gregory ganger, garth goodson, michael reiter, and jay wylie. fault-scalable byzantine fault-tolerant services. proceedings the twentieth acm symposium operating systems principles, sosp pages new york, ny, usa, acm. jean-philippe martin and lorenzo alvisi. fast byzantine consensus. ieee trans. dependable secur. comput., ():-, july abraham, gueta, malkhi, alvisi, kotla, and j.-p. martin. revisiting fast practical byzantine fault tolerance. arxiv e-prints, december ramakrishna kotla. zyzzyva: speculative byzantine fault tolerance. acm transactions computer systems (tocs), issue article no. december aublin, mokhtar, and quema. rbft: redundant byzantine fault tolerance. ieee international conference distributed computing systems, pages july christopher copeland and hongxia zhong. tangaroa: byzantine fault tolerant raft, bibliography jae kwon. tendermint: consensus without mining. https://tendermint.com/docs/tendermint.pdf, pierre-louis aublin, rachid guerraoui, nikola knezevic, vivien quema, and marko vukolic. the next bft protocols. acm trans. comput. syst., ()::-:, january leemon baird. the swirlds hashgraph consensus algorithm: fair, fast, byzantine fault tolerance, andrew miller, xia, kyle croman, elaine shi, and dawn song. the honey badger bft protocols. cryptology eprint archive, report https://eprint.iacr.org//. yossi gilad, rotem hemo, silvio micali, georgios vlachos, and nickolai zeldovich. algorand: scaling byzantine agreements for cryptocurrencies. proceedings the symposium operating systems principles, sosp pages new york, ny, usa, acm. vitalik buterin and virgil griffith. casper the friendly finality gadget. corr, abs/., serguei popov. the tangle. https://iota.org/iota_whitepaper.pdf, team rocket. snowflake avalanche: novel metastable consensus protocol family for cryptocurrencies. https://ipfs.io/ipfs/qmuyjhmgnzvlkjiesrwmyuvjhofyopnpvywrrvgv, pierre chevalier, bartlomiej kaminski, fraser hutchison, ma, and spandan sharma. protocol for asynchronous, reliable, secure and efficient consensus (parsec). http://docs.maidsafe.net/whitepapers/pdf/parsec.pdf, james mickens. the saddest moment. ;login: logout, may https://scholar.harvard.edu/files/mickens/files/thesaddestmoment.pdf.