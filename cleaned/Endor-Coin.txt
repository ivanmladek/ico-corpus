endor.coin protocol make artificial intelligence predictions accessible for all powered endor ltd. february make artificial intelligence predictions accessible for all abstract endor.coin reinventing predictive analytics democratizing access artificial intelligence data analysis, making accessible, trustless, censorship-resistance and useful for all. this achieved through the endor.coin protocol, offering the world's first automated, self-served, predictive platform that allows business users and unprofessional crypto-token holders alike ask complex predictive questions and obtain high-quality results minutes. this aims democratize the field data science, that today reserved mostly for fortune companies. endor.coin based the novel science social physics developed mit the project's team members prof. alex pentland and dr. yaniv altshuler. constantly-expanding catalogue predictions: endor.coin will launched with variety pre-defined tokens-related predictions (e.g. tokens predicted increase volume, decrease volatility, ...). these predictions would accessible for purchase using the platform's dedicated edr token. endor.coin would then expand the selection predictions caters allowing users send requests for predictions (rfp) suggesting new types predictions, that would implemented and become available for purchase. do-it-yourself api for advanced users: tech-savvy users and professionals would provided self-serve interface, allowing them easily provide definition any desired behavioral pattern, having the endor.coin platform automatically generating "look-a-likes" prediction such pattern return. automatic fusion private and public data: endor.coin commercial customers (such banks, retailers and insurers) will able easily integrate their proprietary data streams with the platform producing high-quality predictive insights harvested from the fusion private and public data. thanks the use social physics data integration automatic and friendly requiring cleaning data-preparation. data privacy: guaranteed, due the unique ability social physics use data that fully encrypted the customer's side. predictions the people, for the people: using social physics data processed once, and users pay only for the personalization component they require, offering the "%" predictive capability reserved today for tech-giants, for its cost. endor.coin rooted the technological platform endor. gartner cool vendor, recently recognized the world economic forum technology pioneer, endor mit spin-off financially backed leading investors such innovation endeavors, working with fortune- companies such coca cola, walmart and mastercard. contents value proposition introduction inefficient and troubled market democratization requires decentralization technological gap endor.coin protocol project overview token implementation blockchain structure and data separation democratizing the prediction science social physics new science from mit phase endor.com automatic prediction engine for enterprises phase endor.coin protocol data science for the masses definitions data providers prediction engines pre-defined predictions and request for predictions (rfps) private-data analysis and self-serve api roadmap trustless, censorship-resistant and accountable accountability and authenticity for artificial intelligence censorship-resistance through decentralized protocol privacy-preserving data analytics: using encrypted data network effects enabling ecosystem public data providers academic research groups catalysts application developers data sovereignty smart contracts endor's roles launch token privileges and economy use proceeds technological advantages and differentiation scientific revolution from the mit furnaces real product. proven technology usability, value users and value for token holders team key team members advisors appendix social physics explained appendix endor's common use-cases for enterprises appendix endor.coin examples pre-defined predictions appendix knowledge sphere class api bibliography chapter value proposition the ability understand, predict and influence consumer behavior quickly could give any business unfair advantage over its competition. smart business leaders have many ideas for influencing customer behavior improve business performance. implement them, they need answer questions such as: who are our top customers and how acquire more them? who likely try this newly-launched product? how can reduce our reliance promotions? where should open our next store? who will switch from product next month? answer these questions, organizations turn powerful tools like data science and predictive analytics. unfortunately, the current process for implementing these tools slow, painful, and expensive. requires 'unicorns': well-trained, expensive, rare data scientists and phds requires iterations, each taking few days weeks each new business question requires building new model over few weeks when products and behaviors change, the model breaks powered mit's novel social physics technology the endor.coin blockchainbased protocol the first decentralized, trustless, censorship resistance behavioral prediction platform that provides high-quality results for any predictive question minutes. coding, data cleaning, team phds required. following are the key aspects the endor.coin project: technology: powered mit social physics technology [], providing higher accuracy for trends prediction, well the ability generate predictive insights from fully-encrypted data (see our academic work trends prediction financial markets [-], relevant patents and additional reviews [-]). innovation: focuses the automatic modeling short and medium-range behavioral patterns (days weeks), detecting such signals before they become observable any other available technology (see dr. altshuler's talk firstmark "data-driven new york" talk [], large-scale real-time analysis financial investments []). industry validation: endor.coin based the technology developed endor.com mit spin-off [], financially backed leading investors [], working with fortune companies such coca cola [], mastercard [], walmart and others. see endor's product featured finnovate []. awards and recognition: endor gartner cool vendor [], and was recognized the world economic forum "technological pioneer" []. the research done the project's team mit had led winning several additional prizes such the prestigious darpa network challenge [], and the mckinsey award []. team: spearheaded team world experts blockchain, digital banking technologies and predictive analytics from mit's computer science and artificial intelligence lab, the mit media lab and mit sloan school management. prof. pentland (co-founder), member the u.s. national academy engineering, one the world's most-cited scientists [], and was recently declared forbes the most powerful data scientists the world" []. the team has collectively published hundreds scientific papers, dozens commercial patents, well books dedicated blockchain, machine intelligence, and data privacy [-]. reinventing predictive analytics: after years where artificial intelligence and machine learning capabilities were reserved for deep-pocketed companies, endor.coin offers individuals and small businesses access superior tech, for fraction the cost. predictive insights are based the collective analysis the contributed data, offered low cost, while allowing data owners control the privacy their data. the endor.coin protocol enables the integration new data sources, well new prediction engines, creating double network effect the more players join, cost per prediction decreases, while increasing prediction accuracy. trustless, fully decentralized, accountable and censorship resistance: the endor.coin protocol fully-decentralized, providing complete accountability for the prediction results. this prevents any manipulation bias during the predictions. addition, the decentralized and open nature the protocol enables the support any prediction, preventing censorship any single point authority. chapter introduction inefficient and troubled market our world there many are organizations that gather, possess and carefully maintain data. there are data scientists and machine learning experts, who can process data and build predictive models. there are also many people with desire predict the future (these range from high ranking executives, through middle range marketing product managers large companies, and individuals who try decide the right timing buy plane ticket). today, order for the latter offered predictions for their questions, all three entities must co-exist the same organization. this means that the predictions are being generated by, and for, stakeholders large companies. furthermore, this process today highly expensive, data scientists are rare and expensive, and the process producing predictions often requires months hard-work, dedicated every single project. this sets very high entry barrier (and price tag) for anyone who interested prediction. democratization requires decentralization and data separation the democratization artificial intelligence and predictive analytics, making accessible for the 'everyman', requires the development new paradigm, that would meet the following requirements: separation: apart from large corporations and research labs, there almost commercial, academic non-for-profit organization that can sustain high-quality activities that combines data curation, data science and the generation semantic oriented questions. individuals, ngos and small-to-medium businesses usually only focus one those dimensions they either possess (or produce) carefully maintained data, employ strong (and expensive) data scientists, are experts 'asking the right questions'. therefore, order open the bottleneck and make predictions accessible outside the fortune- club, these three basic elements need inherently separated: truly democratized prediction protocol must allow data-providers freely contribute data (either marked public private), while allowing technology experts contribute and prediction engines (that would seamlessly plugged in, and integrated with the protocol), all this allow the end-users easily consume predictions that are based these data sources, and prepared for them these engines. accountability: data science department tech-giant accountability not required key feature, automatically provided the fact that "everyone working for the same boss" (e.g. the relevant c-level executive charge business intelligence, marketing prediction, and on). democratized platform, where data, intelligence and computation constantly being rented ad-hoc basis, accountability becomes essential, for guaranteeing 'fair play', and tuning the merit function all stakeholders for the 'long terms' rather then encouraging short-term revenues. decentralization: there are two critical contributions decentralized prediction framework one being engineering related, and the other revolves around censorship resistance and bias prevention. demonstrated many examples, decentralized solution tend easier scale well extend. adding more data sources, computational resources and different types prediction engines all greatly benefit from decentralized solution. addition, decentralized architecture the only one that guarantees that these predictions would not subject explicit censorship, implicit one, that executed through the generation biased results, arbitration monetary resources. furthermore, efficient decentralization the key emerging network effects, that push cost-per-user down, while constantly increasing prediction accuracy, with the increase the numbers participants (both data-providers, prediction engines, and prediction consumers). technological gap unfortunately, although the benefits data science democratization have been clear for quite some time, implementing such framework reality has been challenging, say the least. the main reasons technological the prevailing science today simply cannot support "generic decentralized behavioral prediction" paradigm... the technologies that exist today, neural networks (or deep learning), genetic programming, decision forests, svms and all require massive amount data sanitation, processing and understanding, before any 'real' machine learning work can commence. this the source the bottleneck that the industry faces today, the rise skilled data scientists' salaries, and their scarcity. detailed discussion this topic appears section and appendix without scientific breakthrough that can automatically digest any data, allowing nonprofessionals and professionals alike ask any predictive question the industry was detained the existing paradigm, heavily bottlenecked the pace company hires new data scientists, and the -figures salaries willing pay them. endor.coin protocol project overview order transcend these limitations, new science had developed. social physics, developed endor.coin founders dr. yaniv altshuler and prof. alex "sandy" pentland, mathematical theory that efficiently models the way human crowds behave. through set mathematical equations that are shown emerge behavioral data sources, the social physics theory enables the automatic transformation any behavioral data source set behavioral clusters. this requires cleaning, pre-processing, understanding the semantics the data (or the questions asked). this collection behavioral clusters known the "knowledge sphere". the endor.coin protocol based the fact that when behavioral data being handled canonic representation behavioral clusters, the traditional process data science can (finally) broken down into basic components, allocating each them, decentralized way, different executors. following the basic outline and main components the endor.coin protocol: canonic data representation: every data that contributed the endor.coin network gets transformed the "knowledge sphere" canonical representation. this can done the various predictions engines (see below), and the execution cost paid the engines. once data undergoes this transformation the various behavioral clusters extracted from can then bundled together with clusters from other types data, resulting efficient and automatic prediction process (see more details section and appendix a). separation data providers: data gets transformed into the canonic "knowledge sphere" representation, data providers are longer required actively take part the later phases the analysis. this enables data-owners integrate their data (in full part) with the endor.coin network, acting autonomous stakeholder the ecosystem focusing maintaining the quality their data, controlling who will have access which part it, and benefiting financially from future value provides. separation prediction engines: known secret among data scientists that around the time spent data science project spent data sanitation and preprocessing. due the revolutionary aspect social physics that for the first time automates these steps various prediction engines can finally seamlessly connected data sources various types. the only thing required for provider prediction engine order connect the endor.coin network support the endor.coin protocol defined the ability digest datasets (selectively, defined the engine), and provide output the form "knowledge sphere" (see complete specification and api code appendix d). decentralized execution: using blockchain. data-providers can donate data (stored aws) and accessible using the endor.coin protocol. extraction behavioral clusters done decentralized way the various prediction engines. queries are being triggered end-users requesting the endor.coin smart contract issue certain prediction (for edr fee). for each prediction the best behavioral cluster chosen the endor.coin prediction code (can freely accessible the project's git account []). funds arbitration being taken care the smart contract among data contributors and prediction engines, according the clusters chosen for that prediction. this arbitration optimizes the quality prediction, and unbiased results. data sovereignty: every data element that contributed the endor.coin network can flagged either "public" "private" (the same data source may contain certain columns marked public and others kept private). public data sources are accessible every prediction engine, being source behavioral clusters for any future predictions. return, their providers are being compensated with edr tokens when they are selected the protocol. private data elements are still accessible the various prediction engines, albeit encrypted way. the clusters extracted from such sources however can selected source for predictions only when the user who requested these predictions provides the key for the data. see more details section accountability and censorship resistance: using blockchain, predictions are stored indefinitely, and can accessible anyone who interested deducing the reputation the platform, the data that was used for it, the prediction engine that analyzed it. addition, the endor.coin protocol contains open-source prediction code that charge selecting the behavioral clusters that are used for the generation each prediction (and the arbitration the funds paid for it), guaranteed free bias, optimizing accuracy only. see more details section and prediction efficiency for all: ultimately, the endor.coin protocol enables end-users obtain superior predictions low cost. this based the automatization the process (saving the need employ expensive full time data scientists), and the ability social physics seamlessly 'fuse' together various types behavioral data sources. this means that even large commercial customers, such coca cola, would able immediately benefit from migrating the endor.coin network flagging their data "fully private" they could offered predictive insights that are based the fusion the proprietary data, fused together with public data that was contributed the system, and pay significantly lower fee that the alternative cost obtaining this data their own, and analyzing it. this also offers positive network effect lowering the cost more users and data providers join the endor.coin network (see more details section .). chapter democratizing the prediction science social physics new science from mit social physics revolutionary new science which uses big data analysis and the mathematical laws biology understand the behavior human crowds, enabling endor overcome traditional machine learning limitations. this new science originated mit through research prof. alex "sandy" pentland and dr. yaniv altshuler. was further developed endor using proprietary technology, resulting powerful engine that able explain and predict any sort human behavior, even when the behavior rapidly changing and evolving. simply put, social physics based the premise that all event-data representing human activity (e.g. phone call records, credit card purchases, taxi rides, web activity) guaranteed contain special set human activity patterns that are embedded within that data. these mathematical invariances, which are common all human data-types, across all demographics, can then serve filter for detecting emerging behavioral patterns before they can observed any other technique. illustrating the power social physics: imagine that the marketing department large bank constantly calls customers who are potentially need loan the near future. the revenues the department are directly derived from the portion the customers actually responding positively the offer. the direct-marketing costs involved this ongoing campaign are significant, crucial contact the right customers, the right time: too late, and they might have already taken loan from another source. too soon and the need has not materialized yet. for this, the bank may consider two tools predict who these customers are: machine learning model developed in-house the bank's data science team; and endor's engine. here simplified representation what each tool recommended: the group customers that were detected the machine learning model comprised customers who will respond positively marketing offer the bank (e.g. true positives), well customers who will not (e.g. false positives). for example, let's assume that the true positives are the model's results. extensive experiments show that can expect the vast majority those also detected the endor social physics engine, with two main differences: (a) many the false positives the machine learning model will not reported the endor engine; (b) endor's results will contain many additional true positives, not detected the traditional model. the result was significant improvement the sales efforts, thanks endor's better precision recall trade-off. how? detecting temporal patterns: human reality composed many small temporary events and changes. social physics incorporates the underlying dynamics human behavior and therefore better equipped uncover small groups the population who are likely behave certain way due recent changes their social environments. social physics therefore uniquely capable identifying dynamic signals human behavior data: this because without the aid social physics such signals lack any sort statistical significance, rendering them indistinguishable from noise for traditional machine learning and deep learning methods. machine learning and deep learning vs. social physics which one better for which purpose? when solving business query using data science and big data analytics tools, both machine learning and social physics are viable options. the table below can help identify the appropriate tool, based its attributes. why social physics? rooted back the the various mathematical and statistical machine learning techniques were historically developed for 'static problems', such image processing and text recognition. such problems are dominated relatively small number relatively stable 'signals'. trained text recognition model would achieve similar perfor mance when processing the handwritten text mit student and when analyzing albert einstein's personal letters. similarly, neither siri nor google's speech recognition engine would find difficult transcribed high-quality recording j.f.k's famous "ich bin ein berliner" speech. human behavior, however, different story. governed multitude 'dynamic signals' highly dynamic and highly 'fractured'. traditional machine learning model trained detect millennials from credit-card purchases rapidly deteriorates accuracy over time, requiring constant maintenance skilled expert continuously incorporating new semantics knowledge into it. millennials' behavior subject frequent (and constantly changing) trends, locating this the data dictates not only constant re-training the model, but also the frequent development new features intended detect these trends (i.e. complex aggregative behavioral properties that are not part the raw data). this can only done through the combined work semantics domain expert working side side with data expert. following are the primary advantages using social physics tool for behavioral prediction human data: phase endor.com automatic prediction engine for enterprises discussed the previous sections, the state data science today, well the available machine learning technologies, dictate that the use such capabilities was preserved for deep pocketed tech giants. companies who can afford six-digit annual salary for these growing-in-rareness experts. however, even for those companies data science and predictive analytics were far from being commodity. the contrary project aimed producing reliable predictions would typically block team experts for iterations few weeks each, and after the first model produced would usually require constant maintenance. industry standards therefore estimate the overall cost average prediction project approximately million. this means that: most companies cannot afford fund more than handful projects. order have positive roi projects today must show huge margins, and focus only the most central business aspects. companies constantly have prioritize, aiming the super-expensive weapon data science only the projects they think would result (a) technological success (b) high business returns. endor, mit spin-off, was established around the novel science social physics, bid solve this problem, disrupting the way data science being perceived today. based years research mit, and additional years development elite team researchers and engineers, endor has developed the world's first ever fully-automatic "data science service" engine, that allows companies onboard any behavioral data they possess, and after quick integration (that typically requires mere few hours) start asking unlimited amount prediction questions regarding the future behavior any 'object' (users, products, coupons, locations, etc.) contained the data. for annual cost less than million, companies were given the opportunity ask dozens predictive questions, getting quick results within minutes. this not just linear improvement, but rather paradigm change, executing 'prediction project' became easy 'googling', virtually rendering the prioritization potential projects unnecessary, any decision could have now become prediction-oriented. example coca cola joint study: recent collaboration between endor and coca-cola the ability social physics provide accurate predictions regarding wide variety consumer behaviors was demonstrated. these included brand defection and loy alty, adoption new products, response marketing campaigns and others. using millions point-of-sale transactions its raw input (representing -months period), endor's social physics engine detected nearly million 'correlated anomalies', each representing single real-world behavioral group. whereas the exact meaning each group unknown, the groups are then used for 'behavioral extrapolation': given sample-set (e.g. early-adopters new product), the system uses the collection behavioral groups find lookalikes users who are behaviorally similar the members the sample-set (e.g. users likely experience the new product soon). using this methodology different predictive questions were asked, each yielding prediction report demand. reports high accuracy was demonstrated using out-of-sample data kept for validation. "social physics about behavioral analysis big data, but takes completely new level. were very fortunate find endor and work with it." dr. alan boehme, cto, the coca cola company example automatic analysis tweets: recent test million tweets' meta-data were provided the endor engine raw-data for analysis. addition, the customer revealed the identity twitter accounts known isis activists that were contained the input data, and tested endor's ability detect additional accounts that were hidden within the data. endor's engine completed the task single laptop only minutes (measured from the time the raw data was introduced into the system until the final results were available), identifying twitter accounts 'look-a-likes' the provided example, which (%) turned out part the list the hidden accounts. importantly, this provided extremely low false alarm rate false positive results), that the customer could easily afford have human experts investigate the identified targets. revolutionary concept and truly technological breakthrough. the results they presented are unmatched any competing tool." cio israeli intelligence corps the need for ecosystem-enabling decentralized protocol: using social physics and strong team professionals, endor's engine has demonstrated how large fortune company can pay significantly less, and get significantly more. however, this product was made available mainly for large banks and retailers, and still comes cost ranging from $.m for annual license. this course not deal that applicable most 'long tail' businesses, not mention individuals. therefore, the need for another solution became apparent. one that would allow anyone benefit from the new technology social physics, and reasonable price. such solution must therefore: self-reliant, comprised the resources made available its ad-hoc participants. generate strong network effect, incentivizing participants continuously join it, becoming better and cheaper grows. preserve fairness, and inherently unbiased and trustless. this requirement unlike other services, prediction expected have what can called 'an external effect', caused the action the person organization relying it. with this concept mind, the endor team proud present the next step its revolution, the endor.coin protocol! phase endor.coin protocol data science for the masses following the successful industry implementation social physics inspired behavioral analysis endor, the endor.coin protocol was created with the aim bringing this ability the long-tail business, well professional individuals. reinventing predictive analytics, the endor.coin protocol democratizes artificial intelligence for behavioral prediction enabling the creation ecosystem that makes accessible all. furthermore, the protocol's fully-decentralized nature guarantees trustless, censorship-resistance and accountability. for the first time, behavioral predictions become available for all, for affordable fee, secured framework, free from potential manipulation tech-giants who control data and technology today. definitions raw data: the endor.coin protocol supports any time related, transactional data source (e.g. call data records, erc blockchain, in-app purchases, and on). data providers must indicate the column that the basis for predictions. for example, taking the erc possible predict and discover addresses interesting future behaviors, tokens themselves. tweets data can used find interesting twitter ids, alternatively interesting locations hashtags. processed data: denotes data that has undergone extraction behavioral clusters and was transformed the social physics canonical representation 'knowledge sphere'. knowledge sphere: denotes the collection behavioral clusters extracted from one more raw data sources. the endor.coin protocol enables the split union any number knowledge sphere. different behavioral clusters are different relevant for different types predictions, for given prediction the endor.coin protocol selects the most relevant clusters any given time, and generates the 'knowledge sphere' that then used order generate the actual prediction. predictions: the endor.coin protocol support any question that can phrased the form "rank group its likelihood behaviorally similar group y". for example, group contains all the erc tokens and group contains tokens that have recently increased volume dramatically, then the prediction result would contain the list erc tokens, tanked such that the top contains tokens most statistically likely increase volume the near future, and the bottom containing tokens least likely display this behavior. data providers the endor.coin protocol supports the integration any behavioral, time-associated structured data. data onboarding done using simple api call, allowing the data owner contribute data while controlling which columns remain private and which become accessible the public analysis. data defined endor.coin users remain private still automatically integrated with public data-streams, producing high-quality predictive insights harvested from the fusion privacy and public data. using the notion social physics' knowledge sphere (see appendix data integration automatic and friendly requiring cleaning data-preparation. data providers are required pay edr tokens for the analysis their data, the providers prediction engines. they are, turn, being rewarded edr tokens when insights derived from their data are being used for predictions. this incentivizes the contribution and maintenance high-quality data streams. prediction engines the endor.coin protocol defines 'prediction language' that based the projection the data feature space partially overlapping behavioral clusters. the current endor engine would become the first prediction engine plugged that network, order make usable immediately after launch. however, endor.coin would facilitate, assist and fund the development new prediction engines, aiming for the creation ecosystem that comprised multiple types engines, providing complementary capabilities, boosting performance accuracy and increasing reliability. the growing number prediction engines would also provide guarantee for the unbias nature the results, predictions would created based the most relevant clusters, automatically chosen the endor.coin open source protocol from the collection clusters, extracted the various prediction engines. pre-defined predictions and request for predictions (rfps) the endor.coin protocol will launched with large catalogue containing variety predefined predictions. these predictions would accessible for purchase using the platform's edr token. endor.coin would then expand the selection predictions caters allowing users send requests for predictions (rfp) suggesting new types predictions, that would implemented and become available for purchase. this method gradually expanding the predictions supported the platform would utilize the wisdom the crowd (as manifested the requests the protocol users) optimize the selection newly supported predictions. private-data analysis and self-serve api the later releases the endor.coin protocol would include support complete 'do-ityourself' api for advanced users: tech-savvy users and professional data scientists would able use self-serve interface easily onboard proprietary data and create their own new types predictions. these predictions can defined private, shared with the public (rewarding the prediction develop with edr tokens, they become widely used. roadmap today's general software market, our approach can compared offerings such platform-as-a-service more recently blockchain-as-a-service. the endor.coin tokens (or edr will used power transactions the platform. edr serve key software license, and more tokens can used over time increase performance and scale via community developers that will enticed expand the areas applications. addition, dashboard will created for our administrators distribute tokens, monitor usage and purchase more tokens necessary. the next step the endor.coin evolution will expand our ecosystem organically ever growing community catalysts. everyone can provide rfps ("request for prediction"), paid for edr and the challenge addressing the respective rfp can undertaken any catalyst who will rewarded from the respective payments through smart contract for contributing expand our range predictions which will embed newly supported queries endor.coin (further accessible everyone, using edr this second phase endor.coin will become modular app platform which catalysts can expand the prediction domain through the power blockchain. enable this endor.coin endowed open ended stream micropayments authors reusable software components that can perpetually combined and recombined create ever expanding library useful, highly customizable query apps. catalysts are entitled micro use license for each component they add endor.coin platform. end users install the apps their choice. the license paid the sum all the micro use licenses the components used that app. through smart contracts endor.coin responsible for charging end users and distributing the payments the respective catalysts involved. with time endor.coin will become the premier development platform for entrepreneurial coders and enterprises looking build data-rich web and mobile products blockchain thus democratizing machine learning which far accessible only those highly skilled. furthermore, existing blockchains that today not support the volume transactions needed efficiently execute data-analytics algorithms will provided these capabilities, through the help ever growing community catalysts who will build applications using the engine continuously expanding the domain queries. our vision create fully automatic, trust-less decentralized predictions infrastructure, that will fully transparent end users. incentives are paid edr tokens and applications are essentially set plugins. positive feedback loop foreseen: the more applications built, the more plugins are added the system and more unique components are ready reused. this contributes mutually self-reinforcing network effects across the endor.coin ecosystem. ultimately will deliver platinum platform for high end customers, where professional experts and businesses can submit more complex fine grained rfps. deliver such complex queries will expand our data access rewarding providers with edr follows: when catalyst requests data from entity (timed limited) the respective entity proposes price (timed limited). the catalyst agrees the payment the entity sends link the data encrypted with the catalyst public key (at which point the money being disbursed via smart contract). reviews can published both catalysts and data providers (for future reputation). edr will generated from the arbitrage between the data providers payment and the funding received from the platinum application developer. chapter trustless, censorship-resistant and accountable accountability and authenticity for artificial intelligence his article "why you want blockchain-based ai, even you don't know yet" [], the author jeremy epstein shares with his readers conversation and his young daughter had with amazon's gadget alexa. this conversation revolved around the topic 'network neutrality', during which alexa shared with its listeners abundance relevant information. alas, given epstein's familiarity with the topic, had surprisingly discovered the information be, albeit accurate, potentially one-sided. the experience highlights some the risks the ai-powered future into which are hurtling warp speed. also reminder that big companies, such amazon, have traditionally had big advantages when comes big data and ai. quoting epstein: the race about gathering, storing, and analyzing much data possible, then who the pole position win? that's right, the fangs the u.s. (facebook, apple, netflix, google), the bats china (baidu, alibaba, tencent), and the wealthy fortune multinational corporations. they are the only ones with the reach and capital get more data, store it, analyze it, and build models top it. what's more, they are the only ones who can offer starting salaries the range and top-tier salaries that extend into seven and eight digits. your son daughter may not make the nba nfl, but become top scientist and you're doing great. the net effect all this that the rich become even richer and more powerful and the barriers innovation become even higher. not only innovation that suffers, however. the closed nature big-company means society must put its trust "black boxes." providing "prediction authenticity" therefore relies the availability infrastructure that would provide all the following: accountability: providers predictions can prove, retrospect, that their predictions were correct. consumers predictions can reliably deduce retrospect the efficiency (or lack of) any prediction provider produce accurate predictions any type. other words, reputation should impossible manipulate. authenticity: providers predictions are who they say they are. impersonation for the cause misleading prediction consumers should impossible. bias: there should fair competition among prediction providers, that would utilize market forces incentivize accuracy rather than implicit gain achieved through biased predictions. accuracy: predictions should accurate enough, generally speaking, provide monetary inflow the network, that would close the positive feedback loop with respect the previous three requirements. the endor.coin protocol provides infrastructure that would satisfy all the above requirements: endor.coin protocol accountability and authenticity: using blockchain, predictions are stored indefinitely, and are accessible everyone. when time-sensitive predictions are only sent consumers, they are also stored encrypted-yet-public version, with key that released after certain period time. endor.coin protocol zero bias: the protocol selects for each prediction the most relevant behavioral clusters (regardless the analytics engine, data source). therefore, the protocol separated entity, uncontrolled data providers, well analytics engines providers, bias inherently prevented. accuracy: endor.coin based mit's social physics technology, extensively proven the industry consistently provide accurate predictions for large variety use-cases. censorship-resistance through decentralized protocol censorship tricky business. when exists its explicit manifestation easy detect, and therefore bypass, through the use "low level" technological solutions such ipproxies, and on. implicit censorship the other hand different issue altogether. known that companies such google and facebook block certain types search queries. many times this done for the right moral legal reasons, but how can assured that this always the case? can "force" google provide relevant webpage, has decided that "should not have easy access it"? can "force" alexa comply with some our requests would deem inappropriate? the answer simple no. prediction this context very similar search, the sense that subject censorship the operating entity, the regulatory agencies they are subject to. look how and predictive analytics work, they have three essential layers with respect potential censorship: data repository: guaranteeing the integrity, completeness and security the data (are the inputs accurate and reliable, and can they manipulated stolen?) the algorithm/machine learning engine: making sure predictions are not inspected centralized authority, and that all prediction requests are being executed fairly, with quality-of-service considerations unrelated the topic prediction. queries interface: reliably representing the output the prediction query, effectively capturing new data, and having any limitations supported predictions being unrelated topic prediction. one going trust one's decision-making centralized prediction source, one implicitly assumes with absolute confidence that all the above requirements are met. centralized, closed model prediction scheme, when one asked trust each layer without knowing what going behind the scenes, such confidence difficult justify (if not plain gullibility). the endor.coin protocol inherently provides these assertions allowing any prediction executed, fully decentralized, trustless way. once data contributed and onboarded the network, any relevant prediction can executed, with the endor.coin protocol optimizing automatically. privacy-preserving data analytics: using encrypted data the use the novel new science social physics offers key advantage when comes the formation democratized ecosystem for data analysis and prediction: allows computing across data silos without compromising data privacy integrity, because the social physics computation can done encrypted data. specifically, the unique way which endor.coin analyzes data enables data contributors monetize the analytics their data encrypted form without exposing the data itself. instead, the system enables the extraction 'implicit behavioral clusters' (mapped onto hashed encrypted data space) that will subsequently used the basis for 'look-a-like prediction' resulting accurate behavioral prediction that oblivious the nature the source data, and the semantics the prediction question. similar technology already used endor today, working with large banks and financial customers, allowing the latter onboard their data, fully-encrypted version, rendering the compromise the data privacy and integrity impossible. network effects the full-decentralization the endor.coin protocol results among others several network effects, that increase its merits its usage widens. following are the main expected thrusts that will triggered increased adoption the protocol: adding users decreases the cost for each them: unlike large commercial customers, individuals and small businesses require predictions that are based mainly public datasets. the endor.coin protocol enables such datasets analyzed once generating collection behavioral clusters known "knowledge sphere". this data structure can then support all the prediction use-cases that are based this data source, requiring end-users pay small fee that encapsulates their "personalization factor" the delta between the extraction the behavioral clusters, and the specific use-case they are interested in. other words, for given dataset (i.e. the bitcoin blockchain), and for specific use-case, the cost calculated follows: knowledge sphere calculation: consists roughly the overall cost, and calculated once. resources required for this task are amortized among all users this data-steam. prediction personalization factor: consists roughly the overall cost, and calculated (and paid for) for each user. addition, for similar predictions made different users, the endor.coin protocol automatically reuses insights derived from the prediction that calculated first, during the calculation the later ones, further improving prediction accuracy for the same cost. this means that the governing cost equation network end-users would be: for denoting the cost the same prediction single commercial player. this ultimately means that the number users increases, the accuracy the predictions increases, while its cost aspires approximately the cost the same system, used large commercial customer. ultimately, the more people asking questions the better the results they receive, and for lower cost, the vast majority the cost divided across all active users. adding data-providers increases accuracy: per the definitions the endor.coin protocol, data providers are required fund the execution their data using edr tokens, for creating the "knowledge sphere" that based the behavioral clusters that are extracted from the data. this initial execution cost then repaid the data-providers part, equally, with significant interest, the end-users all according the quality the data, and its contribution the various prediction queries. this funds arbitration being taken care the endor.coin protocol, whch able access the entire collection behavioral clusters detected the various analytic engine, the various datasets, and select the top clusters highest-relevance. execution tokens are then delivered the providers these datasets, pro-rata, with respect their contribution the final prediction. the result this mechanism that owners high-quality data are incentivized continue supporting their data sources (and even further increase their quality and availability), while providers data poor quality are being rooted out, simply because their costs are not being repaid. this economy utilizes the market forces automatically guarantee that the available data-sources the prediction protocol remain the highest possible fit. new data-providers can therefore only increase overall prediction accuracy, without increasing their overall costs. adding prediction-engines increases prediction efficiency: the first phase the endor.coin project relies the endor prediction engine act the first provider behavioral clusters extraction. however, time additional prediction engines are expected support the endor.coin protocol. the introduction such new prediction engines expected have significant positive effect both data-providers well end-users: engines that utilize different technologies are expected produce different types clusters from the same data source. this means that the variety technologies used for predictions engines that support the endor.coin protocol increases, will the variety clusters which will become available for the protocol select from, when new predictions are being requested. this expected have three main effects: improved accuracy, and increased support for new predictions: new types clusters become available, the endor.coin protocol will able better select clusters used for the generation each requested prediction. for existing predictions this would result increased accuracy (due the availability "orthogonal insights", derived from the use clustering technology). however, this would also imply that new predictions, that until that point were not supported the system due suboptimal accuracy, would now become cost-effective execute, increasing the variety predictions supported the system, and subsequently further boosting the overall accuracy, due the sharing insights among predictions executed adjacent times. reduced cost per prediction: the increase the amount predictions supported the system would also result increase the number end-users who pay for predictions immediately increasing the overall pool available edr tokens paid for the initial data analysis, and subsequently increasing the amount funds received the data-providers, while decreasing the cost-per-prediction for the end-users. economic sustainability new data sources: finally, the availability new clustering technologies may also render the contribution certain types data sources economically feasible, cases where these new technologies are better compatible for the analysis such data sources then the available ones. such cases, such data sources would suddenly become viable source information for the endor.coin protocol, repaying the cost their initial integration the platform, and subsequently also increasing the amount available data sources, with the benefits that are derived from it. chapter enabling ecosystem the endor.coin project aims towards the establishment multi-faceted ecosystem, creating synergic collaboration between data owners, developers, data science professionals, small businesses, and individual users. enabling each these players contribute their assets (be data, funds, ideas regarding new predictions that should added, and on) the endor.coin community will and enable and facilitate self-perpetuating positivefeedback business loop. following short description regarding the main aspects this ecosystem. public data providers the endor.coin protocol would incentivize from day one the contribution public data streams, provided first phase 'registered data providers' (undergoing brief compliance and quality assurance process, making sure the ecosystem gets bootstrapped with the highest quality data), and later any person company that wishes enrich the data available for analysis, and being rewarded edr for doing so. examples for potential data contributors are data partners: projects such enigma, twine, thasos, ocean protocol and others, connecting their data stream (either partially full) the endor.coin platform for analysis. social channels: such twitter, reddit and others. data scrapers: harvesting, cleaning, structuring and semantically enriching data from variety publicly available sources. blockchain protocols: such orbs similar, establishing node that would download and parse the data, and make accessible for analysis. endor.coin already negotiating partnerships with several such potential contributors. academic research groups the endor.coin protocol supports the integration multiple analytics engines for clustering raw data, the company intends utilize some the proceeds order encourage the collaboration with leading academic research groups the field. technologies that would developed during this activity could easily integrated the growing endor.coin ecosystem, further rewarding their developers with edr tokens whenever they are used enrich the accuracy predictions. this effort would spearheaded under the guidance endor.coin scientific advisory board (see detailed list section that comprised world leaders data science, machine learning and blockchain technologies from the industry and academia. mit spin-off that relies the scientific revolution social physics that was developed mit the project's founders, the company intends strive for the creation strong and sustainable alliance, led endor.coin encompassing leading research groups the academia, blockchain projects the fields data analysis and data marketplaces, and infrastructures for the proliferation and usage data insights and predictions. catalysts application developers increase the range questions that endor.coin can address, the domain infinite and limited only those who want build applications using the social physics engine, the community "catalysts". fueled them, endor.coin will become modular app platform which catalysts can expand the prediction domain through the power blockchain. increase the range questions that endor.coin can address, the domain infinite and limited only those who want build applications using the engine, the community "catalysts" endor.coin will become modular app platform which catalysts can expand the prediction domain through the power blockchain. endor.coin enables open ended stream micropayments authors reusable software components that can perpetually combined and recombined create ever expanding library useful, highly customizable apps. catalysts are entitled micro use license for each component they add the framework. end users install the apps their choice. the license paid the sum all the micro use licenses the components used that app. through smart contracts endor.coin responsible for charging end users and distributing the payments the respective catalysts involved. everything automated and transparent end users. incentives are paid tokens. applications are essentially set plugins, positive feedback loop foreseen: the more applications built, the more plugins are added the system and more unique components are ready reused. this contributes mutually self-reinforcing network effects across the endor.coin ecosystem. data sovereignty world which consumes our privacy for the benefit greedy corporations, endor.coin committed changing the game and return the wealth back those who provide access their data. simple sign-up endor.coin starts the process which can unfold according various scenarios enabled the edr tokens. members can receive individual insights based their own data team group aggregate larger pool data working together with small business get more personalized services, for example zooming the right product, the right time, for the best price. based the fundamental belief that people have the right own, control and benefit from the data they generate endor.coin opens the power insights currently available only those with deep pockets, the far disadvantaged long tail small businesses which can now aggregate endor.coin members' digital data for their own use, with the data owners' permission, provide high value data analysis that brings back its members both highly efficient services resulted from that analysis, well monetary gain when the data utilized other ways. this personal data independence based approach, championed endor.coin unlocks tremendous value for both companies and people, without loss value trust for either. endor.coin supports businesses co-opting membership and brand participation negative acquisition cost companies and other organizations sign eliminate data liability, access superior data about their customers, and develop customer relationship. membership will continue increase, endor.coin will also provide suite tools and services that will help members gain insights and understanding regarding many aspects their lives from their own data. with the mission "data the people, the people for the people" endor.coin gives back the individuals their inherent right capitalize their lives which have been far violated the likes facebook, google, uber and many other data-based conglomerates, thriving the "platform economy". through multi-faceted approach that integrates proven enterprise expertise, forward legal thinking, technical knowhow and value creation for consumers, endor.coin offers unique business model which resolves the "data piracy" problem creating consensual data relationship which enables companies generate good will with their customers rewarding members directly for access their data insights. ensure the integrity the process are creating endor.coin trust, independently managed entity that works synergistic relationship with endor.coin maximize the benefits, value creation and data security for members. people sign endor.coin their data overseen the trust and they automatically become trust members. the endor.coin trust represents its members many ways ensure and maximize the safety, security, privacy and value their data. the trust protects members' data setting and enforcing standards personal data control, value realization and privacy. chapter token implementation blockchain structure data storage: blockchains are not general-purpose databases. endor.coin has decentralized off-chain distributed hash-table storage that accessible through the blockchain, which stores references the data but not the data itself. private data encrypted (using either aes- amazon server-side encryption, aws iam mechanism) prior transfer and storage, having the access-control protocols programmed into the blockchain. endor.coin designed connect existing blockchain well private and public datasets off-chain network (stored any form database, central storage, etc., that can exported into structured format that eventually uploaded endor.coin aws infrastructure). future releases the data layer will opened external players which will able sell data for edr tokens. the data will certified and uploaded the endor.coin off-chain infrastructure, where will available the owners the data, and marked 'public' for any customers for which the prediction algorithm deem relevant. the pricing will determined the data provider, and adjusted automatically the endor.coin protocol dynamically, according the demand for this prediction, incentivizing users share the cost for the same data source. consuming predictions: the blockchain cannot handle heavy complex transactions. the same off-chain computational network used run heavy computations (required the various predictive engines). once results are available, they are broadcast throughout the public blockchain for end users use (authenticated using the key the user who requested the prediction). parallel, the same results are encrypted using different,. temporary key, and are broadcasted publicly. this key released following pre-defined time, allowing results authenticated later stage, even users who did not require them originally. this mechanism can adjusted way that creates such multiple temporary keys, with varying cost that depends the 'freshness' the prediction. processing data: code execution divided for execution the blockchain (public predictions, public rfp orders, private rfp orders) and endor.coin infrastructure based using proprietary hyper-elastic computational layer running aws, similar environments (for example, the golem project). processing data and extracting behavioral clusters requires complex and expensive execution environment, later releases the endor.coin protocol would open the execution layer the protocol well. this would enable new type stakeholder join the endor.coin ecosystem one that specializes intensive execution. the larger the data analyzed and the heavier (computation-wise) the clusters-extraction algorithm the higher the price edr tokens needed run will be. payment: user can pay for every prediction request amount edr depending the prediction complexity, dynamically defined the endor.coin protocol, per the available resources and the demand the time request, and the number users asking similar questions. pre-defined queries are expected remain relatively steady price, whereas rfps (request for prediction) would start with relatively high cost (as definition they start with small number users), gradually decreasing community users for which relevant, formed. the larger this community the less everyone eventually pays, incentivizing the creators new prediction queries 'spread the word' among people their network. smart contracts the endor.coin protocol provides several basic primitives for its end users: getprediction(prediction def) implemented phase putpredictionreq(prediction def) implemented phase putdata(data def) implemented phase runcustomprediction(data alg prediction def,price) implemented phase these primitives allow customers retrieve predictions available the endor.coin platform for dynamic cost and later upload data and sell used for prediction purposes. while the primitives cover the default use cases for the endor.coin protocol, future releases would enable far more complex operations designed top get and put supporting deployment smart contracts. the protocol progresses towards private data and custom private predictions, will enable additional layers security and complexity top the basic smart contract that will introduced the first public predictions version. smart contracts would enable endor.coin users write stateful programs that can spend tokens, request predictions and used for retrieval data the markets, well validate data quality proofs. users will able interact with the smart contracts sending transactions the ledger that would trigger function calls the contract. the smart contract system will extended support endor.coin specific operations (proof verification), supporting contracts specific data upload (which will used for public private use later stages), well more generic smart contracts. getprediction(prediction def allowing users retrieve predictions stored the endor.coin network paying edr tokens. clients initiate the get protocol submitting bid order the retrieval predictions market order book (by propagating their order the network). when matching ask order from prediction provider, the client receives private temporal link the prediction. when received, both parties sign deal order and submit the blockchain confirm that the exchange succeeded. the prediction consumer will pay the equivalent price which was attached the prediction that asked get. the predictions will accessible for download from the endor.coin webportal. putpredictionreq(prediction def allowing users suggest additional prediction types submitting the endor.coin platform. the interface will accessible the endor.coin webportal. the suggested predictions will visible all endor.coin users and they will able rank them. the most highly ranked predictions will added the continuously growing predictions catalog. prediction requests will identified with public address the prediction issuer, once the prediction added the catalog, the issuer the prediction will compensated each time the prediction consumed other users. putdata(data def used for data providers and partners. data providers are rewarded edr tokens whenever their data being used for predictions, having the pricing automatically calculated the endor.coin protocol prediction algorithm. this variant the function will accessible the second phase the endor.coin project. during the third release the protocol api for 'do-it-yourself' mode, allowing customers upload their proprietary data streams, marking them 'private', and having them fused together with the public data streams that are accessible the platform. client initiates the put protocol submitting bid order the storage market order book (by submitting their order the blockchain), waiting for matching ask order placed from data validator (e.g. the prediction engines providers). clients are required fund the extraction behavioral clusters the prediction engines, but are then able decide the price they want get for each usage the end users. runcustomprediction(data alg prediction def, price): this method would implemented the third release the protocol, supporting the creation predictions that are not included the endor.coin catalogue, 'do-it-yourself' mode. input contains the description the desired behavior, the way example, description logic that refers publicly proprietary data. maximal price defined, the request can handled any available engine. the fees will arbitrated between the different players that contributed the prediction (e.g. the prediction engine and data providers). endor's roles launch described details the previous sections, the endor.coin protocol enables the intercommunication among multi-faceted players (e.g. data providers, analytics engines developers, and on). this sense, the protocol not source predictions per-se, but rather but rather enabler and language, that allows the creation value the 'edges'. similar the tcp/ip protocol that provides easy communication between various value-offerers, the endor.coin protocol would enable data-providers offer their goods, prediction-engines offer their analytics services, and catalysts offer ways that these would able used for the creation synergic value offering. for this, endor.coin would operate under separate legal entity, focused the implementation the protocol, and securing the optimal composition the its surrounding ecosystem, and maximizing its growth acceleration. however, order guarantee swift establishment production-grade workflow the protocol, and allow customers consume variety accurate predictions early possible (and even day one), the endor.coin project would optionally feature endor actor playing several key roles. later on, new players adopt the endor.coin protocol, endor's roles are expected decrease dominance, making way innovation and value that would introduced new participants the endor.coin ecosystem. following are the main roles provided endor upon endor.coin launch. each these activities would require dedicated funding, that would allocated from the proceeds received during the ico exchange for edr tokens. the companies are now negotiating the exact details this collaboration. public data provider: endor would use its dev team implement production-grade infrastructures that would harvest and clean variety data streams, uploading them the endor.coin infrastructure data ready for analysis. these will include the bitcoin blockchain, the erc blockchain, and several other proprietary blockchains. addition, this will later support social network feeds such twitter, reddit, etc. endor would provide this service 'cost plus' basis, covering its expenses. processed data provider: endor would act the first analytics engine that supports the endor.coin protocol, contributing its capability for extracting behavioral clusters from transactional data streams. endor would provide this service 'cost plus' basis, covering its aws expenses. demand provider: endor would act demand provider, channeling hunger-for-predictions from its existing (and new) enterprise customers. endor customer-success crews will use usd paid enterprise customers and purchase edr tokens, generating the desired predictions. applications and partnerships development order further expand the endor.coin ecosystem and proliferate the penetration endor.coin based predictions, endor would actively strive for the creation new prediction-based businesses, either through partnerships, forming new types predictions and releasing them the public acting catalyst. this would utilizing the business-development team endor, well its large and high quality advisory board. token privileges and economy discussed previous chapters, the edr tokens issued endor.coin would utilized the payment mechanism for variety services offered the endor.coin ecosystem. noting that some these service would rendered free-of-charge the initial phase the project order facilitate its growth, following are the main usages the edr tokens purchasing pre-defined diy predictions: the main usage the edr tokens expected the payment mechanism for the consumption predictions. these would include pre-defined predictions available the ever-growing catalogue, well (at later stage) "do-it-yourself" predictions, ordered the users using dedicated self-serve api. the payment would mainly cover the cloud-computation resources (e.g. amazon aws, golem, etc.), with allocated payment the providers the data used for the creation the predictions, pro-rate basis. cost predictions would pegged the cost cloud resources required generate it. the main component the prediction generation process shared among different predictions, this would create cost function that asymptotically decreases with the increase the number users, with temporal spikes associated with sudden increase demand: payment for data providers between the prediction cost. cloud resources for data analysis (aws similar) approximately cloud cost, divided the number active users (namely, between and from the overall cost prediction, divided the number active users). cloud resources for personalized prediction (aws similar) approximately cloud cost (namely, between and from the overall cost prediction). qualitative payment model for prediction cost would therefore redict ion redict ion such that redict ion denotes the cost single prediction the launch the endor.coin platform and denote the number active platform users. submitting rfps (request for predictions): users interested specific predictions that are not yet supported the catalogue, could apply for the creation new predictions, using the rfp mechanism. such case, users will pay edr tokens covering the cost time required for the creation and optimization the prediction skilled endor.coin team-member, well the cloud resources requires. this sense, rfps would pegged the combination cloud-cost and the average market salary for skilled data engineers. requesting qos preference for predictions: when purchasing prediction, requesting new prediction through the rfp mechanism, user may select purchase premium access the prediction, either receiving several hours before being released the non-premium purchasers, the case the rfp mechanism retaining exclusive access for pre-defined period time. such quality-of-service elements would possible purchase multiplying the cost the prediction pre-defined factor. this sense, this premium-service would also pegged the cost cloud resources and market salaries, multiplication the basis cost prediction. submitting new data streams for the platform: data provider who wishes contribute new data stream for the endor.coin ecosystem will required pay the cost the cloud resources required for the initial analysis the data, and its adaptation the endor.coin protocol. this required order provide positive incentives for the integration high-quality data sources, order enhance the overall quality predictions. this initial process fee would grant the data contributor the right have their data supported the endor.coin protocol source behavioral clusters for predictions, and subsequently, also the right compensated edr tokens when being used create predictions. use proceeds proceeds received from the purchase edr tokens will used implement the endor.coin protocol software infrastructure, support its initial growth and adoption through marketing and strategic collaborations, fund its required cloud resources, and cover required legal and administrative expenses. the main part the proceeds will allocated for endor.coin r&d team that comprised cutting edge cloud and blockchain engineers, well world experts data science, machine learning and social physics. important role endor.coin project and team would guarantee the quick adoption the endor.coin protocol many leading players possible. one the ways this will achieved forming joint research activity with the academia, providing leading research access the endor.coin protocol and infrastructure day one. the proceeds will allocating support such activity, with world leading research institute. the proceeds will allocated for the possibility purchasing relevant proprietary technologies such prediction engines etl connectors, these are required for facilitating and accelerating the proper bootstrapping the endor.coin protocol and its surrounding ecosystem. chapter technological advantages and differentiation scientific revolution from the mit furnaces powered mit's novel social physics technology [], endor.coin utilizes the world's most advanced behavioral prediction technology. this scientific breakthrough that started the mit media lab the beginning this decade revolutionized the field behavioral data analysis, and was the forefront technological accomplishments which included winning the prestigious darpa network challenge [], boosting the returns community retail investors and successfully forecasting the existence efficient unknown cyberattacks []. this technology was developed team academic and industry experts, that have collectively published hundreds scientific papers, dozens patents, and several books dedicated these subjects. and now, this revolution reaches the public, utilizing blockchain technology order provide professionals and laymen alike access capabilities that hitherto were the sole privilege large retailers, banks and tech giants. detailed discussion about social physics appears section and appendix real product. proven technology previously outlined section endor mit spinoff that took upon itself carry out the implementation social physics product, designed offer large banks and retailers saas solution that would open their predictions bottleneck. the company backed leading investors [], and works with fortune- companies such coca cola [], mastercard [], walmart and others, successfully demonstrating its ability automatically generate accurate predictions for variety use-cases. the value the technology for blockchain and cryptocurrency analysis was demonstrated and others. endor's product has been featured leading venues, such money- andfinnovate []. endor gartner cool vendor [], and was recognized the world economic forum "technological pioneer" []. usability, value users and value for token holders key uniqueness endor.coin that the edr token will usable day the token launch offering token holders complete access the pre-defined predictions. addition, group beta users will selected shortly after token launch, and will given the opportunity request predictions addition the pre-defined ones. furthermore, token owners who believe they might require affordable access predictions the future would incentivized buy and hold edr tokens, expected increased usage will result increased cost predictions, driving token value up, rewarding prediction users, who purchased edr tokens early on. chapter team key team members dr. yaniv altshuler dr. altshuler the ceo endor, and research affiliate the mit media lab, were together with prof. pentland developed "social physics", new science that models crowds behavior. mit, the technology was used win the prestigious darpa challenge [], help the government singapore improve its ability predict traffic jams, and assist community thousands financial investors improve their financial returns []. endor, the technology was used accurately predict the behavior crowds large variety use-cases, efficiently catering large banks and retailers []. prior his position mit and the incorporation endor, altshuler was researcher ibm, developing novel optimization technique used boost the performance supercomputers. active blockchain research since dr. altshuler has authored over scientific papers and filed patents. altshuler's works have been featured popular venues such the financial times [], harvard business review and others. his recent published books are 'security and privacy social networks' and 'swarms and network intelligence search' []. prof. alex "sandy" pentland (chairman the scientific advisory board) director the mit media lab entrepreneurship program, well the mit connection science and human dynamics labs. prof. pentland one the world's most-cited scientists [], and was recently declared forbes the most powerful data scientists the world" along with google founders and the chief technical officer the united states []. has received numerous awards and prizes such the mckinsey award from harvard business review [], the anniversary the internet from darpa [], and the brandeis award for his work privacy. founding member advisory boards for google, at&t, nissan, and the secretary general, serial entrepreneur who has co-founded more than dozen com panies, well social enterprises such the data transparency lab, the harvardodi-mit datapop alliance and the institute for data driven design. member the u.s. national academy engineering and leader within the world economic forum. his most recent books are 'social physics', and 'honest signals'. stav grinshpon mr. grinshpon veteran tech-industry expert, with years experience product and management companies such sap and at&t. grinshpon world expert cyber defense, serving years technological leader the israeli technological unit. grinshpon the author patents focusing data analytics, and headed the r&d activities endor. david shrier david shrier globally recognized authority financial innovation, and leads the university oxford online programmes oxford fintech and oxford blockchain strategy, well mit's future commerce. has published multiple books fintech, blockchain and cybersecurity, including new solutions for cybersecurity, frontiers financial technology, and trust::data shrier the ceo distilled analytics, machine learning company derived from mit research that transforming the financial services industry with behavioral analytics, and chairman riff learning, ai-driven collaboration technology platform provider. david associate fellow with the said business school, university oxford, and lecturer the mit media lab. also counsels the government dubai blockchain and digital identity; millennium advisors, middle market credit liquidity provider, technology trends; and cleer.digital, blockchain-based digital commodities exchange, strategy. david the advisory board worldquant university, program offering totallyfree, accredited, online masters degree financial engineering. previously advised the european commission commercializing innovation with focus digital technology. presently member the fintech industry committee for finra, the u.s. securities industry's self-regulatory body, counseling new developments impacting financial services. prof. mihaela ulieru expert computational intelligence, blockchain champion the world eco nomic forum []. her research distributed intelligent systems created strong foundation for governance blockchain institutional technology after revolutionized manufacturing, logistics and homeland security. prof. ulieru, california berkeley alumna, member the world economic forum's global agenda council, the science and engineering research council singapore and the canadian science technology and innovation council. dr. goren gordon dr. gordon the head the curiosity lab the tel aviv university, where develops state-of-the-art models computational curiosity. gordon, leading expert deep learning and neural networks optimization, holds phd quantum physics, and another phd neurobiology. dr. gordon had utilized this experience, together with his additional degree medical sciences, during his work mit media lab with prof. cynthia breazeal, where studied how curious robots interact with curious children. gordon also interested scientific education, cause for which developed "quantum computer games" that teach quantum physics children via play. gordon also popular lecturer the topics quantum physics, the brain and inter-disciplinary thinking. dr. arie matsliah dr. matsliah world expert the theory graphs analysis, and spent years working for industry giants such google, ibm, intel and lyft well being the chief architect tripactions, successful menlo-park based start-up. dr. matsliah has also published over scientific papers, focused fundamental research algorithms, complexity, and quantum computing. shahar somin-gavrielov ms. somin-gavrielov expert statistical learning theory, holding masters degree (with honors) from the hebrew university. somin-gavrielov seasoned researcher, combining deep theoretic realization data science with hands-on industry experience. her past, she was decorated analyst the israeli intelligence unit. edo eisenberg financial risk management professional, previously managed retail credit portfolios morgan stanley and barclays. additional years experience nice, designing and implementing fraud prevention solutions deployed top u.s. banks. alumnus the duke mba program. lior regev mr. regev seasoned software engineer with great passion for technology. regev ex-intelligence tech-leader, with vast experience distributed systems, cloud architectures and saas products. liat yitshaki ms. yitshaki expert public law litigation. she holds law and ethics and llb with honours from the university london. the past yitshaki worked for mckinsey co., well senior legal adviser the british government. advisors prof. alexander lipton professor lipton served the managing director the global quantitative solutions for bank america, well the managing director global quantitative and credit analytics groups merrill lynch. prior this, prof. lipton was the head quantitative research citadel, and head equity derivatives credit suisse. lipton currently professor financial engineering epfl well fellow mit connection science center, and recent author the scientific american article titled "how technology could help fix our broken financial system" []. ron gross ron gross the founder and board member the israeli bitcoin association. has been active the bitcoin and blockchain worlds since and was the executive director mastercoin (now omni the world's first ico. gross ex-googler, and has also served the chief architect commerce science. dr. nuria oliver dr. oliver, mit alumna, the director research data science vodafone and the chief data scientist the data-pop alliance. formerly, dr. oliver was the scientific director telefonica r&d and researcher microsoft. oliver world expert data philanthropy the use big data analytics form collaboration which private sector companies share data for public benefit. oliver has authors over scientific papers and book chapters and has filed over patent applications. dr. daniel tunkelang world expert data science, tunkelang, ex-googler well ex-ibm-er, advisor tech giants such apple, salesforce etsy, yelp and pinterest. previously, tunkelang had served the director engineering search linkedin well the latter's data scientist residence, well being the chief scientist endeca (acquired oracle). tunkelang holds masters degree from mit, and phd from carnegie mellon university. guy zyskind founder and ceo enigma, the company behind the enigma protocol for creating secret smart contracts, and catalyst platform that allows anyone start crypto hedge fund using sophisticated tools and data. prior enigma, guy was graduate student mit, researching and teaching blockchain technology. guy authored several academic papers, most recently privacy and the blockchain, including the enigma whitepaper and "decentralizing privacy: using blockchain protect personal data". guy holds m.s. from mit and b.s. electrical engineering and computer science from tel-aviv university. prof. michael bronstein serial entrepreneur and leading researcher, prof. bronstein research fellow harvard university, and professor informatics the university lugano. bronstein has authored over publications leading journals and conferences, over patents, the research monograph "numerical geometry non-rigid shapes", and edited books. bronstein one few researchers who received european research council (erc) grants, and was also awarded the google faculty research award, the radcliffe fellowship from harvard university, and the rudolf diesel industrial fellowship. was selected the world economic forum one the world's leading researchers under the age forty. prof. bronstein actively involved industrial applications. co-founded and served vice president technology novafora (licensed turner broadcast) and was co-founder and one the main developers the sensing technology company invision (acquired intel, where bronstein currently serves research scientist for perceptual computing). bronstein co-founder and technical advisor videocites. dr. wei pan dr. pan mit alumnus, ex-googler, and world expert big data analytics, machine learning and complex systems. the inventor the analytic approach known "reality hedging", that tries understand financial markets dynamics and macro economics through the understanding and models social systems, and big data measurements people and crowds. currently, dr. pan co-founder and chief scientist thasos group, new york based start-up. pan previously worked fidelity investments under the chief economist where focused systemic research and the flash crash. igor gonta mit alumni engineering and computer science, mr. gonta the commodity and risk management blackrock. his past gonta served the ceo market prophit real-time stocks sentiment generation engine, that based social-media conversation analysis. gonta was the founder the company, which then sold large hedge-fund. his early career gonta served the managing director commodities sales barclays, well vice president commodities sales goldman sachs. thomas hardjono hardjono security expert, specializing decentralized identity, blockchains and smart contracts. hardjono was the executive director verisign and the mit ker- beros consortium, and has published books dedicated computer security and cryptography [-]. appendix social physics explained how social physics works? clarification: this document intends provide comprehensive view social physics, the high level principles behind it, well its technical implementation endor. however, certain technical details regarding the specific mathematical formulation the social physics laws that are used endor, well the specific implementation the mechanisms used order detect violations these laws, were intentionally omitted, due considerations. the information age, companies gather data all types and from numerous sources about their businesses operations. data encompasses images and videos, text and tweets, transactions and usage logs. however, the majority data originates from single underlying source: people. thus, for example, tweets and blog posts are written humans for humans; purchase transactions and phone call information convey human desires for things and other people; usage and app logs report how people interact with computers and mobile devices. data derived from human behavior "messy": dynamic, complex and extremely versatile. humans' behavior, recorded such digital data channels, changes drastically over time, influenced underlying complex social networks, and conveyed highly multimodal data streams. these characteristics pose significant challenges for companies that wish analyze, understand, and predict their customer behavior order improve their business operations. recent years, data scientists have started employ "heavy-weight" statistical methods and machine learning algorithms try and cope with this complexity. these powerful tools, including the new "deep learning" techniques, collect data and analyze its attributes order able classify behavioral patterns, detect anomalies, and predict future trends. however, such tools historically developed for "static problems" such image processing and text recognition cannot easily cope with human behavior data: learning dynamic, complex, and versatile data streams extremely hard and sometimes nearly impossible. _________________________________________________________________________ appendix social physics explained endor's social physics engine works completely different way. instead deriving patterns from input data itself, based the discovery that all human behavioral data guaranteed contain within set common "social behavioral laws" mathematical relationships that emerge whenever large enough number people operate the same space. these laws govern the way various statistical properties crowd behavior evolve over time, regardless the type data, the demographics the users who created it, the data size. endor has integrated these laws into its data analytics engine, which efficiently extracts the underlying social attributes all people contained the raw data being provided input (e.g. phone calls, taxi rides, financial investments, etc.). _________________________________________________________________________ appendix social physics explained why social physics needed? abstractly, learning problems, the ability classify objects produce predictions for future events, requires data analyzed, and algorithm that analyzes it. the quantity the required data depends several factors: internal complexity the problem problems come different shapes and sizes, and some are undoubtedly harder than others. the "hardness" "complexity" problem refers the minimal "strength" learning algorithm must possess order successfully learn the problem: learning algorithm not strong enough, will simply not able learn instance the problem correctly. for example, can easily shown that "perceptron" (i.e. the simplest neural network, comprised single neuron) can never learn the "xor" function (namely, the boolean "exclusive-or" function). the reason embedded the way perceptron works (which can imagined linearly dividing the input space) whereas the xor function simply "too complicated" represented this way: example the xor function appears the right chart (source: wikipedia) the learning-complexity the underlying model problem often referred the vapnik-chervonenkis dimension (or dimension) the problem. the higher the learning-complexity problem is, the stronger prediction algorithm needs order able learn successfully, the more data such algorithm requires order properly model it. for example, function f(x) that has linear complexity (meaning, can well-modeled using polynomial rank requires, definition, less sampling points than function that requires polynomial higher degree order accurately modeled. _________________________________________________________________________ appendix social physics explained learning efficiency the algorithm there are many learning algorithms, each requiring different quantities training data and expert domain knowledge properly ascertain the model parameters. for example, simple regression requires large amounts data and many problem-specific features work well, whereas deep learning, while requiring vast amounts data, can automatically learn the domain features. for our discussion can therefore quantify the overall efficiency the learning process, that how much data would required order learn the underlying model, and how much does the algorithm learn itself the proper representation: (m) (m) bits model, the number bits required formulate model the data, e.g. number parameters perfect description the model would require. this sense, akin the kolmogorov complexity the problem known theoretical measure the data complexity, referring the shortest turing machine that can generate given data. the model determined the problem and cannot changed, i.e. it's simple problem then the model has few bits. ra(m) bits model-specific representation, the number bits learned the algorithm represent the underlying model this number represents the automatic feature detection the algorithm and inversely proportional the number domainspecific knowledge experts must manually program into the algorithm. da(m) bits data, the number bits required the algorithm learn model learning efficiency the algorithm that is, the ratio between the automatic representation learning the algorithm and the amount data required preserve its learn quality. high efficiency algorithm can learn the proper representation with small amounts data, whereas low efficiency algorithm requires manually crafting features and vast amounts data tune them. for example, given problem class some algorithms would require more data than others, preserve the learning quality: logistic regression algorithms usually require manual coding features experts and large amounts data fine-tune them the specific problem hand: _________________________________________________________________________ appendix social physics explained rm(a) dm(a) endor's algorithm uses social physics automatically extract the relevant behavioral features from only small sample the data: rm(a) dm(a) rate change the problem another factor that influences the amount data required produce accurate prediction the rate change the problem's underlying model. some problems are static, wherein their underlying parameters not change, change rarely. for example, faces images not change over months; faces are faces. the other hand, underlying behavior patterns that lead the churning out paid service may change over time, either gradually via social changes over months, rapidly matter days, response successful marketing campaign competitor. quantify this dynamics follows: the problem's persistence tenacity, denoting the rate which the underlying model changes, where represents the duration over which the model changes bits. for example, day bits means that the model changes drastically over period one day (i.e. fast rate change) compared month bits which refers much slower change the model. effectively, dynamic model presents different model every period, requiring re-training re-learning the model. _________________________________________________________________________ appendix social physics explained operational implementation predictive analytics: chasing the changing model the main challenge feasible implementation predictive analytics for given problem therefore obtaining enough information cope with the behavioral change the modeled objects. the amount information bits that one can accumulate per one time unit denoted it. operators extremely-large social networks search engines (e.g. google facebook) can often accumulate vast amounts information relatively short periods times. this however impossible for the majority the companies interested predicting their customers' behavior. addition, even large players that acquire vast amounts information every day would find challenging accurately models problems that are either (a) too complex, (b) change too quickly, course (c) combination the former two. can therefore point simple equation that determines the ability company implement operational predictive analytics solution. companies that are able satisfy this principle using the learning algorithms they employ and new data they continuously acquire, for the problems they are interested predicting, will able successfully construct operational process that achieves this goal, whereas companies who fail so, will not equally successful. the fundamental operation learning principle: the practical implications this principle dictate that companies who fail their attempts construct operationally functioning prediction systems should either: improve their data collection bandwidth, acquiring larger amounts relevant data per day; focus more static problems; resort more efficient learning algorithms. _________________________________________________________________________ appendix social physics explained this simple relation means that one needs more information per day the problem's complexity increases and its persistence decreases illustrated the following chart: easier for "static" problems [days/bit] [bits/day] easier for larger streamer incoming data here how few common predictive problems look when analyzed with this easier for "static" problems [days/bit] [bits/day] easier for larger streamer incoming data face recognition: this problem characterized very low change rate, namely >>, generally speaking faces not change. therefore, even for inefficient learning algorithm for which relatively small, can still very small, one needs lot information initially learn the problem, but not lot information re-train it, since does not change much. tracking maneuvering mobile target: tracking moving target characterized which means that the dynamic nature the target's location fully conveyed the learning algorithm the input data stream. this enables relatively simpler algorithms with solve the problem efficiently. human behavior: human behavior extremely dynamic, having this means that order efficiently predict human behavior, one must employ extremely efficient algorithms >>. similarly, can see how different solution techniques can best used for each problem, modeled using this relation: the chart illustrates the previously mentioned fundamental operational learning principle the more static the problem is, and the more data about have the closer are the upper-right corner the chart (and the more accurate our predictions will likely be). and the closest are the upper-right corner the less-efficient algorithms need employ produce accurate predictions. techniques that require vast amounts information train not work well with dynamics problems, hence deep learning works best for problems whose underlying structure does not change fast, such image processing and gesture recognition. contrast, simple algorithms that can process information quickly, e.g. kalman filter, can deal with dynamics problems but require high throughput information successfully predict. traditional machine learning approaches such logistic regression would efficient scenarios where have relatively high volumes incoming training data provided that the problems are fairly static well. _________________________________________________________________________ appendix social physics explained social physics the solution the never-ending chase endor tackles the need for constantly obtaining large quantities data for changing models from two orthogonal points view: transforming dynamic problems static model: the laws social physics are immutable and agnostic data-type and origin. therefore, using them project raw data into social physics canonic representation space transforms the original problem into instance new class problem, whose underlying model static (thanks the inherent static nature the social physics laws). this transforms the actual creating "big data" from small data: mentioned above, when faced with the challenge predicting their customers' behavior, most companies face major hurdle the amount relevant data they possess often insufficient. this specifically true for dynamic problems (frequent marketing use-cases) for problems that involve the introduction new element (such predicting the response new product, utilizing new kind input data). endor transforms all the data streams receives from its customers into social physics canonical form, regardless type, size and source. using this canonical form, endor able unify and consolidate all the data from all clients and all their queries into single extremely large data base, growing consistently high pace (namely, with >>). this then used train single, immutable deep-learning network that trained analyzing instances social physics canonical forms (and not specific query customer). hence, even each customer provides very limited data, endor able accumulate vast amount (canonically formed) data. endor's engine transforms the most difficult problems human behavior predictions into slowly changing ones (via social physics), and big-data (via the transformation canonical form), resulting "easy and efficient" problem, then solved using deep learning tools. this illustrated the following chart: easier for "static" problems [days/bit] [bits/day] easier for larger streams incoming data _________________________________________________________________________ appendix social physics explained social physics: overview the previous section have briefly described how social physics can used overcome the inherent challenge predicting human behavior, using semantics agnostic static mathematical invariances. order better understand how this feasible, consider physical laws for example newton's second law the law momentum conservation. any object maintains its initial path unless external force acts upon it. note that order deduce the existence such hidden force there need "learn the data", understand its statistical attributes, test many systems that behave similar way. since the physical law given, any violation abnormal and can immediately detected and interpreted the outcome some "invisible hand." one detects objects that suddenly change their direction, possible immediately deduce that force has acted upon them. their change similar, then most probable that the same force was exerted all them. this simple realization only possible due the understanding the physical law. while social physics far less absolute and rigorous than physical laws, the concept similar. something violates social physics laws, can immediately qualified "interesting," being the data manifestation some valuable property attribution the real world. this does not require learning, benchmarking, baselines any other data science machine learning tools. violation the social physics laws can detected extremely fast and very robust way, regardless the data type that generated it. endor: powered social physics ... data transformation into canonical representation extracting the previously discussed "behavioral clusters" (namely, the detected violations social physics invariances) and aggregating them into "knowledge sphere," the raw input data (of whatever shape form, long originates from human actors) being transformed into canonical form. this form represents clustering people who violated social physics law "together", other words people that display "too high synchronous change" their behavior, vis-a-vis certain invariance. this akin physical objects that would change direction specific time similar manner while the force that caused these changes not visible, can deduce with high likelihood that the objects were all affected single source. similarly, endor's canonical representation data the form behavioral clusters contains groups people who most probably were influenced the same hidden "social forces" and thus share common "real world traits". whenever new raw data available, sent endor the customer (usually daily weekly basis), allowing for additional behavioral clusters automatically extracted. _________________________________________________________________________ appendix social physics explained the benefit this representation threefold: automatic: using the mathematical invariances social physics, any data that originates from human behavior can automatically transformed into collection behavioral clusters, regardless the type input data, that does not need declared analyzed (e.g. phone call records, credit card purchases, taxi rides, any other type proprietary data the customer may possess). this, combined with extremely high resilience noises and gaps the input data, means that the process transforming dirty raw data unknown type into uniform behavioral clusters becomes, for the first time, fully automatic (see "robustness noise" section for additional information). uniform: stripping the data any domain, demographics semantics aspects, the remaining information containing the behavioral clusters ideally shaped for the following "querying" phase. fact, using this uniform representation, endor can "create big data" when there none, allowing the querying phase the process utilize deep-learning techniques that were impossible for the original owner the data! this enabled the endor deep-learning engine has access behavioral clusters originating from many types data, and from many customers, all transformed into single-form. emerging trends: simply put, the social physics invariances describe the way certain statistical properties human crowds evolve over time. this time-oriented aspect enables endor easily detect emerging behavioral changes dynamics that only recently occur, and that most cases did not have enough time generate enough observable data that would enable them detected with high enough statistical significance using traditional methods. addition the fact that through the use social physics these additional signals can detected, these are usually the very signals that are high importance for variety business questions they contain information about recent trends. "old-school" machine learning worked with pre-defined features and could extract relevant information from relatively small amounts generic data. however, much the results depended the selected features. deep learning identifies the most relevant features itself, but requires huge amounts data. each data type and question asked require finding the relevant features again, thus requiring more data. social physics transforms any type human behavior data canonical form human clusters based their behavior. this works with both small and large amounts data. addition, thanks the social physics' canonical form, endor can ingest all data types and all _________________________________________________________________________ appendix social physics explained questions, regardless data size, and generate one huge human behavior data set that uses the power deep learning answer any question. current predictive analytics paradigm (top charts) vs. new paradigm enabled through social physics (bottom charts). _________________________________________________________________________ appendix social physics explained ... querying the canonical representation ("knowledge sphere") the knowledge sphere contains the overall information detected about all users, can provide predictions for any question with the ease and speed simple data search: the initial creation the knowledge sphere usually takes hours for typical data consisting billion records. after this process complete, the same knowledge sphere can used answer dozens questions, minutes. there need for prior domain knowledge extraction relevant features. query submitted providing "example" (positively labeled ids), any size. endor's engine then uses the knowledge sphere generate answer: ranked list users, most probable "behaviorally similar" the relevant query. there "training" "learning" phase; there need "interpret the result." the equivalent this automatic process for each specific question would have required anything from weeks months effort for data scientists using conventional methods. for example, query regarding users who are mostly likely churn the near future described list that contains the identity previous churners. alternatively, query that tried identify which new customers are likely convert premium account would described list containing customers who recently converted this premium service. both queries, however, would use the same knowledge sphere, requiring re-training any sort. notice that whereas most such behavioral predictive questions would extremely hard resolve using deep learning (due lack sufficient data, and the frequent change the underlying model), endor circumvents this issue using the collection all knowledge spheres and queries. thus, endor "creates" the big data that required for its internal deep learning component, which charge providing the actual predictions. these predictions are based the particular knowledge sphere, even based extremely small amount data. the following chart illustrates the flow traditional deep learning (upper flow) comparison endor's one (lower flow). for non-human behavioral data (e.g. millions images) deep learning can likely produce high quality predictions, given proper training done data expert current deep learning tools are designed used engineers, who are addition, experienced with the use such tools. human data the other (e.g. taxi data) has underlying model that changes frequently that cannot easily resolved deep learning modeling (see section for in-depth discussion this issue). _________________________________________________________________________ appendix social physics explained however, when transformed into the form set behavioral clusters, taxi data stripped its semantics and becomes virtually identical representation behavioral clusters obtained from phone call records, credit card purchases, any other type human data. addition, there are many different customers who contribute data each those types (namely, many e-commerce platforms, each uploading their own purchase and web-activity data), training deep learning model now becomes possibility. this enabled the model would not trained the raw data problems, but the multitude behavioral clusters large collection data uniform representation, that also characterized static underlying model (i.e. the social physics laws, that are mathematical invariants and are therefore static, compared the behavioral dynamics the raw data, that highly dynamic). _________________________________________________________________________ appendix social physics explained data security and anonymity ... data stored endor addition its ability automatically resolve endless variety behavioral prediction questions, endor's solution provides high level data security and the ability anonymize any required data field. detailed the previous sections, the prediction done using the knowledge sphere collection semantics-free behavioral clusters, containing simply large number groups-of-users (each guaranteed have common social behavioral trait). easy see that any sensitive personal information, such information existed the original raw data, longer part any data used the system this stage. the only information that available the system ids users, shown the following example: behavioral_cluster (id, id, id, ...) behavioral_cluster (id, id, id, ...) ... behavioral_cluster, (id, id, id, ...) should noted that even this information can easily hidden hashing the ids the raw-data level the customer, upon data onboarding (see below). ... data onboarding endor presented previous sections the methods used for extracting the behavioral clusters from raw input data rely the detection groups users who display data-patterns that violate social physics invariance. this done tracking the dynamics certain statistical properties that portrays the synchronous nature the activity the users. for tis implementation, the aforementioned does not require the actual values contained the data, but alternatively can done fully hashed replacement. this enables the customer provide endor with fully hashed dataset, while still benefitting from its superior predictive capability. addition, endor semantics agnostic, the names the data-fields can hashed well. _________________________________________________________________________ appendix social physics explained example such hashing for financial records appears below: header before hashing: account number branch gender type transaction destintation account data records before hashing: male transfer male balance inquiry n/a female transfer field field field ... header after hashing: field field data records after hashing: ajfsdx qkpbva wsqpx qvbmas jeqxn ajfsdx qkpbva wsqpx qvbmas xpnma jeqxn qkpbva msba qvbmas ylbgq ... _________________________________________________________________________ appendix social physics explained social physics: mathematical explananation ... framework first introduce the basic principles endor's engine with their generic mathematical formalism. this followed two examples possible implementations: computervision-oriented and social-graph-based. the first example demonstrates endor's drawback when used with sensor-based data (that human-unrelated), whereas the latter illustrates the concepts social physics, and its benefits predicting human behavior. note: throughout this discussion provide numerous mathematical illustrations for the principles social physics and the way used the endor engine. however, certain mathematical details regarding the social physics laws were omitted from this discussion due considerations. let d(x, temporal data stream, where represents single data point. let l() law operator which transforms the raw data d(x, into law representation: dxdt the law itself formulated equation that equates the law operator a-priori constant (which can number, distribution class, such power law, etc.). this represents the invariant represented the law. for the purpose illustration, can imagine hypothetical law that represents the understanding that the change the output white noise signal over time limited very small threshold. violation this law would take the form, for example, signal that displays sudden strong output-spike. mechanism that filters signals and can detect such anomalous outputs can then built, order find violations this law. naturally using such mechanism would only make sense the case signals known governed the law question (i.e. origins white noise), for other signals such spikes cannot classified "violations". _________________________________________________________________________ appendix social physics explained another example can x-ray machine, built order detect anomalous "chunks" radiation-absorbing materials. this example the x-ray device (and the technician operating and deciphering the resulting images) serve "violation detector" for analyzing dimensional information streams and locating violations invariance that asserts that coherent x-ray beam that hit film should create image equal absorption (more less, depending the quality the film, and the coherence the beam). this invariance, law, known physical fact, and used for medical applications artificially inducing coherent x-ray beams onto high-quality films, while passing through third substance with the aspiration finding "violations" that would indicate the existence x-ray absorbing element this substance. bones for example are such material, representing this case "real world phenomenon" that detected the "data" (that the exposed film), manifested its unequal absorption radiation, that itself violation the aforementioned law. concrete formal mathematical examples for the implementation the above principles appear sections and ... the case social physics such violations would comprise group people, having generated certain dynamic the input data, analytically known statistically highly improbable, under the assumption that the input data was originated from human activities, and therefore adheres the social physics laws. ... validating that data-subset violation laws endor's engine implements law operators and constants several social physics laws (the specific details which are not described due considerations). this section describe the validation law violation. important note that many other computation problems, the validation that specific signal consists law violation fundamentally different the detection such interferences. this section only the former discussed, namely given the validation whether "potential interference" indeed law violation _________________________________________________________________________ appendix social physics explained not. the details endor's search algorithm that can efficiently scan large scale highdimensional data source and perform such on-the-fly validation will not discussed here. the mathematical formulation the laws validate always given the following form: given the explicit formulation law, local deviations from can validated measuring their deviation from it, denoted follows: lx,t here represents subspace whereas represents temporal window. this deviation can thus calculated for every subspace and any period time, and generates measure how much that subspace violates the relevant law, during the given time period. comparing this measure pre-defined threshold xthreshold the subspaces that violate the law can detected: threshold the violation threshold xthreshold selected such that the spontaneous emergence signal that would defer from the law further than the threshold highly improbable. this enables automatic verification that certain data subset violation law, with high-enough statistical significance, without any prior knowledge the semantics the data itself. notice that the signal changes both time and space, different temporal windows can create different subspaces that are detected law violations. endor uses pre-defined fixed set temporal windows that (derived from the laws and not from the data) -day, days, -days, -days. the data highly dynamic, the longer temporal windows are unlikely generate any deviation groups; the data static, the shorter temporal windows are unlikely generate any deviation groups. regardless, none the windows generate "junk-groups", because definition noise cannot generate consistent law violation (or more formal terms, the probability that noise will generate large enough violation the law, close zero, due the fact that this the way the threshold xthreshold selected). _________________________________________________________________________ appendix social physics explained the knowledge sphere aggregation all group deviations from all laws, for all relevant temporal windows: sphere this knowledge sphere calculated once per data-set, this process unaffected the queries being asked, but rather the internal behavioral structure originating from the raw data. from abstract point view, endor's social physics engine "compresses" the anonymized raw data into behaviorally relevant canonical representation. ... examples following are two examples that demonstrate the use laws for detection violating-patterns data. way illustration use known mathematical phenomena for demonstrating this mechanism. example vision let d(x,t) denote the color specific pixel specific frame may now define the same color law, dictating that every color any sub-region must some pre-defined color dxdt this operator takes region and time window and calculates the average color for this input data. this example illustrates the concept laws negation: there inherent a-priori known law about pixels video (certainly not one that assumes every sub-region the same average color...). considering this low operator and constant light blue, can now define square window size nxn pixels, and use single frame this yields the following the local deviation nxn windows from light blue: lx,t -dimenional images this can easily applied measure the deviation patches from light blue, comparing this deviation pre-defined threshold. patches whose deviation surpass this threshold will classified "clusters pixels with similar property", representing all the pixels the square that violated the "light blue law". _________________________________________________________________________ appendix social physics explained can illustrate this using slightly more generic hypothetical law: let's assume that images are always "smooth" (namely, they are monotonous smaller scales, lacking "peaks", local maxima\minima). under this assumption, can observe the following "data" image that contains "lawful" background smooth color, with two patches (one blue, and the other red). sampling large number small square-shaped random regions would easily locate these two "violations": then, "query" can asked, the form red pixel. such pixel would identified being contained the red violation, returning the red patch the result. note again that the intention these examples illustrate the mathematics and mechanics social physics rather suggest that advantageous for simple vision-based application, such applications can well addressed with traditional computational vision deep learning methods. example scale-free networks this example d(x, abstractly represents graph with being the graph's nodes. the law operator the degree-distribution operator, formulated as: has degree otherwise this vector operator generates for the degree each node. the summation the result this operator over all the graph's nodes yields cardinality vector for the graph's degrees (equivalent the degrees distribution, when dividing the number nodes). _________________________________________________________________________ appendix social physics explained this example shall assume that the graph scale-free network. therefore law constant that assumes the power-law degree distribution can applied (for some normalization constant this law constant can then formulated as: dxdt this law implies that the overall graph should obey power law distribution the degrees all its node. however, many large real-world scale-free graphs there could significant local deviations from such distribution. this may occur for example around cliques (i.e. fullyconnected sub-graphs) chains (i.e. sub-sets the nodes that form connected tree with node having more than neighbors). such deviation, violations, the law are illustrated the following chart, containing their manifestation both structural representation (left) and adjacency matrix (right): note that given the law, such violations can easily validated variety measures, such as: lx,t _________________________________________________________________________ appendix social physics explained this deviation measures the cumulative square the differences (whereas another example for such measure can the kl-divergence both probability distributions). here, represents all possible subgraphs the graph. obviously, scanning all possible subgraphs input graph not feasible, member class "difficult problems" known "non-polynomial hard problems". this sense, important distinguish between validating law violation (that requires knowing the details the law) and detecting law violation (that requires set proprietary techniques that are specifically developed for each law). such detection techniques are part endor's proprietary technology, tailored-made for the social physics mathematical laws. returning our scale-free example, assuming possessed efficient technique for finding such local interferences graphs, they would have resulted collection sub-graphs that can formulated follows: threshold and the knowledge sphere implied this law defined as: sphere once knowledge sphere containing the collection violations social physics laws available, can used detect "lookalikes" for any given labeled exemplar, defined list objects from the same domain. our example, given list graph nodes all the other graph nodes can scored according how many clusters they share with the labeled exemplar. alternatively, different scoring metrics can used, long they solely rely the detected clusters and the labeled list input, order produce output the form population scoring. refer such metrics, scoring mechanisms "scorers", and will elaborate these the next section. note that different temporal windows can generate different knowledge spheres, representing very different associations among the graph nodes. addition, these clusters represent behavioral connections that are not generated from external data sources such social media social networks, but rather from the customer's own internal transactional data source(s). this enables endor detect implicit behavioral clusters that are not explicitly manifested any available data. _________________________________________________________________________ appendix social physics explained ... answering questions: simple "scorer" once the knowledge sphere available the user can start asking business-relevant questions providing queries lists (possibly very small) labeled data with semantic meaning: this section give two examples "scorers" functions that use the behavioral clusters query generate ranked population list output. the first scorer discuss simple co-clusters aggregation scorer that for each candidate calculates the following score: scorey sphere otherwise each possible "candidate" scored according the number clusters the knowledge sphere that they co-inhabit with members the labeled data. this simple scorer aims for counting the number behavioral similarities the members with the input labeled list. the list objects that share clusters with objects the labeled input list represented as: sphere for example, given colored point and the "same color law" from example the colored area around point that given input. the "scale free graphs law" given node the output would the nodes all subgraphs containing that node that locally violate the degree distribution. _________________________________________________________________________ appendix social physics explained ... answering questions: deep learning scorer the scorer discussed the previous section intuitive example the way the clusters comprising the social physics knowledge sphere can used order produce high quality "lookalikes prediction" for any "examples input" (i.e, query), without knowing advance the nature this query. this simple example illustrates the advantages the social physics canonical data representation. however, order produce accurate prediction endor developed more robust scorer based state-of-the-art deep learning algorithms. noted above, where human behavior concerned, efficient use deep learning requires vast amounts data for its training phase. this hard achieve using the raw data, each labeled query usually contains small number labels, and there are typically only handful instances per query, and also relatively small number different queries per client. this type complex, dynamic and small-sized label data not well suited deep learning. endor's social physics engine overcomes this limitation transforming all datasets from all clients, and all instances all the queries into large collection single canonical form: knowledge sphere that composed clusters people, and examples that refer these clusters. combining all data from all these sources enabled endor create large enough labeled training-set train deep-learning network that scores each person, given the labels and the knowledge sphere. this process done once, resulting high-quality trained deep-learning model, that can then used efficiently process any new clusters set that produced from new data sources, and new queries. note that this trained model does not need periodically retrained not for new queries, nor for new types datasets, was trained (many) instances data represented collection clusters (i.e. the social physics canonical representation form). using this method, endor combines the advantages both social physics and deep learning: social physics transforms anonymized raw data into canonical form based violations social physics laws, whereas deep learning algorithms then score candidates based the created knowledge sphere and the (possibly few) labeled data. _________________________________________________________________________ appendix social physics explained this flow illustrated the following charts: _________________________________________________________________________ appendix social physics explained robustness noise one the main strengths the endor predictive platform its high resilience data-gaps and noisy data. traditionally, every data-analytics project begins with "data sanitation" phase, consisting the detection and desired mitigation undesired data segments such as: data gaps (i.e. periods missing data, either full partial) gibberish insertions the raw data semantic ambiguities, such category name that may appear the data more than one form normalization issues numeric values required binning numeric values ... machine learning algorithms are usually highly susceptible noise and given that data normally noisy, the phase mitigating those data problems typically costly and protracted. the reason for this that whereas the real world data generated its raw form (financial transactions, phone calls, etc.) order for analyze-able traditional machine learning techniques must converted aggregative form, referred "features", "properties". this aggregation can turn significantly affected small number wrong data values, different quantities values for different users. endor overcomes this requirement analyzing the raw data itself (as described detail the previous sections). addition, endor's engine does not perform statistical analysis the data the search patterns that can used for prediction, but rather uses the laws social physics mathematical invariances, external the data, and unaffected it. this approach has several significant advantages, stemming from the following basic notion: noise cannot create data-pattern that "cannot arbitrarily emerge" (where the latter defined data-patterns that can analytically shown exist negligible probability) order understand how this observation allows for the automatic extraction data insights from any human-data without any prior cleaning, let recall that the extraction process searches for groups objects the data that violates one the social physics laws. namely, groups that display data patterns that can prove cannot naturally emerge the data (using the mathematical analysis enabled social physics). this means that whereas noise can indeed hide insights from our engine, can, definition, (almost) never create data pattern that would detected law violation. noisy data cannot violate social physics law, only human-driven signal data can. _________________________________________________________________________ appendix social physics explained results this section presents variety use-cases that illustrate how the endor prediction system can utilized. detailed description the overall prediction process, using data which comprises days' worth activity large financial investment platform accurately answer different predictive questions (including comparative analysis with google's tensor flow deep learning platform); example using endor fully automatic way order crack kaggle challenge. coca-cola case-study which endor provides accurate predictions for multitude business questions using point-of-sale data, less than hours. using financial activity over days accurately and automatically answer four predictive questions this section demonstrate the overall prediction process using the endor system: ... description the raw data used the transformation the data the social physics representation (namely, the knowledge sphere that comprised set behavioral clusters) the definition four predictive questions the manifestation the queries the social physics form the overall predictive accuracy comparison google's tensor-flow deep learning platform. data: the data that was used this example originated from retail financial investment platform and contained the entire investment transactions members investment community. the data was anonymized and made public for research purposes mit (the data can shared upon request). following overall summary the dataset: days data rows unique users contextual semantic interpretation the data was given data fields: _________________________________________________________________________ appendix social physics explained field name type unique values time time unique time time user int categorical unique record int categorical unique property int categorical unique property int categorical unique property int categorical unique property int categorical unique property int categorical unique property int categorical unique property int categorical unique property double numeric unique unique the following important aspects the dataset should noted the data was given its raw non-aggregated form, and contained events user-level. contextual semantic interpretation the data was provided. data-sanitation was done. the data contained noises, gaps, duplicate records, and on. the data contained extremely uneven distribution records-per-user the users has less transactions, but there are with records. median records per user): records per user log-log representation the number records per user users _________________________________________________________________________ appendix social physics explained ... automatic clusters extraction upon first analysis the data the endor system detects and extracts "behavioral clusters" groups users whose data dynamics violates the mathematical invariances the social physics. these clusters are based all the columns the data, but limited only the last days this the data that was provided the system input. following summary the behavioral clusters from the dataset that were detected the system: number clusters: clusters sizes: (mean), (median), (max), (min) clusters per user: (mean), (median), (max), (min) users clusters: out the users records per user: (median), (mean): applies only users clusters (compared and general population) ... prediction queries the following prediction queries were defined: new users become "whales": users who joined the last weeks that will generate least commission the next days reducing activity users who were active the last week that will reduce activity the next days (but will not churn, and will still continue trading) churn "whales": currently active "whales" (as defined their activity during the last days), who were active the past week, become inactive for the next days will trade apple share for the first time: users who had never invested apple share, and would buy for the first time the coming days can seen, all the above questions refer data extending beyond the days dataset both past data (used generate the "search population", "examples"), and future data (used for validation the predictions). order avoid providing the system with any information external _________________________________________________________________________ appendix social physics explained the -day period, the queries were formulated the form lists containing users_ids values. for example: ... query name: "new users become "whales" search population: list user_ids containing users who joined the past weeks (prior end_of_data). "examples": list user_ids containing users known "whales" (i.e. users who generated more than commission the days prior end_of_data). list targets that need found: list user_ids, that subset "search population", containing users who generated more than commission the days after end_of_data. this list was only used for validation purposes, and was not provided the system. knowledge sphere manifestation queries again important note that the definition the search queries completely orthogonal the extraction behavioral clusters and the generation the knowledge sphere, which was done independently the queries definition. therefore, interesting analyze the manifestation the queries the clusters detected the system: the clusters contain information that relevant the definition the queries, despite the fact that: the clusters were extracted fully automatic way, using semantic information about the data, and the queries were defined after the clusters were extracted, and did not affect this process. this analysis done measuring the number clusters that contain very high concentration "samples"; other words, looking for clusters that contain "many more examples than statistically expected". high number such clusters (provided that significantly higher than the amount received when randomly sampling the same population) proves the ability this process extract valuable relevant semantic insights fully automatic way. the following table illustrates this observation, comparing the number behavioral clusters that contain certain amount "samples", compared the number "random clusters" that contain the same amount samples. random clusters refers set groups users randomly sampled from the customers population, such that equals the number behavioral clusters detected the system, and that the sizes these randomly sampled groups equals the sizes the clusters detected the system. this used demonstrate the information encapsulated the behavioral clusters, they contain significantly more clusters high consistency "target users" compared the random samples (and recalling they were detected the system prior the definitions the "questions"). the number "samples" requested given units "baseline". namely baseline" for the "reducing activity" query (for which the baseline approximately means clusters that have overlap with the "samples" (namely, that their members also appear the list "previous users who reduced activity"). _________________________________________________________________________ appendix social physics explained clusters containing many target customers reducing activity churn "whales" never bought new whales #random clusters #behavioral clusters from baseline from baseline from baseline from baseline from baseline from baseline from baseline from baseline from baseline from baseline ... prediction results the following table illustrates the accuracy the predictions for the four queries baseline: the average portion requested target customers random sample the population, representing the accuracy random guess. candidates: the size the search population. for example, the "new users become whales", the number candidates refers the number new users. top-: the portion requested targets customers the top- members the prediction report (similarly, for top- and top-). new users become "whales" users who joined the last weeks that will generate least commission the next days baseline top top top out candidates) reducing activity who among the current active users will reduce activity the next days (but will not churn) out candidates) churn "whales" currently active "whales", who were active the past week, become inactive for the next days? out candidates) will trade apple for the first time users who had never bought apple share, and will buy for the first time the coming days. out candidates) can seen, and expected, accuracy decreases reach deeper the predictions list. _________________________________________________________________________ appendix social physics explained ... comparison tensor-flow this section comparison between prediction results obtained the endor system and google's tensor flow presented. important note that tensor flow, like any other deep learning library, faces some difficulties when dealing with data similar the one under discussion: extremely uneven distribution the number records per user requires some canonization the data, which turn requires: some manual work, done individual who has least some understanding data science. some understanding the semantics the data, that requires investment time, well access the owner provider the data single-class classification, using extremely uneven distribution positive vs. negative samples, tends lead the overfitting the results and require some non-trivial maneuvering. this again necessitates the involvement expert deep learning (unlike the endor system which can used business, product marketing experts, with perquisites machine learning data science). have asked expert deep learning spend weeks crafting solution that would based tensor flow and has sufficient expertise able handle the data. the solution that was created uses the following auxiliary techniques: trimming the data sequence records per customer, and padding the streams for users who have less than records with neutral records. creating training sets, each having customers known positive labels, unknown) and then using these training sets train the model. using sequence classification (rnn with lstms) with output neurons (positive, negative), with the overall result being the difference between the scores the two. _________________________________________________________________________ appendix social physics explained the table below compares the results obtained using these techniques (red) endor's predictions (blue): baseline top top top new users become "whales" users who joined the last weeks that will generate least commission the next days out candidates) examples) reducing activity who among the current active users will reduce activity the next days (but will not churn) out candidates) examples) churn "whales" currently active "whales", who were active the past week, become inactive for the next days? out candidates) examples) will trade apple for the first time users who had never bought apple share, and will buy for the first time the coming days. out candidates) observations: endor outperforms tensor flow out queries, and results the same accuracy the th. the superiority endor increasingly evident the task becomes "more difficult" focusing the top- rather than the top-. there clear distinction between "less dynamic queries" (becoming whale, churn, reduce activity" for which static signals should likely easier detect) than the "who will trade apple for the first time" query, which are (a) more dynamic, and (b) have very low baseline, such that for the latter, endor times more accurate! previously mentioned the tensor flow results illustrated here employ -weeks manual improvements done deep learning expert, whereas the endor results are automatic. _________________________________________________________________________ appendix social physics explained automatically cracking kaggle challenge hours another example, have tested endor's system with the publicly available data from the kaggle competition known "acquire valued shoppers challenge". the data contained nearly million point-of-sale records, referring hundreds thousands customers, whereas the challenge entailed predicting which users who received certain promotional coupon would become recurring customer. the original challenge encompasses teams and lasted months. the endor machine, running fully automatically the raw data given the challenge's participants, was able produce predictions that outperformed the team who originally won first place. competing data science teams months challenge endor: few clicks, place _________________________________________________________________________ appendix social physics explained appendix endor's common use-cases for enterprises what would like have your own personal oracle? what would like have access the powerful data engines which only the likes google and facebook exploit? what would like you could use the most powerful such engine date endor get the most reliable answer any available tech can give? with the vision make blockchain prediction accessible and useful for all democratizing advanced machine learning, endor.coin empowers you with that ability, which until now was reserved only technological giants, equipped with internal teams professional data-experts. following summary the common prediction use-cases requested endor's enterprise customers. categorizes the use-cases the main segments endor currently serves: retail banks insurance companies retails and e-commerce however, thanks the use social physics, new use-cases can easily supported average requiring few hours endor's sales engineer. appendix endor.coin examples pre-defined predictions while endor.coin grand vision aims any sector being transformed blockchain, such insurance, banking, ecommerce health, start with are offering unprecedented prediction platform for cryptocurrency insights support cryptoholders seeking trading leads. blockchain's trust proven decentralized infrastructure, allowing anyone test any hypothesis, any data source, without disclosing their actual trading strategies, endor.coin empowers you see the future before becomes observable through the lenses any other existing technologies. following example some the first pre-defined blockchain predictions supported the endor.coin platform upon launch. new use-cases would gradually added, requested the community, using the rfp (request for predictions) mechanism. cryptocurrency addresses predictions: these predictions receives pre-defined list addresses (i.e. "bitcoin addresses that have had least one outgoing transaction the past month") and rank them according their behavioral similarity pre-defined behavior the recent past (i.e. "addresses that have received least bitcoin the past week"). the resulting list would contain its top the addresses who most resemble the pre-defined behavior (and therefore are statistically more likely display the same behavior the future), whereas the bottom the results list would contain addresses that least resemble the pre-defined behavior. active addresses: from all addresses who were active least once the past month, which most resemble the addresses which significantly increased their number transactions recently? heavy-trading addresses: from all addresses who were active least once the past month, which most resemble the addresses whose overall transactions volume the past month passed btc? becoming inactive addresses: from all addresses who were active least once the past month, which most resemble the addresses which decreased their overall transactions volume recently? token predictions: these predictions receives pre-defined list token (i.e. "tokens that had least trading volume during the last month") and rank them according their behavioral similarity pre-defined behavior the recent past (i.e. "tokens that increased their average monthly volume trading "). the resulting list would have tokens that most resemble the pre-defined behavior (and therefore are statistically more likely display the same behavior the future) the top, whereas tokens that least resemble the pre-defined behavior would places the bottom the list. profitable tokens: from all tokens with least usd trading volume the last month, which ones most resemble tokens that have increased their price with respect btc over the last month? non-profitable tokens: from all tokens with least usd trading volume the last month, which ones most resemble tokens that have decreased their price with respect btc over the last month? volatile tokens: from all tokens with least usd trading volume the last month, which ones most resemble the most volatile tokens last month? stable tokens: from all tokens with least usd trading volume the last month, which ones most resemble the least volatile tokens last month? increasing volume tokens: from all tokens with least usd trading volume the last month, which ones most resemble tokens that have doubled their monthly trading volume the last month? decreasing volume tokens: from all tokens with least usd trading volume the last month, which ones most resemble tokens that have decreased their monthly trading volume the last month? appendix knowledge sphere class: api access this appendix contains the specification the "knowledge sphere" data structure the basic building block the endor.coin protocol well the complete description the class that provides access it, for implementation future analytics engines, plugged the endor.coin network. usage explanation: the clusters object consists entities: sparse matrix dimensions (|searchable objects| behavioral clusters|), representing the connectivity between searchable objects and behavioral clusters, having mi,j iff searchable object index cluster searchable objects refer tokens, wallet addresses, locations, phone numbers any other type objects that included the data, and serve the basis the prediction. array mapping each searchable object index the sparse matrix dataframe containing miscellaneous clusters properties, defined and calculated each prediction engine. such properties can for example the size the cluster, ratio internal connectivity vs. external connectivity, the internal module used the prediction engine generate this cluster, and on. order extract and build the clusters object for usage, the following files are required: order build the sparse matrix spmat spmat spmat order build the mapping array newformat translationtabletmp numbers spmathlp order build clusters properties dataframe mat the above mentioned files are placed specific path, referred the clustersextractor class depicted below, as: usage example: after placing the relevant clusters files !cluster files path?, clusters can extracted using the following code: >>> >>> this example "pop clusters map" points the sparse matrix "clusters props" points the dataframe containing the clusters properties. "translation pop" points the array mapping each searchable object its index the sparse matrix, the clusters extractor class: following the complete description the knowledge sphere api: import from import import import import numpy import import import pandas from import namedtemporaryfile from import class clustersextractor object init self cluster files path def path mat init name blk data self mat fields src type blk type field fieldby 'wn' deg thrs 'sub clusters file percinternal def get path return path def pop clusters map build pop clusters map clusters props build mat props translation pop get translation pop return pop clusters map clusters props translation pop def build pop clusters map dims for mat get dimensions for matrix indices self indptr pop cluster map dims for mat indices indptr return pop cluster map def get dimensions for matrix mtfile spmat dims file name full path open stringio read unpack prop names temp loadmat mat name mat name prop names else prop names loadmat prop names return props def prop names dataframe for mat file prop names append index range len index return def prop names try prop names except exception msg """ doesn mat but cannot """ prop names msg prop try prop keyerror doesn mat remove from prop break prop else prop dataframe noinspection pyunresolvedreferences names 'wn' 'n' astype prcntg del mat file return cluster prop def get translation pop self spmat help name spmathlp spmat help name full path spmathlp data open read path mkdtemp spmat help name with open 'w' write spmathlp data with open num unpack <q' pytypechecker dtype=np num msg where self raise """ went wrong found """ num msg msg unlink local temp path return ids def name all files dir list self ()) file name for file name name return def path path expanduser path not path lookuperror not format open def get mat files names mat names list mat init name key=lambda group return mat names list def list dir self path path expanduser bibliography wikipedia social physics (). url https://en.wikipedia.org/wiki/social_physics pan, altshuler, pentland, decoding social influence and the wisdom the crowd financial trading network, in: privacy, security, risk and trust (passat), international conference and international confernece social computing (socialcom), ieee, pp. y.-y. liu, nacher, ochiai, martino, altshuler, prospect theory for online financial trading, plos one altshuler, pan, pentland, trends prediction using social diffusion models, arxiv.org, krafft, zheng, pan, della penna, altshuler, shmueli, tenenbaum, pentland, human collective intelligence distributed bayesian inference, arxiv preprint arxiv:.. altshuler, pentland, gordon, social behavior bias and knowledge management optimization, in: social computing, behavioral-cultural modeling, and prediction, springer, pp. altshuler, pentland, methods and apparatus for tuning network for optimal performance, patent (dec. url https://www.google.com/patents/us pan, altshuler, pentland, aharony, methods and apparatus for prediction and modification behavior networks, patent (aug. url https://www.google.com/patents/us tuning social networks gain the wisdom the crowd (mit media lab website (). url https://www.media.mit.edu/research/highlights/tuning-social-networks-gain-wisdom-crowd markets insight: wake the twitter effect markets (financial times) (). url http://web.media.mit.edu/~yanival/markets_insight.htm beyond the echo chamber (harvard business review (). url https://hbr.org///beyond-the-echo-chamber rethinking predictive analytics (firstmark's data driven) (). url http://firstmarkcap.com/insights/rethinking-predictive-analytics/ mit million test see social media can make investors money (). url https://tinyurl.com/mit-m-usd endor inventing the "google for predictive analytics" (). url http://news.mit.edu//endor-inventing-google-predictive-analytics- endor leading investor innovation endeavors (). url http://www.innovationendeavors.com boehme, altshuler, using social physics predict consumers behaviors, in: network science (netsci), mastercard brings new startups into start path accelerator program (). url https://tinyurl.com/mastercard-endor endor finnovate fall (). url https://www.youtube.com/watch?v=ruqloq-qa endor gartner cool vendor (). url https://www.gartner.com/doc/ endor technological pioneer acknowledgement the world economic forum (). url http://widgets.weforum.org/techpioneers-/ darpa network challange (). url http://archive.darpa.mil/networkchallenge/ the mckinsey award winners (). url https://hbr.org///the--mckinsey-award-winners google scholars professor alex "sandy" pentland (). url https://scholar.google.com/citations?user=pnfokyaaaaj&hl=en tim oreilly: the world's most powerful data scientists (). url http://www.forbes.com/pictures/lmmemkh/-alex-sandy-pentland-professor-mit/ altshuler, pentland, bruckstein, swarms and network intelligence search, springer, altshuler, elovici, cremers, aharony, pentland, security and privacy social networks, springer science business media, shrobe, shrier, pentland, new solutions for cybersecurity, mit press, clippinger, bollier, from bitcoin burning man and beyond: the quest for identity and autonomy digital society, and off the common books, hardjono, shrier, pentland, trust:: data: new framework for identity and data sharing, pentland, heibeck, honest signals: how they shape our world, mit press, pentland, social physics: how good ideas spread-the lessons from new science, penguin, shrier, pentland, frontiers financial technology: expeditions future commerce, from blockchain and digital banking prediction markets and beyond, publisher: visionary future, endor.coin protocol git (). url https://github.com/orgs/endorcoin why you want blockchain-based ai, even you dont know yet (). url https://tinyurl.com/blockchain-based-ai altshuler, aharony, pentland, elovici, cebrian, stealing reality: when criminals become data scientists (or vice versa), intelligent systems, ieee doi:./mis... ulieru, blockchain: what is, how really can change the world, world economic forum. how technology could help fix our broken financial system (). url https://tinyurl.com/technology-fixing-our-financia pieprzyk, hardjono, seberry, fundamentals computer security, springer science business media, hardjono, dondeti, multicast and group security, artech house, hardjono, dondeti, security wireless lans and mans (artech house computer security), artech house, inc., seberry, hardjono, towards the cryptanalysis bahasa indonesia and malaysia, ong, seberry, hardjono, academy., towards the cryptanalysis mandarin (pinyin), disclaimer: this white paper for discussion purpose only. endor.coin does not guarantee the accuracy the conclusions reached this white paper. copyright endor.coin.