unstoppable data for unstoppable apps: datacoin streamr july th, version this whitepaper for information only and does not constitute offer any kind investment advice. any element this whitepaper may undergo significant changes the project further develops. streamr vision streamr delivers unstoppable data unstoppable applications. the real-time data backbone the global supercomputer. decentralized network for scalable, low-latency, untamperable data delivery and persistence, operated the datacoin token. anyone anything can publish new data data streams, and others can subscribe these streams power dapps, smart contracts, microservices, and intelligent data pipelines. incentivize user participation the network, there's built-in mechanism for data monetization. valuable data from security exchanges, connected devices, iot sensors, and social media can offered companies, developers, and private citizens. machines can autonomously sell their data, get paid, and purchase the data they require. global market for real-time data emerges, with built-in data provenance, encryption, and access control. alongside the decentralized data network and marketplace, the full streamr stack includes powerful analytics engine and for rapid development real-time dapps. data streams, smart contracts, and decentralized computing resources can interconnected low-code environment using high-level building blocks. streamr will the easiest place create real-time, data-driven, and trustworthy blockchain applications. revolution taking place where centralized cloud services are one one being superseded tokenized, decentralized solutions. golem, for example, replaces azure virtual machine, and ipfs replaces azure blob storage. streamr proud join the revolution providing decentralized solution messaging and event processing, replacing platforms such azure eventhub and azure stream analytics. background real-time data will increasingly turn into commodity the coming years. huge volumes timestamped data being generated sensors and connected devices manufacturing, the service sector, and the entire supply chain which underlies the modern economy, with much the data generated streaming fashion,. the amount data increasing exponentially along the growth iot and the ubiquity connected devices. the global iot market, ihs markit forecasts that the installed base will grow from billion devices billion devices and billion much the newly generated data valuable: can used optimise manufacturing operations, track assets with increasing accuracy, target existing consumer services with high granularity, and create entirely new services and business models. the same time, there megatrend motion towards the next generation the computing stack. distributed future, the backend code decentralized apps dapps runs peer-to-peer networks. ethereum dapp itself, golem, and there are many more development. however, dapps not run isolation: they need external data function. is, storage and distribution real-world data remain centralised, and dapps remain liable all the known problems: concentration power, lack robustness, and vulnerability cyber attacks. sure, you can already store data the blockchain. there are also decentralized file storage apps such ipfs, swarm, and storj, and databases like bigchaindb are starting emerge. while such solutions are surely part the new decentralized fabric, they don't really provide answer cases where real-time data needed any significant volumes. the chain not designed for high throughput low latency, does not scale, and storage expensive. what needed natively decentralized data backbone complement decentralized apps. this real-time data backbone will the missing link, and the link that want help provide. the infrastructure create consists technology stack which helps connect and susan o'brien: big data trends shaping the future data-driven businesses", datameer, may (https://www.datameer.com/company/datameer-blog/-big-data-trends-shaping-future-data-driven-businesses/) tony baer: trends watch: big data", ovum, november (https://ovum.informa.com/~/media/informa-shop-window/tmt/files/whitepapers/_trends_to_watch_big_data .pdf) sam lucero: "iot platforms: enabling the internet things", ihs markit, march (https://cdn.ihs.com/www/pdf/enabling-iot.pdf) for definition dapps, see johnston al.: the general theory decentralized applications (https://github.com/davidjohnstonceo/decentralizedapplications) incentivise computers global peer-to-peer (pp) network. this network which provides low-latency, robust and secure data delivery and persistency, and all scale. dapps the future are fuelled data, and our mission make sure that the data keeps flowing. also create market for real-time data. the data market, anyone can publish events data streams, and anyone can subscribe streams and use the data decentralized apps. much the data free, but where that's not the case, the terms use are stored ethereum smart contracts. digital token datacoin needed access and operate the data market, and compensate nodes the network. subscribers pay for the data with the token, and data producers and network participants are reimbursed automatically and securely. our stack built decentralized transport layer. apart from greater robustness, resilience and fault tolerance, decentralization facilitates openness, transparency, and community building. the power over data not with large corporations like google, amazon, microsoft, and ibm. the network consists multitude data producers, data consumers, and message broker nodes in-between. you make reputation for yourself and earn good karma contributing data exchange and helping run the network everyone's benefit. believe that sustained growth the blockchain community will facilitated having good usability layer place. tools are needed that non-experts can create secure smart contracts, and connect those contracts and dapps reliable data sources. will help build the required toolkit providing visual editor, wrappers, and templates. short, want the place for anyone who's the business creating data-driven decentralized services. the rest this paper describe the streamr technology stack, define the role the digital token, explain the status quo, present the r&d roadmap, and introduce the team. streamr stack the decentralized real-time data pipeline built top multi-layered technology stack: streamr editor constitutes usability layer and toolkit which enables rapid development decentralized, data-driven apps. streamr engine high-performance event processing and analytics engine that executes off-chain decentralized fashion. can run decentralized computing provider such golem. streamr data market universe shared data streams which anyone can contribute and subscribe to. streamr network the data transport layer, defining incentivized peer-to-peer network for messaging the decentralized data pipeline. streamr smart contracts enable nodes the streamr network reach consensus, hold stream metadata, handle permissioning and integrity checking, and facilitate secure token transfers. the following section goes through each layer the stack (see figure detail, following top-down approach. figure streamr technology stack. streamr editor streamr editor enables rapid development data-driven smart contracts, lowers the threshold for dapp creation, and comes with ready-made templates for common use cases built in. there considerable interest the blockchain and decentralized applications within the business community, but the number real-life use cases remains limited. these are the early days, and not unreasonable postulate that many those who want get involved are not deep experts the minutiae ethereum, solidity, encryption, data provenance, and other technical issues. our view, the commercial growth the ecosystem requires tools which allow non-experts set smart contracts, connect trusted data sources, make use secure off-chain modules for data filtering, aggregation, and refinement, deploy decentralized applications, track smart contract execution, and visualise the flow input data and blockchain events. address the need for usability layer providing powerful tools (such easy-to-use visual editor), wrappers, and smart contract templates which are targeted domain professionals and business users. these tools hide the deep technology under the hood, handle the data integrations and communications, and automate the routine steps the deployment and monitoring smart contracts. foresee ecosystem where there are several usability platforms and tools available. the existing streamr platform already implements some elements the usability layer, with more functionality being added the coming months and years. the aim reach stage where you can build and deploy useful and functioning data-driven smart contract minutes. this more than fantasy; our demo edcon paris february taster what can already done (see figure for illustration). figure alpha version the streamr editor workspace. these are some the planned features for the usability layer: henri pihkala: "connecting ethereum with the real world: how easily create data-driven smart contracts", european ethereum development conference (edcon), paris, february, (https://www.youtube.com/watch?v=clrcj-fok) visual editor for creating smart contracts, feeding real-world data, and constructing off-chain data processing pipelines. modules for communication with smart contracts and interacting with the blockchain. modules for off-chain processing: data filtering, refinement, and aggregation, deployment decentralized applications, tracking smart contract execution, and visualising the flow input data and blockchain events. solidity editor where the smart contract code can written and modified context-sensitive environment. built-in and tested open source solidity templates for different use cases ethereum smart contracts. playback functionality for simulating smart contract functionality, debugging contract code, and testing functionality before deployment. one-click deployment submit smart contract either testnet the mainnet. streamr engine streamr engine the high-performance analytics engine that executes off-chain within decentralized computing provider (e.g. docker container golem). figure typical data flow pattern and outcomes for the streamr analytics engine. dapps, usually with web-based uis and smart contract-based back-ends, currently have way process raw data and turn into information. group iot sensors the stock market might produce thousands even millions events per second, quantity impossible far too expensive push onto any blockchain for computation. streaming analytics layer needed turn raw data into refined information and ready for consumption dapps and smart contracts. raw data may need filtered, downsampled, aggregated, combined with other data, run through anomaly detection algorithms, processed advanced machine learning and pattern recognition models. you may want things which simply cannot done smart contracts, such calling external apis part the processing chain. the streamr engine listens events the streamr network, and models built using the streamr editor refine incoming data and react new events real time. there are many ways react, including the following: publishing refined data another stream the streamr network, perhaps shown real-time dapp also connected the network. interacting with iot device, for example controlling actuator, opening lock, turning the lights on, calling the elevator. sending alert via email push notification. calling function smart contract. using the streamr network messaging glue between dapps and off-chain computation the engine enables whole new category decentralized apps: apps driven non-trivial data volume. obviously, the results can also consumed traditional centralized apps, while still enjoying the benefits decentralized messaging and analytics. data market streamr data market global universe shared data streams which anyone can contribute and subscribe to. it's place for data monetisation and machine-to-machine (mm) data exchange. the data market supports anonymity, but allows for the verification digital identity where required. the data market meeting place for data producers and data consumers. data consumers find value the data offered, and wish access order use input for dapps, smart contracts, traditional apps. the data organised data streams, the basic building block the data market and primitive the streamr network (see chapter below). data streams contain events from data sources that keep emitting new data points either regular irregular intervals. here are some typical settings where real-time data produced streaming fashion: stock market generates new event every time there new bid offer, and every time trade takes place. public transport vehicle broadcasts its identity, status, speed, acceleration, geolocation, and heading every few seconds. motion detector transmits signal when moving object detected its range. iiot sensors attached electrical drive measure the temperature, speed, and vibrations during the drive operation smart factory. air quality sensors measure carbon monoxide, sulfur dioxide, nitrogen dioxide, and ozone levels urban area. seismometers measure ground motion area with volcanic activity. smart clothing worn professional athletes collects biometric data such the heartbeat, temperature, and acceleration. the data market makes wide selection trustworthy timestamped data available for subscription. some the data sourced from established and professional data vendors and redistributors, and some from public, open data sources. importantly, the platform allows anyone contribute and monetize their data. whilst companies have valuable data streaming from sensors and devices, private citizens are producing valuable information too. for example, people wearing smartwatch might place their heart rate data sale the data market. data can offered anonymously, privacy not violated. who would interested such data? well, pharmaceutical company might buy for research, public healthcare organization might use find out how often people sports, what the stress level the public is. smartwatch manufacturer might buy get diagnostics how their heart rate sensors perform. and the data producers earn daily income just making their data available. there reason why subscriptions the data market should initiated human software developers, data engineers, data scientists. fact, the decentralized market may well end being dominated machine-to-machine transactions. autonomous machines, robots, smart appliances will all need data their operations, and they are producing data which valuable other participants the ecosystem. automatic, value-adding refinement patterns will emerge. might subscribe raw stock market feed, apply proprietary pattern recognition generate trading signals, and offer those signals for sale the same data market. whilst much the content the data market will freely available for all, there will data that needs paid for, and there will data where end user license applies. such cases, subscription license needed. license gives the right access the data for specific period time, certain conditions, and for fee. there's close analogy streaming music: you don't get own the subscribed data, any more than you get own the rights song hearing spotify downloading from itunes. data licenses are implemented smart contracts (see section ..). the great benefit the blockchain that offers trustless and decentralized way store the terms use and the access rights, and ensure that data payments are made agreed. wider context, there's potential for powerful network effect the marketplace. the more content there is, the more attractive the proposition becomes for both data contributors and data consumers. streamr data market, web portal (implemented dapp) facilitates the discovery what data exist out there, provides comprehensive toolkit for the creation and management data streams, and makes easy subscribe data streams choice. streamr network streamr network the data transport layer the technology stack. consists streamr broker nodes which establish network. the network hosts publish/subscribe mechanism and supports decentralized storage events. the network throughput scales linearly with the number participating nodes, and can process millions events per second. figure example event traveling through the broker network from data source subscriber dapp. the streamr network (figure transport layer the streamr stack. the network handles all messaging the decentralized data pipeline. the layer consists primitives (events and streams) and broker nodes. the broker nodes operate the primitives, and the collection broker nodes constitutes the network which handles decentralized storage and decentralized messaging. the infrastructure layer uses the underlying ethereum stack for its operations. node coordination requires robust consensus, which implemented through smart contracts. the raw event data itself usually doesn't into the blockchain, which together with partitioning allows the streamr network scale millions events per second and higher. the streamr network combines the best parts scalable cloud-based real-time data transports (e.g. kafka, zeromq, activemq) and what's available the decentralised pp/crypto community (whisper, bitmessage). the cloud-based frameworks use efficient sharding and persistence schemes reach high throughput, but only trusted local network environment. the peer-to-peer protocols showcase effective strategies for routing, peer discovery, nat traversal, location obfuscation, and on, but fail deliver the throughput needed for data intensive real-world applications. events event timestamped piece information. every event contains headers and content. the headers specify the metadata for the event, such its timestamp, origin, and content type. the event protocol supports arbitrary types and formats content payload, e.g. json messages binary images. the content type indicates what format the content in. event headers and content are encoded binary format for transmission. all events the streamr network are cryptographically signed. all events have origin, for example ethereum address. signature calculated from private key and the rest the message. the signature used prove the message origin well the message integrity. since the event format allows for any kind origins and signatures, the system future-proof. the following table lists the information contained event. field description version version the event protocol stream stream (ethereum address the stream smart contract) partition stream partition (see section partitioning) timestamp event timestamp (iso contenttype instruction how parse the body (e.g. json) encryptiontype encryption algorithm used encrypt the content gav wood: "whisper poc protocol spec" (https://github.com/ethereum/wiki/wiki/whisper-poc--protocol-spec) https://bitmessage.org/wiki/main_page. treamr events should not confused with events ethereum smart contracts. content data payload origintype instruction how interpret the origin origin data originator signaturetype instruction how interpret the signature signature cryptographic signature proves origin and integrity message streams all events belong stream. there can any number streams, each which groups together events that are logically related and stored ascending chronological order. stream metadata stored ethereum smart contract. each stream identified the ethereum address the contract. for scalability, events (i.e. the actual data points) are not stored smart contracts the blockchain. data stream holds set permissions. the permissions control who can read events from the stream (subscribe), and who can write new events the stream (publish). the stream owner controls the permissions, but she can also grant delegate the permission control third parties where needed. the following table lists the metadata for stream. field description stream (ethereum address) name stream name description stream description owner stream owner permissions mapping from ethereum address permission levels publish/subscribe data delivery the network follows the publish/subscribe paradigm. events published stream are promptly delivered all authorized and connected subscribers the stream. wikipedia: publish/subscribe pattern (https://en.wikipedia.org/wiki/publish-subscribe_pattern) subscribing streams can restricted certain users only, free the public. similarly, the permission publish content stream can held one, many, everyone. the publish/subscribe paradigm enables many messaging topologies used real-world applications: one-to-many (for example, news channel stock ticker) many-to-many (for example, group chat multiplayer game) one-to-one (for example, private chat analytics pipeline) many-to-one (for example, voting system) note that publishing event need not imply that the event delivered any clients: may the case that there are subscribers. still, the event persisted and delivered number broker nodes for redundancy. technically, there are two types subscribers. the majority the data flow goes subscribers connected the network via direct connection broker node (see section ... below). they can be, for example, web front-ends dapps, event processing chains running streamr engine, iot devices controlled data from the network. smart contracts are special type subscriber supported the streamr network. broker nodes the network are incentivized deliver events subscribing smart contracts. this scenario, course, blockchain scalability limits apply. the mechanism allows the network act oracle, meaning that data can pushed smart contracts without help from party oracle. since all data the network signed the source, can always verified and trusted. broker node the streamr broker node the core software component the network. broker node handles tasks such publishing events, subscribing streams, handling storage, and communicating with ethereum nodes via json rpc calls. the broker node exposes its functionality connected applications via apis. the broker api can used from apps using standard http and websocket libraries any language. for ease use, we'll provide reference implementations number languages. the primary client library platform will written javascript. can used deliver data web-based dapps running the browser well back-end applications running node.js. websocket api handles event delivery from data sources the network and from the network client dapps. for stream management, json api used. the websocket streaming api takes care the following tasks: authenticate session publish events subscribe events streams deliver events subscribed clients query historical events streams the json api exposes the following functionality: create stream configure stream delete stream get info about stream find stream(s) search criteria publish events (alternative websocket api) query historical events streams (alternative websocket api) most the traffic between brokers consists event messages, but there also traffic related routing and peer discovery. important coordination task between brokers partition assignment, which reliable consensus must achieved. this mechanism implemented smart contract which leverages the power the ethereum network (see section below). partitioning (sharding) event traffic the whole network divided into independent parts called partitions. each broker node handles the traffic belonging set partitions. this how scalability achieved: not all nodes handle all the traffic. this similar the partitioning scheme found e.g. apache kafka. the partition for particular event calculated hashing the stream id. this fast operation and done locally. using the stream the partition key means that all events particular stream always the same partition. this allows the network maintain the ordering events within stream, and store them efficiently. may happen that stream receives such volume messages that single broker cannot handle them. this case, another round partitioning applied the streams themselves, and the traffic within stream split independent parts. this case, hash the (stream id, stream partition) tuple assign the network partition, and the publisher provides the partition key which assigns the event partition within the stream. the order events for stream partition key preserved. the number partitions the network remains constant until automatically incremented over time. described the next section, there coordinator smart contract which controls network partitioning. the number partitions proportional the number broker nodes participating the network. node coordination distributed data systems such apache kafka and apache cassandra, node coordination usually achieved using component like apache zookeeper. there centralized process for establishing consensus processes like leader election. alternatively, some systems use manual assignment coordinator nodes which have special privileges the network. decentralized network, such centralized privileged components cannot exist. instead, the streamr network uses the underlying ethereum network establish consensus for node coordination the network. the key coordination task the assignment network partitions broker nodes the network, and the maintenance changes this information when nodes appear disappear. instead centralized component like zookeeper, this task implemented smart contract: the network coordinator the network coordinator contract deployed the ethereum blockchain. broker nodes find out the current network state watching and querying the smart contract. upgrading the network achieved simply switching new network coordinator contract. rebalancing the partition assignments one the tasks the network coordinator contract. only useful changes are made, and there are none, the function does nothing. when the network unbalanced, calling the function awards datacoin the caller. this incentive ensures that network rebalancing takes place when needed. the nodes assigned partition receive all the data for that partition. some all them calculate rolling checksums the data, and report the checksums the network coordinator smart contract certain intervals. large public network, there are enough nodes for each partition make difficult for them collude. partition assignment the network coordinator smart contract also difficult influence. incentivization subscribers are the consumers data the network. datacoin, the network's usage token, enables subscribers subscribe streams. other parties gain datacoin contributing the network: the broker nodes (the "miners" this network), and the data publishers. broker nodes are incentivized two things: report checksums for their assigned partitions the network coordinator smart contract (see section ... above), and deliver data any smart contract subscribers (see section ..). both operations cost some ethereum gas, paid the broker. this cost covered datacoin the brokers receive for making the network function. figure schematic diagram the incentive structure the streamr network. checksums for partition are calculated and reported multiple broker nodes, and the brokers are rewarded only the brokers agree the checksums coherence threshold set the coordinator smart contract (for example, assigned brokers need report particular checksum for considered valid). node reports deviant checksums, none all, the checksums are not coherent, reward obtained and offending nodes become less likely assigned responsibility for partition the future. discussed, smart contracts can subscribers stream. the subscriber sets bounty datacoin for delivering events the smart contract. the bounty collected whoever delivers the data first. usually this would the broker node directly connected the publisher, that broker forerunner position make the delivery. other nodes external subscribers may watch this process and identify opportunities deliver the data, not delivered the usual suspect. mechanism also needed prevent flooding the network. minimal cost must associated with all publish operations well deliveries subscribers. the network can aggregate the costs and commit every now and then the underlying blockchain for scalability, similar how state channels micropayment channels work some blockchain networks. event persistence events data streams are persisted the peer-to-peer network. this effectively turns the network into decentralized time series database. the decentralization brings number advantages, including greater robustness, fault tolerance, lower cost, and the potential for anonymity. with streams being sequences events, the simplest form storage event log. event log can stored any block storage, such file system the nodes themselves, decentralised object storage such swarm, ifps storj. for storage with much more granularity and querying features, decentralized databases such bigchaindb are emerging. solution like this likely candidate for event storage the streamr network. however the landscape changing rapidly, and won't commit specific storage solution this time. data provenance security and data provenance are critical issues whenever external data used inputs dapps and smart contracts. blockchain transactions are irrevocable, there's clear incentive for honest parties ensure that the inputs are trustworthy. there also incentive for dishonest parties well unscrupulous hackers manipulate the data for monetary benefit. the streamr network, every data point cryptographically signed private key owned the data origin. the area undergoing rapid development, and many different methods are possible. events could signed with, for example, ethereum private key, certificate owned iot sensor, trusted hardware enclaves using the intel sgx chip and the town crier relay, tlsnotary service bridging data from web api. design, the streamr network unopinionated the method used attest the data provenance, and can indeed support any method available now the future. events the network always carry both the signature itself well information which method use verify the signature. the client libraries which publish and subscribe events can incrementally add support for different methods, abstracting away the inner workings each method and making signature verification easy from developer's point view. viktor tron al: "introduction swarm" (http://swarm-guide.readthedocs.io/en/latest/introduction.html) "ipfs the permanent web" (https://github.com/ipfs/ipfs) torj: peer-to-peer cloud storage network" https://storj.io/storj.pdf see "bigchaindb: scalable blockchain database" (https://www.bigchaindb.com/whitepaper/bigchaindb-whitepaper.pdf) fan zhang al.: "town crier: authenticated data feed for smart contracts", proceedings the acm sigsac conference computer and communications security (ccs), vienna, austria, october pp. https://eprint.iacr.org//.pdf "tlsnotary mechanism for independently audited https sessions", whitepaper, september https://tlsnotary.org/tlsnotary.pdf the initially supported signature algorithm the same secpk ecdsa used ethereum. this conveniently allows the network reliably map any published data ethereum address. data confidentiality given that anyone can participate the streamr network running broker node, event payloads non-public streams the streamr network are strongly encrypted using asymmetric key cryptography. only parties holding authorized private key can read the data. the stream smart contracts hold the public keys whoever allowed access the stream. the time publishing, the public keys authorized recipients are used encrypt the data that only the authorized recipients can access the data. multicast encryption, techniques may used balance message size with keying complexity. built-in encryption support also allows for straight-forward data monetization, services such the streamr data market can created sell rent access stream contents. publishers can rekey the data stream selectively deny access e.g. case they catch subscribers reselling their data outside the network. decentralized approach combined with encryption brings safety: the data fragmented number unknown physical locations, and protected strong encryption while transit and storage. the design addresses the fears companies and organisations that are concerned about the potential for compromised data via physical access data centers and storage facilities. streamr smart contracts number ethereum smart contracts support the operation the streamr network and the data market. the streamr network uses smart contracts for incentivization, coordination, permissioning, and integrity checking. the data market builds upon features provided the network for data licensing and monetization. datacoin, erc token, used both layers for incentivization, reputation metric, and the means payment. micciancio, daniele and saurabh panjwani. "multicast encryption: how maintain secrecy large, dynamic groups?" http://cseweb.ucsd.edu/~spanjwan/multicast.html duan, yitao and john canny. computer science division, berkeley. "how construct multicast cryptosystems provably secure against adaptive chosen ciphertext attack" http://www.cs.berkeley.edu/~jfc/papers//ct-rsa.pdf stream the stream smart contract holds information about stream (see section ..). besides holding static information, carries the permissions for the stream. particular, carries the public keys authorized recipients for encrypted streams, possibly tied data license (see below). stream registry the stream registry contract holds information about known streams the network. streams can added the registry for lookup purposes. the stream registry can also register streams ens (ethereum name service). network coordinator the network coordinator contract assigns partitions broker nodes (see section ..). broker nodes register themselves with the coordinator and receive updates the network state watching the smart contract. data license the data license contract represents product listed the streamr data market. return for datacoin, the contract grants access associated set streams registering the purchaser's public key with the streams. the data license can valid for certain period. after the license expires the buyer will longer have access new data published the stream(s). the raison d'etre license contract hold the proof that the recipient has the right access data stream specific and immutable terms use, and simultaneously guarantee that the data provider receives the agreed payment for real-time data and when published. the terms use may stored the data license contract either directly (and hashed needed) link content-addressed storage such ipfs. the contract may also contain proof about fulfilled legal requirements such the result know-your-customer (kyc) process. broker nodes report event counts and rolling hashes stream smart contracts, which turn can report them the associated license contract. the license smart contracts can implement almost arbitrary safeguards prevent publisher fraud. they may, for example, lock the payment until certain amount events have been published the stream. the payment could also made incrementally over time event-by-event they are published. there may also mechanism for subscribers flag bad data, negatively affecting the publisher's reputation (see section datacoin and karma). these safety features ensure that the publisher cannot get paid without delivering quality data promised. datacoin datacoin the means compensation between data producers and consumers. also incentive for running broker nodes the network. datacoin the basis for karma, the reputation metric the community. bigger picture, way gain exposure data valuable commodity. figure datacoin flows the opposite direction from the data. there's integral role for digital token the decentralized data pipeline. datacoin the usage token the streamr network. data the symbol the token. maintaining and operating network takes resources: time, electricity, computing power, and communication bandwidth. incentivization the participating broker nodes are described section ... datacoin the means compensation between producers and consumers. other words, implements monetization mechanism for data producers. this incentive for data vendors step and help grow the community everyone's benefit. datacoin the basis for karma, the reputation metric the community data producers, data consumers, and message brokers. parties gain karma from datacoin transactions: publishing data, consuming data, and running broker nodes that operate the network. data producer gains karma when events she published are delivered subscribers. subscribers earn karma receiving events. broker nodes earn karma for helping with data delivery and persistence. bookkeeping easy: the amount new karma equals the amount datacoin exchanged. the difference that karma decays and eventually expires, while the token balance does not. datacoin implemented erc token ethereum. the token smart contract maintains datacoin balances, and ensures that payments are handled trustless and secure way. following the erc standard ensures interoperability with wallets and other tokens. datacoin will created token generating event (tge) currently scheduled for september its details, terms and conditions, and detailed schedule will announced later. current state there's highly advanced platform already place for creating data pipelines, visualisations, and off-chain computing microservices. the platform provides functional starting point, but reach full decentralization must refitted run decentralized container and use the new streamr network layer for message transport. not start from scratch. there's functional and highly advanced platform place for creating data pipelines, visualisations, off-chain processing, and ethereum smart contracts. the software built for the cloud environment with scalability, integrations, and fault tolerance mind. big data frameworks like kafka and cassandra are used for data handling and messaging. the streamr platform was demonstrated live edcon february and various blockchain meetups since. the pedigree, created the first version the software for our own use algorithmic high-frequency trading soon years ago. the principals all come from financial background, being either quants, trading system developers, stat arb traders, and some cases all the above. quantitative finance field where automatic processing high volumes real-time data has been the reality for the past years and more. only the past few years where the same kind modus operandi and the same kind tools and platforms are finding their way into the world iot, ioe, and now into the blockchain space. the current platform functional, scalable, and live use corporate customers. most the components not, however, translate directly the new world. storage needs decentralized, messaging, pub/sub functionality, and data monetization and encryption built into the transport layer, and the peer-to-peer network established along with node coordination and incentivization. the roadmap how these things presented the next section. roadmap figure r&d roadmap for the streamr project. the roadmap (figure divided three milestones (m-m). each milestone iteratively brings new features into layers the stack. each the three milestones consists work packages (wp-wp). completing all wps milestone completes the milestone. each work package has specific focus certain layers the stack. all wps milestone will worked roughly simultaneously, but wps the next milestone will not begin before the current milestone complete. there will also security audit the end each milestone. all smart contracts will audited, will relevant parts any non-smart contract code, for example the broker client itself. have chosen the iterative full-stack approach the following reasoning: can provide something that's working and usable the community from day one. are starting with existing technology stack. the stack modular that any layers can upgraded any point along the road. tangible and useful deliverables will achieved regardless which milestones reach. the community won't left high and dry with half-way solution; there will functional technology that fulfils many business use cases. milestone milestone releases the first version the incentivised data delivery network and the underlying smart contracts. work other layers builds what already have towards full integration with the ethereum ecosystem. (network, smart contracts): prototype decentralized broker network create first prototype version the decentralized broker node integrate libpp secpk ecdsa signing and verification event protocol json and websocket apis stream smart contract stream registry smart contract network controller smart contract partition assignment checksum reporting flood prevention using publisher/subscriber fees basic datacoin reward scheme the old "cloud" messaging layer and the new decentralized network will coexist side side for while, until the decentralized broker reaches scale and stability. the existing production engine and editor will run with the old broker until the network layer upgraded. (marketplace): barebones marketplace discovery publicly available data streams, initial categorisation and search stream product implementation, buyable items which grant access set streams exchange for datacoin users can define stream products and offer them the marketplace populate the marketplace with real-time data streams from different verticals such the following: financial market data: stock prices, option prices, commodity prices, cryptocurrency rates, fiat exchange rates, trading volume, and corporate actions. social media data: twitter, instagram, facebook, reddit, flickr, etc. transportation data: departures, arrivals, geolocation, speed, heading for airplanes, ships, trains, and local transport. weather data: temperature, precipitation, humidity, cloud cover, wind speed, both current and forecast. the first version the marketplace will have some centralized "training wheels" simplify access management and mirror the centralized/decentralized the production network (engine): engine goes ethereum streamr-web bridge support ethereum interactions from streamr smart contract deployment, abi support for predeployed contracts local and transactional function calls event watching key and account management support for different testnets and mainnet signatures and signature verification dockerizing the engine for golem other decentralized computing provider (editor): seamless on-chain and off-chain computing improving the visual editor fully support all ethereum-related features the engine integrated solidity editor for writing custom smart contract modules built-in selection smart contract templates for the most common use cases (payments, betting, sla monitoring, forecasting, etc.), and the ability easily apply real-world data streams these templates. redoing the ui/ux the editor and associated web application milestone this milestone, launch the first version the data marketplace, along with the features needs the underlying network layer. (network, smart contracts): support for data monetization and encryption first stable version basic encryption support data license smart contract support smart contracts subscription target add support for further data signing methods, e.g. based x., sgx basic storage either decentralized block storage decentralized implement and utilize karma stress testing, optimizing scalability (data market): completely decentralized marketplace "training wheels" removed from the initial marketplace implementation achieve full decentralization, with data licenses modeled smart contracts the blockchain, and used for permissioning and access control initial key distribution mechanism support permission-controlled stream content public network seller identity verification seller reputation mechanism (engine, editor): achieving decentralization migrate the new network layer deployment golem other container provider fault tolerance and failure recovery decentralized environment milestone the goal milestone reach the full streamr vision. the r&d milestone bound change along the way, the previous milestones will spark various ideas and the community will request new features. milestone will also contain significant marketing efforts drive adoption the stack. (network): advanced routing, location obfuscation location obfuscation multicast encryption stress testing, optimizing scalability work any problems found large-scale deployments work features requested community work integrations with emerging platforms (data market): community building adding more data streams the data market stream wishlist, bounty program accelerate adoption improvements reputation mechanism increased community building efforts (engine, editor): productization improving the and the tools onboarding, tutorial videos, help materials add integrations relevant platforms blockchain, iot, ai, other spaces project management team henri pihkala, m.sc henri software engineer, serial entrepreneur, and algorithmic trader. has led the development two high-frequency algorithmic trading platforms and designed and managed building the distributed streamr cloud analytics platform. henri passionate about complex architecture, scalability, usability, and the blockchain. risto karjalainen, ph.d. risto data scientist and finance professional with ph.d. from the wharton school. he's quant with international career automated, systematic trading and institutional asset management. risto's interests include real-time computing, machine learning, evolutionary algorithms, behavioural finance, the blockchain, and fintech general. nikke nylund, b.sc. nikke former low latency algorithmic trading strategist. he's got some years managerial and entrepreneurial experience founder and/or serial investor ict and tech companies with several successful exits under his belt. nikke holds bsc finance and entrepreneurship from the helsinki school economics. michael malka, m.sc. michael entrepreneur and technology enthusiast with years experience different roles from software developer ceo. has been involved software projects different sectors from startups banks and telcos. michael studied computer science the university helsinki before starting his first software company. advisors tba conclusion this whitepaper outlines our vision for robust, natively decentralized real-time data backbone for decentralized apps. believe that the combination real-time data market and the data pipeline will transformative for ethereum smart contract developers and the dapp ecosystem large. our ambition build well thought-out and professionally implemented technology stack that serves the future needs our audience, and provides unstoppable data for unstoppable apps. our technology stack layered and modular, and built decentralized transport layer. there peer-to-peer network consisting incentivized broker nodes. the network hosts publish/subscribe mechanism and supports decentralized storage encrypted events. throughput scales linearly with the number participating nodes, and the network can process millions events per second. the data backbone ideal facilitator the economy, where autonomous machines, bots, and robots buy sell tiny pieces data. the idea has been raised that machines will barter resources such storage, processing capacity, communication bandwidth, etc. believe that using datacoin leads much lower transaction costs than bartering. streamr part the computing revolution where monolithic solutions are being superseded decentralized computing alternatives. distributed computing, golem replaces azure virtual machines. block storage, ipfs, storj, and others replace azure blob storage. data pipeline and messaging, streamr replaces centralized messaging and event processing platforms such azure eventhub and azure stream analytics. there's power transfer taking place from corporations and enterprises individual citizens, autonomous agents, apps, and machines, leading improved privacy, efficiency, resilience, and fault tolerance, and ultimately higher welfare for the good denizens the connected society. alex puig: "how the blockchain could enable the machine economy (mm)", january, (https://www.linkedin.com/pulse/how-blockchain-could-enable-machine-economy-mm-alex-puig)